{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJHxXDos1knx"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhXnEffJD8N3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "DQeT7--F6cfB",
        "outputId": "f9b20e78-4f24-4eb2-ae0d-4dbf59f0c879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6hrQwxEEFqX",
        "outputId": "386f31d2-a0ee-40da-95d1-e80c457c4d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/MyDrive/University/U of T/4th Year/ECE496 Team/Hardware/_WORKING_VERSION_\n",
            "best_128CNNsBiLSTMmodel\t\t    mcu_data\t\t mcu_eeg_data1_1.txt\n",
            "best_doubleCNNmodel\t\t    mcu_data_batch\t mcu_eeg_data1_2.txt\n",
            "best_doubleCNNmodel.cc\t\t    mcu_eeg_data0_0.txt  mcu_eeg_data1_3.txt\n",
            "best_doubleCNNmodel.tflite\t    mcu_eeg_data0_1.txt  mcu_eeg_data1_4.txt\n",
            "Convert_New_Model.ipynb\t\t    mcu_eeg_data0_2.txt  testdata\n",
            "lfinalbest_doubleCNNmodel_3\t    mcu_eeg_data0_3.txt  testdata20230405\n",
            "lfinalbest_doubleCNNmodel_3.cc\t    mcu_eeg_data0_4.txt  Videos\n",
            "lfinalbest_doubleCNNmodel_3.tflite  mcu_eeg_data1_0.txt\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "%cd '/content/gdrive/MyDrive/University/U of T/4th Year/ECE496 Team/Hardware/_WORKING_VERSION_'\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eovxOInFUfRx"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXAdJ8q3oGAC"
      },
      "outputs": [],
      "source": [
        "# Label values\n",
        "W = 0\n",
        "N1 = 1\n",
        "N2 = 2\n",
        "N3 = 3\n",
        "REM = 4\n",
        "UNKNOWN = 5\n",
        "\n",
        "NUM_CLASSES = 6  # exclude UNKNOWN\n",
        "\n",
        "class_dict = {\n",
        "    0: \"W\",\n",
        "    1: \"N1\",\n",
        "    2: \"N2\",\n",
        "    3: \"N3\",\n",
        "    4: \"REM\",\n",
        "    5: \"?\"\n",
        "}\n",
        "\n",
        "EPOCH_SEC_LEN = 30  # seconds\n",
        "SAMPLING_RATE = 256\n",
        "\n",
        "def print_n_samples_each_class(labels):\n",
        "    \n",
        "    unique_labels = np.unique(labels)\n",
        "    for c in unique_labels:\n",
        "        n_samples = len(np.where(labels == c)[0])\n",
        "        # print(\"{}: {}\".format(class_dict[c], n_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HklRYzPkm3q7"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "\n",
        "def load_npz_file(npz_file):\n",
        "    \"\"\"Load data and labels from a npz file.\"\"\"\n",
        "    with np.load(npz_file) as f:\n",
        "        data = f[\"x\"]\n",
        "        labels = f[\"y\"]\n",
        "        sampling_rate = f[\"fs\"]\n",
        "    return data, labels, sampling_rate\n",
        "    \n",
        "def load_npz_list_files(data_dir, npz_files):\n",
        "    \"\"\"Load data and labels from list of npz files.\"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    fs = None\n",
        "    for npz_f in npz_files:\n",
        "        # print(\"Loading {} ...\".format(npz_f))\n",
        "        # print(\"npz_f.....\",npz_f)\n",
        "        npz_file = data_dir + '/' + npz_f\n",
        "        tmp_data, tmp_labels, sampling_rate = load_npz_file(npz_file)\n",
        "        \n",
        "        if fs is None:\n",
        "            fs = sampling_rate\n",
        "        elif fs != sampling_rate:\n",
        "            raise Exception(\"Found mismatch in sampling rate.\")\n",
        "        data.append(tmp_data)\n",
        "        labels.append(tmp_labels)\n",
        "\n",
        "    data = np.vstack(data)\n",
        "    labels = np.hstack(labels)\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "def get_balance_class_oversample(x, y):\n",
        "    \"\"\"\n",
        "    Balance the number of samples of all classes by (oversampling):\n",
        "        1. Find the class that has the largest number of samples\n",
        "        2. Randomly select samples in each class equal to that largest number\n",
        "    \"\"\"\n",
        "    class_labels = np.unique(y)\n",
        "    n_max_classes = -1\n",
        "    for c in class_labels:\n",
        "        n_samples = len(np.where(y == c)[0])\n",
        "        if n_max_classes < n_samples:\n",
        "            n_max_classes = n_samples\n",
        "\n",
        "    balance_x = []\n",
        "    balance_y = []\n",
        "    for c in class_labels:\n",
        "\n",
        "        idx = np.where(y == c)[0]\n",
        "        # print('c',c)\n",
        "        # print('idx',idx)\n",
        "        n_samples = len(idx)\n",
        "        n_repeats = int(n_max_classes / n_samples)\n",
        "        # print('n_repeats',n_repeats)\n",
        "        # print('x',x)\n",
        "        # print('len(x)',len(x))\n",
        "        # print('x[idx]',x[idx])\n",
        "        tmp_x = np.repeat(x[idx], n_repeats, axis=0)\n",
        "        tmp_y = np.repeat(y[idx], n_repeats, axis=0)\n",
        "        n_remains = n_max_classes - len(tmp_x)\n",
        "        if n_remains > 0:\n",
        "            sub_idx = np.random.permutation(idx)[:n_remains]\n",
        "            tmp_x = np.vstack([tmp_x, x[sub_idx]])\n",
        "            tmp_y = np.hstack([tmp_y, y[sub_idx]])\n",
        "        balance_x.append(tmp_x)\n",
        "        balance_y.append(tmp_y)\n",
        "    balance_x = np.vstack(balance_x)\n",
        "    balance_y = np.hstack(balance_y)\n",
        "\n",
        "    return balance_x, balance_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGX9V3VF-EqL"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data_dir, trainfile_idx, testfile_idx):\n",
        "    # Load all files\n",
        "    print('data_dir:',data_dir)\n",
        "    allfiles = os.listdir(data_dir)\n",
        "    print(\"allfiles\",allfiles)\n",
        "    train_files = []\n",
        "    test_files = []\n",
        "    \n",
        "    for idx in trainfile_idx:\n",
        "       \n",
        "        train_files.append(allfiles[idx])\n",
        "        \n",
        "    for idx in testfile_idx:\n",
        "        test_files.append(allfiles[idx])\n",
        "    # # Ns: the number of subjects in the dataset\n",
        "    # Ns = int(len(allfiles) / k_folds)\n",
        "    # print('Ns:',Ns)\n",
        "    \n",
        "    # Split train and test files\n",
        "    test_files = sorted(test_files)\n",
        "    train_files = sorted(train_files)\n",
        "    \n",
        "    # print(\"test_files:\",test_files)\n",
        "    \n",
        "    # Load data in npz files\n",
        "    # data_train: (2796, 1, 7680)\n",
        "    # label_train: (2796,)\n",
        "    data_train, label_train = load_npz_list_files(data_dir,train_files)\n",
        "    # x_val: (2884, 1, 7680)\n",
        "    # y_val: (2884,)\n",
        "    x_val, y_val = load_npz_list_files(data_dir, test_files)\n",
        "    # print('data_train',data_train.shape) # \n",
        "    # print('x_val',x_val.shape) # (2884, 1, 7680)\n",
        "    # print('label_train',label_train.shape) # (2796,)\n",
        "    # print('y_val',y_val.shape) # (2884,)\n",
        "    \n",
        "    # Reshape the data to match the input of the model - conv2d\n",
        "    data_train = np.squeeze(data_train) # (3868, 7680)\n",
        "    x_val = np.squeeze(x_val) # (1812, 7680)\n",
        "    data_train = data_train[:, :, np.newaxis, np.newaxis] # (3868, 7680, 1, 1)\n",
        "    x_val = x_val[:, :, np.newaxis, np.newaxis] # (1812, 7680, 1, 1)\n",
        "    \n",
        "    # Casting\n",
        "    data_train = data_train.astype(np.float32)\n",
        "    label_train = label_train.astype(np.int32)\n",
        "    x_val = x_val.astype(np.float32)\n",
        "    y_val = y_val.astype(np.int32)\n",
        "        \n",
        "    # print('reshaped data_train',data_train.shape)\n",
        "    # print('reshaped x_val',x_val.shape)\n",
        "    \n",
        "    # print(\"Training set: {}, {}\".format(data_train.shape, label_train.shape))\n",
        "    # print_n_samples_each_class(label_train)\n",
        "    # print(\" \")\n",
        "    # print(\"Validation set: {}, {}\".format(x_val.shape, y_val.shape))\n",
        "    # print_n_samples_each_class(y_val)\n",
        "    # print(\" \")\n",
        "    \n",
        "    # Use balanced-class, oversample training set\n",
        "    x_train, y_train = get_balance_class_oversample(\n",
        "            x=data_train, y=label_train\n",
        "    )\n",
        "    # print(\"Oversampled training set: {}, {}\".format(\n",
        "    #     x_train.shape, y_train.shape\n",
        "    # ))\n",
        "    print_n_samples_each_class(y_train)\n",
        "    print(\" \")\n",
        "    return x_train, y_train, x_val, y_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6FN8MAZUjsg"
      },
      "source": [
        "#Convert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMx_yEZ1eaj8"
      },
      "outputs": [],
      "source": [
        "def representative_data_gen():\n",
        "\n",
        "  #trainfile_idx = [0,3,4,5]\n",
        "  #testfile_idx = [1,2]\n",
        "  trainfile_idx = [0,1,2,3]\n",
        "  testfile_idx = [4,5]\n",
        "  data_dir = \"/content/gdrive/MyDrive/University/U of T/4th Year/ECE496 Team/Software/ECE496Software/data/eeg_fz_ler\"\n",
        "\n",
        "  x_train, y_train, x_val, y_val = preprocess_data(data_dir, trainfile_idx, testfile_idx)\n",
        "  test_batches_ori = (x_val, y_val)\n",
        "  # input_value = x_val[:1, :]\n",
        "  # print(input_value.shape)\n",
        "  # for i in range(500):\n",
        "  #   print(x_val[i-1:i+1, :].shape)\n",
        "  #   yield([x_val[i-1:i+1, :]])\n",
        "  # for input_value, _ in test_batches.take(100):\n",
        "  #   yield [input_value]\n",
        "  #yield input_value\n",
        "  x_val = x_val.swapaxes(1, 2)\n",
        "  for i in range(101):\n",
        "  #   yield [input_value]\n",
        "    #print(x_val[i:i+1, :].shape)\n",
        "    yield [x_val[i:i+1, :]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBUBbtOrCShc",
        "outputId": "e9754c50-10b7-4f7c-d544-74c9058314a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_dir: /content/gdrive/MyDrive/University/U of T/4th Year/ECE496 Team/Software/ECE496Software/data/eeg_fz_ler\n",
            "allfiles ['01-03-0001.npz', '01-03-0002.npz', '01-03-0003.npz', '01-03-0004.npz', '01-03-0012.npz', '01-03-0008.npz']\n",
            " \n"
          ]
        }
      ],
      "source": [
        "export_dir = 'best_128CNNsBiLSTMmodel'\n",
        "tflite_model_file = 'best_128CNNsBiLSTMmodel.tflite'\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "# Set the optimization flag.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Enforce integer only quantization\n",
        "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "#converter.inference_input_type = tf.int8\n",
        "#converter.inference_output_type = tf.int8\n",
        "\n",
        "# For BiLSTM\n",
        "converter._experimental_default_to_single_batch_in_tensor_list_ops = True\n",
        "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "#converter._experimental_lower_tensor_list_ops = False\n",
        "\n",
        "# Provide a representative dataset to ensure we quantize correctly.\n",
        "converter.representative_dataset = representative_data_gen\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLlx-lcdHVgH"
      },
      "outputs": [],
      "source": [
        "!xxd -i 'best_128CNNsBiLSTMmodel.tflite' > 'best_128CNNsBiLSTMmodel.cc'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK6SzZAIWi_O"
      },
      "source": [
        "# TFLITE Analyzer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.lite.experimental.Analyzer.analyze(model_path='best_128CNNsBiLSTMmodel.tflite')"
      ],
      "metadata": {
        "id": "hwAs2bzj_azz",
        "outputId": "8eb22cc1-5f22-48bc-b0f5-4e3ad79f7dde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== best_128CNNsBiLSTMmodel.tflite ===\n",
            "\n",
            "Your TFLite model has '9' subgraph(s). In the subgraph description below,\n",
            "T# represents the Tensor numbers. For example, in Subgraph#0, the QUANTIZE op takes\n",
            "tensor #0 as input and produces tensor #70 as output.\n",
            "\n",
            "Subgraph#0 main(T#0) -> [T#243]\n",
            "  Op#0 QUANTIZE(T#0) -> [T#70]\n",
            "  Op#1 EXPAND_DIMS(T#70, T#3[-3]) -> [T#71]\n",
            "  Op#2 RESHAPE(T#71, T#4[-1, 1, 7680, 1]) -> [T#72]\n",
            "  Op#3 CONV_2D(T#72, T#69, T#67[0, 0, 0, 0, 0, ...]) -> [T#73]\n",
            "  Op#4 SHAPE(T#71) -> [T#74]\n",
            "  Op#5 STRIDED_SLICE(T#74, T#5[0], T#6[-3], T#7[1]) -> [T#75]\n",
            "  Op#6 CONCATENATION(T#75, T#8[1, 1280, 64]) -> [T#76]\n",
            "  Op#7 RESHAPE(T#73, T#76) -> [T#77]\n",
            "  Op#8 SQUEEZE(T#77) -> [T#78]\n",
            "  Op#9 MUL(T#78, T#66) -> [T#79]\n",
            "  Op#10 ADD(T#79, T#65) -> [T#80]\n",
            "  Op#11 MAX_POOL_2D(T#80) -> [T#81]\n",
            "  Op#12 EXPAND_DIMS(T#81, T#3[-3]) -> [T#82]\n",
            "  Op#13 RESHAPE(T#82, T#9[-1, 1, 160, 64]) -> [T#83]\n",
            "  Op#14 CONV_2D(T#83, T#64, T#58[0, 0, 0, 0, 0, ...]) -> [T#84]\n",
            "  Op#15 SHAPE(T#82) -> [T#85]\n",
            "  Op#16 STRIDED_SLICE(T#85, T#5[0], T#6[-3], T#7[1]) -> [T#86]\n",
            "  Op#17 CONCATENATION(T#86, T#10[1, 160, 128]) -> [T#87]\n",
            "  Op#18 RESHAPE(T#84, T#87) -> [T#88]\n",
            "  Op#19 SQUEEZE(T#88) -> [T#89]\n",
            "  Op#20 SHAPE(T#89) -> [T#90]\n",
            "  Op#21 STRIDED_SLICE(T#90, T#5[0], T#11[-2], T#7[1]) -> [T#91]\n",
            "  Op#22 CONCATENATION(T#91, T#12[160, 128]) -> [T#92]\n",
            "  Op#23 ADD(T#89, T#57) -> [T#93]\n",
            "  Op#24 RESHAPE(T#93, T#92) -> [T#94]\n",
            "  Op#25 MUL(T#94, T#56) -> [T#95]\n",
            "  Op#26 ADD(T#95, T#55) -> [T#96]\n",
            "  Op#27 EXPAND_DIMS(T#96, T#3[-3]) -> [T#97]\n",
            "  Op#28 RESHAPE(T#97, T#13[-1, 1, 160, 128]) -> [T#98]\n",
            "  Op#29 CONV_2D(T#98, T#54, T#59[0, 0, 0, 0, 0, ...]) -> [T#99]\n",
            "  Op#30 SHAPE(T#97) -> [T#100]\n",
            "  Op#31 STRIDED_SLICE(T#100, T#5[0], T#6[-3], T#7[1]) -> [T#101]\n",
            "  Op#32 CONCATENATION(T#101, T#10[1, 160, 128]) -> [T#102]\n",
            "  Op#33 RESHAPE(T#99, T#102) -> [T#103]\n",
            "  Op#34 SQUEEZE(T#103) -> [T#104]\n",
            "  Op#35 SHAPE(T#104) -> [T#105]\n",
            "  Op#36 STRIDED_SLICE(T#105, T#5[0], T#11[-2], T#7[1]) -> [T#106]\n",
            "  Op#37 CONCATENATION(T#106, T#12[160, 128]) -> [T#107]\n",
            "  Op#38 ADD(T#104, T#53) -> [T#108]\n",
            "  Op#39 RESHAPE(T#108, T#107) -> [T#109]\n",
            "  Op#40 MUL(T#109, T#52) -> [T#110]\n",
            "  Op#41 ADD(T#110, T#51) -> [T#111]\n",
            "  Op#42 EXPAND_DIMS(T#111, T#3[-3]) -> [T#112]\n",
            "  Op#43 RESHAPE(T#112, T#13[-1, 1, 160, 128]) -> [T#113]\n",
            "  Op#44 CONV_2D(T#113, T#50, T#60[0, 0, 0, 0, 0, ...]) -> [T#114]\n",
            "  Op#45 SHAPE(T#112) -> [T#115]\n",
            "  Op#46 STRIDED_SLICE(T#115, T#5[0], T#6[-3], T#7[1]) -> [T#116]\n",
            "  Op#47 CONCATENATION(T#116, T#10[1, 160, 128]) -> [T#117]\n",
            "  Op#48 RESHAPE(T#114, T#117) -> [T#118]\n",
            "  Op#49 SQUEEZE(T#118) -> [T#119]\n",
            "  Op#50 SHAPE(T#119) -> [T#120]\n",
            "  Op#51 STRIDED_SLICE(T#120, T#5[0], T#11[-2], T#7[1]) -> [T#121]\n",
            "  Op#52 CONCATENATION(T#121, T#12[160, 128]) -> [T#122]\n",
            "  Op#53 ADD(T#119, T#49) -> [T#123]\n",
            "  Op#54 RESHAPE(T#123, T#122) -> [T#124]\n",
            "  Op#55 MUL(T#124, T#48) -> [T#125]\n",
            "  Op#56 ADD(T#125, T#47) -> [T#126]\n",
            "  Op#57 MAX_POOL_2D(T#126) -> [T#127]\n",
            "  Op#58 RESHAPE(T#127, T#14[-1, 5120]) -> [T#128]\n",
            "  Op#59 CONV_2D(T#72, T#46, T#68[0, 0, 0, 0, 0, ...]) -> [T#129]\n",
            "  Op#60 CONCATENATION(T#75, T#15[1, 154, 64]) -> [T#130]\n",
            "  Op#61 RESHAPE(T#129, T#130) -> [T#131]\n",
            "  Op#62 SQUEEZE(T#131) -> [T#132]\n",
            "  Op#63 MUL(T#132, T#45) -> [T#133]\n",
            "  Op#64 ADD(T#133, T#44) -> [T#134]\n",
            "  Op#65 MAX_POOL_2D(T#134) -> [T#135]\n",
            "  Op#66 EXPAND_DIMS(T#135, T#3[-3]) -> [T#136]\n",
            "  Op#67 RESHAPE(T#136, T#16[-1, 1, 39, 64]) -> [T#137]\n",
            "  Op#68 CONV_2D(T#137, T#43, T#61[0, 0, 0, 0, 0, ...]) -> [T#138]\n",
            "  Op#69 SHAPE(T#136) -> [T#139]\n",
            "  Op#70 STRIDED_SLICE(T#139, T#5[0], T#6[-3], T#7[1]) -> [T#140]\n",
            "  Op#71 CONCATENATION(T#140, T#17[1, 39, 128]) -> [T#141]\n",
            "  Op#72 RESHAPE(T#138, T#141) -> [T#142]\n",
            "  Op#73 SQUEEZE(T#142) -> [T#143]\n",
            "  Op#74 SHAPE(T#143) -> [T#144]\n",
            "  Op#75 STRIDED_SLICE(T#144, T#5[0], T#11[-2], T#7[1]) -> [T#145]\n",
            "  Op#76 CONCATENATION(T#145, T#18[39, 128]) -> [T#146]\n",
            "  Op#77 ADD(T#143, T#42) -> [T#147]\n",
            "  Op#78 RESHAPE(T#147, T#146) -> [T#148]\n",
            "  Op#79 MUL(T#148, T#41) -> [T#149]\n",
            "  Op#80 ADD(T#149, T#40) -> [T#150]\n",
            "  Op#81 EXPAND_DIMS(T#150, T#3[-3]) -> [T#151]\n",
            "  Op#82 RESHAPE(T#151, T#19[-1, 1, 39, 128]) -> [T#152]\n",
            "  Op#83 CONV_2D(T#152, T#39, T#62[0, 0, 0, 0, 0, ...]) -> [T#153]\n",
            "  Op#84 SHAPE(T#151) -> [T#154]\n",
            "  Op#85 STRIDED_SLICE(T#154, T#5[0], T#6[-3], T#7[1]) -> [T#155]\n",
            "  Op#86 CONCATENATION(T#155, T#17[1, 39, 128]) -> [T#156]\n",
            "  Op#87 RESHAPE(T#153, T#156) -> [T#157]\n",
            "  Op#88 SQUEEZE(T#157) -> [T#158]\n",
            "  Op#89 SHAPE(T#158) -> [T#159]\n",
            "  Op#90 STRIDED_SLICE(T#159, T#5[0], T#11[-2], T#7[1]) -> [T#160]\n",
            "  Op#91 CONCATENATION(T#160, T#18[39, 128]) -> [T#161]\n",
            "  Op#92 ADD(T#158, T#38) -> [T#162]\n",
            "  Op#93 RESHAPE(T#162, T#161) -> [T#163]\n",
            "  Op#94 MUL(T#163, T#37) -> [T#164]\n",
            "  Op#95 ADD(T#164, T#36) -> [T#165]\n",
            "  Op#96 EXPAND_DIMS(T#165, T#3[-3]) -> [T#166]\n",
            "  Op#97 RESHAPE(T#166, T#19[-1, 1, 39, 128]) -> [T#167]\n",
            "  Op#98 CONV_2D(T#167, T#35, T#63[0, 0, 0, 0, 0, ...]) -> [T#168]\n",
            "  Op#99 SHAPE(T#166) -> [T#169]\n",
            "  Op#100 STRIDED_SLICE(T#169, T#5[0], T#6[-3], T#7[1]) -> [T#170]\n",
            "  Op#101 CONCATENATION(T#170, T#17[1, 39, 128]) -> [T#171]\n",
            "  Op#102 RESHAPE(T#168, T#171) -> [T#172]\n",
            "  Op#103 SQUEEZE(T#172) -> [T#173]\n",
            "  Op#104 SHAPE(T#173) -> [T#174]\n",
            "  Op#105 STRIDED_SLICE(T#174, T#5[0], T#11[-2], T#7[1]) -> [T#175]\n",
            "  Op#106 CONCATENATION(T#175, T#18[39, 128]) -> [T#176]\n",
            "  Op#107 ADD(T#173, T#34) -> [T#177]\n",
            "  Op#108 RESHAPE(T#177, T#176) -> [T#178]\n",
            "  Op#109 MUL(T#178, T#33) -> [T#179]\n",
            "  Op#110 ADD(T#179, T#32) -> [T#180]\n",
            "  Op#111 MAX_POOL_2D(T#180) -> [T#181]\n",
            "  Op#112 RESHAPE(T#181, T#20[-1, 2560]) -> [T#182]\n",
            "  Op#113 CONCATENATION(T#128, T#182) -> [T#183]\n",
            "  Op#114 SHAPE(T#183) -> [T#184]\n",
            "  Op#115 STRIDED_SLICE(T#184, T#5[0], T#7[1], T#7[1]) -> [T#185]\n",
            "  Op#116 PACK(T#185, T#21[60], T#22[128]) -> [T#186]\n",
            "  Op#117 RESHAPE(T#183, T#186) -> [T#187]\n",
            "  Op#118 SHAPE(T#187) -> [T#188]\n",
            "  Op#119 STRIDED_SLICE(T#188, T#5[0], T#7[1], T#7[1]) -> [T#189]\n",
            "  Op#120 PACK(T#189, T#22[128]) -> [T#190]\n",
            "  Op#121 FILL(T#190, T#30) -> [T#191]\n",
            "  Op#122 DEQUANTIZE(T#191) -> [T#192]\n",
            "  Op#123 TRANSPOSE(T#187, T#23[1, 0, 2]) -> [T#193]\n",
            "  Op#124 DEQUANTIZE(T#193) -> [T#194]\n",
            "  Op#125 REVERSE_V2(T#193, T#5[0]) -> [T#195]\n",
            "  Op#126 DEQUANTIZE(T#195) -> [T#196]\n",
            "  Op#127 WHILE(T#24[0], T#24[0], T#2, T#192, T#192, T#196, Cond: Subgraph#1, Body: Subgraph#2) -> [T#197, T#198, T#199, T#200, T#201, T#202]\n",
            "  Op#128 QUANTIZE(T#199) -> [T#203]\n",
            "  Op#129 TRANSPOSE(T#203, T#23[1, 0, 2]) -> [T#204]\n",
            "  Op#130 REVERSE_V2(T#204, T#7[1]) -> [T#205]\n",
            "  Op#131 WHILE(T#24[0], T#24[0], T#2, T#192, T#192, T#194, Cond: Subgraph#3, Body: Subgraph#4) -> [T#206, T#207, T#208, T#209, T#210, T#211]\n",
            "  Op#132 TRANSPOSE(T#208, T#23[1, 0, 2]) -> [T#212]\n",
            "  Op#133 QUANTIZE(T#212) -> [T#213]\n",
            "  Op#134 CONCATENATION(T#213, T#205) -> [T#214]\n",
            "  Op#135 SHAPE(T#214) -> [T#215]\n",
            "  Op#136 STRIDED_SLICE(T#215, T#5[0], T#7[1], T#7[1]) -> [T#216]\n",
            "  Op#137 PACK(T#216, T#22[128]) -> [T#217]\n",
            "  Op#138 FILL(T#217, T#31) -> [T#218]\n",
            "  Op#139 DEQUANTIZE(T#218) -> [T#219]\n",
            "  Op#140 TRANSPOSE(T#214, T#23[1, 0, 2]) -> [T#220]\n",
            "  Op#141 DEQUANTIZE(T#220) -> [T#221]\n",
            "  Op#142 REVERSE_V2(T#220, T#5[0]) -> [T#222]\n",
            "  Op#143 DEQUANTIZE(T#222) -> [T#223]\n",
            "  Op#144 WHILE(T#24[0], T#24[0], T#1, T#219, T#219, T#223, Cond: Subgraph#5, Body: Subgraph#6) -> [T#224, T#225, T#226, T#227, T#228, T#229]\n",
            "  Op#145 STRIDED_SLICE(T#226, T#25[-1, 0, 0], T#26[0, 0, 128], T#27[1, 1, 1]) -> [T#230]\n",
            "  Op#146 QUANTIZE(T#230) -> [T#231]\n",
            "  Op#147 WHILE(T#24[0], T#24[0], T#1, T#219, T#219, T#221, Cond: Subgraph#7, Body: Subgraph#8) -> [T#232, T#233, T#234, T#235, T#236, T#237]\n",
            "  Op#148 STRIDED_SLICE(T#234, T#25[-1, 0, 0], T#26[0, 0, 128], T#27[1, 1, 1]) -> [T#238]\n",
            "  Op#149 QUANTIZE(T#238) -> [T#239]\n",
            "  Op#150 CONCATENATION(T#239, T#231) -> [T#240]\n",
            "  Op#151 FULLY_CONNECTED(T#240, T#29, T#28[-1941, -1383, 3272, -4950, 1829]) -> [T#241]\n",
            "  Op#152 SOFTMAX(T#241) -> [T#242]\n",
            "  Op#153 DEQUANTIZE(T#242) -> [T#243]\n",
            "\n",
            "Tensors of Subgraph#0\n",
            "  T#0(serving_default_input_1:0) shape_signature:[-1, 1, 7680, 1], type:FLOAT32\n",
            "  T#1(TensorArrayV2_1) shape:[1, 1, 128], type:FLOAT32 RO 512 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#2(TensorArrayV2_11) shape:[60, 1, 128], type:FLOAT32 RO 30720 bytes, buffer: 3, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#3(double_cnn_bi_lstm_4/conv1d_32/Conv1D/ExpandDims/dim) shape:[], type:INT32 RO 4 bytes, buffer: 4, data:[-3]\n",
            "  T#4(double_cnn_bi_lstm_4/conv1d_32/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 5, data:[-1, 1, 7680, 1]\n",
            "  T#5(double_cnn_bi_lstm_4/bidirectional_8/backward_lstm_8/strided_slice/stack) shape:[1], type:INT32 RO 4 bytes, buffer: 6, data:[0]\n",
            "  T#6(double_cnn_bi_lstm_4/conv1d_32/Conv1D/strided_slice/stack_1) shape:[1], type:INT32 RO 4 bytes, buffer: 4, data:[-3]\n",
            "  T#7(double_cnn_bi_lstm_4/bidirectional_8/ReverseV2/axis) shape:[1], type:INT32 RO 4 bytes, buffer: 8, data:[1]\n",
            "  T#8(double_cnn_bi_lstm_4/conv1d_32/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 9, data:[1, 1280, 64]\n",
            "  T#9(double_cnn_bi_lstm_4/conv1d_33/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 10, data:[-1, 1, 160, 64]\n",
            "  T#10(double_cnn_bi_lstm_4/conv1d_33/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 11, data:[1, 160, 128]\n",
            "  T#11(double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/strided_slice/stack_1) shape:[1], type:INT32 RO 4 bytes, buffer: 12, data:[-2]\n",
            "  T#12(double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/concat/values_1) shape:[2], type:INT32 RO 8 bytes, buffer: 13, data:[160, 128]\n",
            "  T#13(double_cnn_bi_lstm_4/conv1d_34/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 14, data:[-1, 1, 160, 128]\n",
            "  T#14(double_cnn_bi_lstm_4/flatten_8/Const) shape:[2], type:INT32 RO 8 bytes, buffer: 15, data:[-1, 5120]\n",
            "  T#15(double_cnn_bi_lstm_4/conv1d_36/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 16, data:[1, 154, 64]\n",
            "  T#16(double_cnn_bi_lstm_4/conv1d_37/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 17, data:[-1, 1, 39, 64]\n",
            "  T#17(double_cnn_bi_lstm_4/conv1d_37/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 18, data:[1, 39, 128]\n",
            "  T#18(double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/concat/values_1) shape:[2], type:INT32 RO 8 bytes, buffer: 19, data:[39, 128]\n",
            "  T#19(double_cnn_bi_lstm_4/conv1d_38/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 20, data:[-1, 1, 39, 128]\n",
            "  T#20(double_cnn_bi_lstm_4/flatten_8/Const_1) shape:[2], type:INT32 RO 8 bytes, buffer: 21, data:[-1, 2560]\n",
            "  T#21(double_cnn_bi_lstm_4/reshape/Reshape/shape/1) shape:[], type:INT32 RO 4 bytes, buffer: 22, data:[60]\n",
            "  T#22(double_cnn_bi_lstm_4/bidirectional_8/backward_lstm_8/zeros/packed/1) shape:[], type:INT32 RO 4 bytes, buffer: 23, data:[128]\n",
            "  T#23(transpose/perm) shape:[3], type:INT32 RO 12 bytes, buffer: 24, data:[1, 0, 2]\n",
            "  T#24(time) shape:[], type:INT32 RO 4 bytes, buffer: 6, data:[0]\n",
            "  T#25(strided_slice_2) shape:[3], type:INT32 RO 12 bytes, buffer: 26, data:[-1, 0, 0]\n",
            "  T#26(strided_slice_21) shape:[3], type:INT32 RO 12 bytes, buffer: 27, data:[0, 0, 128]\n",
            "  T#27(strided_slice_22) shape:[3], type:INT32 RO 12 bytes, buffer: 28, data:[1, 1, 1]\n",
            "  T#28(double_cnn_bi_lstm_4/dense_4/BiasAdd/ReadVariableOp) shape:[5], type:INT32 RO 20 bytes, buffer: 29, data:[-1941, -1383, 3272, -4950, 1829]\n",
            "  T#29(double_cnn_bi_lstm_4/dense_4/MatMul) shape:[5, 256], type:INT8 RO 1280 bytes, buffer: 30, data:[., ., ., ., ., ...]\n",
            "  T#30(double_cnn_bi_lstm_4/bidirectional_8/backward_lstm_8/zeros/Const) shape:[], type:INT8 RO 1 bytes, buffer: 31, data:[.]\n",
            "  T#31(double_cnn_bi_lstm_4/bidirectional_8/backward_lstm_8/zeros/Const1) shape:[], type:INT8 RO 1 bytes, buffer: 31, data:[.]\n",
            "  T#32(double_cnn_bi_lstm_4/batch_normalization_39/FusedBatchNormV31) shape:[128], type:INT8 RO 128 bytes, buffer: 33, data:[j, J, ., ., ., ...]\n",
            "  T#33(double_cnn_bi_lstm_4/batch_normalization_39/FusedBatchNormV3) shape:[128], type:INT8 RO 128 bytes, buffer: 34, data:[~, ., ., ., ., ...]\n",
            "  T#34(double_cnn_bi_lstm_4/conv1d_39/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[128], type:INT8 RO 128 bytes, buffer: 35, data:[., ., ., ., ., ...]\n",
            "  T#35(double_cnn_bi_lstm_4/conv1d_39/Conv1D/Conv2D1) shape:[128, 1, 6, 128], type:INT8 RO 98304 bytes, buffer: 36, data:[\", ., ., ., ., ...]\n",
            "  T#36(double_cnn_bi_lstm_4/batch_normalization_38/FusedBatchNormV31) shape:[128], type:INT8 RO 128 bytes, buffer: 37, data:[., ., ., ., /, ...]\n",
            "  T#37(double_cnn_bi_lstm_4/batch_normalization_38/FusedBatchNormV3) shape:[128], type:INT8 RO 128 bytes, buffer: 38, data:[., ., ., ., ., ...]\n",
            "  T#38(double_cnn_bi_lstm_4/conv1d_38/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[128], type:INT8 RO 128 bytes, buffer: 39, data:[., ., ., ., ., ...]\n",
            "  T#39(double_cnn_bi_lstm_4/conv1d_38/Conv1D/Conv2D) shape:[128, 1, 6, 128], type:INT8 RO 98304 bytes, buffer: 40, data:[., ., ., #, ., ...]\n",
            "  T#40(double_cnn_bi_lstm_4/batch_normalization_37/FusedBatchNormV31) shape:[128], type:INT8 RO 128 bytes, buffer: 41, data:[@, ,, 4, I, h, ...]\n",
            "  T#41(double_cnn_bi_lstm_4/batch_normalization_37/FusedBatchNormV3) shape:[128], type:INT8 RO 128 bytes, buffer: 42, data:[\n",
            ", M, ., ;, Q, ...]\n",
            "  T#42(double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[128], type:INT8 RO 128 bytes, buffer: 43, data:[', *, &, ., ., ...]\n",
            "  T#43(double_cnn_bi_lstm_4/conv1d_37/Conv1D/Conv2D) shape:[128, 1, 6, 64], type:INT8 RO 49152 bytes, buffer: 44, data:[., ., ., ., ., ...]\n",
            "  T#44(double_cnn_bi_lstm_4/batch_normalization_36/FusedBatchNormV31) shape:[64], type:INT8 RO 64 bytes, buffer: 45, data:[., ?, ., ., ., ...]\n",
            "  T#45(double_cnn_bi_lstm_4/batch_normalization_36/FusedBatchNormV3) shape:[64], type:INT8 RO 64 bytes, buffer: 46, data:[., ., ., ., ., ...]\n",
            "  T#46(double_cnn_bi_lstm_4/conv1d_36/Conv1D/Conv2D1) shape:[64, 1, 400, 1], type:INT8 RO 25600 bytes, buffer: 47, data:[., ., ., ., ., ...]\n",
            "  T#47(double_cnn_bi_lstm_4/batch_normalization_35/FusedBatchNormV31) shape:[128], type:INT8 RO 128 bytes, buffer: 48, data:[y, ., ., ., ., ...]\n",
            "  T#48(double_cnn_bi_lstm_4/batch_normalization_35/FusedBatchNormV3) shape:[128], type:INT8 RO 128 bytes, buffer: 49, data:[z, ., ., ., ., ...]\n",
            "  T#49(double_cnn_bi_lstm_4/conv1d_35/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[128], type:INT8 RO 128 bytes, buffer: 50, data:[., ., ., ., ., ...]\n",
            "  T#50(double_cnn_bi_lstm_4/conv1d_35/Conv1D/Conv2D) shape:[128, 1, 8, 128], type:INT8 RO 131072 bytes, buffer: 51, data:[., ., ., %, ., ...]\n",
            "  T#51(double_cnn_bi_lstm_4/batch_normalization_34/FusedBatchNormV31) shape:[128], type:INT8 RO 128 bytes, buffer: 52, data:[., @, ?, =, ., ...]\n",
            "  T#52(double_cnn_bi_lstm_4/batch_normalization_34/FusedBatchNormV3) shape:[128], type:INT8 RO 128 bytes, buffer: 53, data:[., ], Z, S, ., ...]\n",
            "  T#53(double_cnn_bi_lstm_4/conv1d_34/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[128], type:INT8 RO 128 bytes, buffer: 54, data:[ , ., ., ., ., ...]\n",
            "  T#54(double_cnn_bi_lstm_4/conv1d_34/Conv1D/Conv2D) shape:[128, 1, 8, 128], type:INT8 RO 131072 bytes, buffer: 55, data:[%, ., ., ., ., ...]\n",
            "  T#55(double_cnn_bi_lstm_4/batch_normalization_33/FusedBatchNormV31) shape:[128], type:INT8 RO 128 bytes, buffer: 56, data:[i, w, {, r, {, ...]\n",
            "  T#56(double_cnn_bi_lstm_4/batch_normalization_33/FusedBatchNormV3) shape:[128], type:INT8 RO 128 bytes, buffer: 57, data:[., ., 2, ., 1, ...]\n",
            "  T#57(double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[128], type:INT8 RO 128 bytes, buffer: 58, data:[., ., ., ., ., ...]\n",
            "  T#58(double_cnn_bi_lstm_4/conv1d_39/Conv1D/Conv2D) shape:[128], type:INT32 RO 512 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#59(double_cnn_bi_lstm_4/conv1d_39/Conv1D/Conv2D2) shape:[128], type:INT32 RO 512 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#60(double_cnn_bi_lstm_4/conv1d_39/Conv1D/Conv2D3) shape:[128], type:INT32 RO 512 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#61(double_cnn_bi_lstm_4/conv1d_39/Conv1D/Conv2D4) shape:[128], type:INT32 RO 512 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#62(double_cnn_bi_lstm_4/conv1d_39/Conv1D/Conv2D5) shape:[128], type:INT32 RO 512 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#63(double_cnn_bi_lstm_4/conv1d_39/Conv1D/Conv2D6) shape:[128], type:INT32 RO 512 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#64(double_cnn_bi_lstm_4/conv1d_33/Conv1D/Conv2D) shape:[128, 1, 8, 64], type:INT8 RO 65536 bytes, buffer: 65, data:[., ., ., ., ., ...]\n",
            "  T#65(double_cnn_bi_lstm_4/batch_normalization_32/FusedBatchNormV31) shape:[64], type:INT8 RO 64 bytes, buffer: 66, data:[., (, ., ., /, ...]\n",
            "  T#66(double_cnn_bi_lstm_4/batch_normalization_32/FusedBatchNormV3) shape:[64], type:INT8 RO 64 bytes, buffer: 67, data:[., ., ., ., ., ...]\n",
            "  T#67(double_cnn_bi_lstm_4/conv1d_36/Conv1D/Conv2D) shape:[64], type:INT32 RO 256 bytes, buffer: 68, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#68(double_cnn_bi_lstm_4/conv1d_36/Conv1D/Conv2D2) shape:[64], type:INT32 RO 256 bytes, buffer: 68, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#69(double_cnn_bi_lstm_4/conv1d_32/Conv1D/Conv2D) shape:[64, 1, 50, 1], type:INT8 RO 3200 bytes, buffer: 70, data:[., ., ., ., ., ...]\n",
            "  T#70(tfl.quantize) shape_signature:[-1, 1, 7680, 1], type:INT8\n",
            "  T#71(double_cnn_bi_lstm_4/conv1d_32/Conv1D/ExpandDims) shape_signature:[-1, 1, 1, 7680, 1], type:INT8\n",
            "  T#72(double_cnn_bi_lstm_4/conv1d_32/Conv1D/Reshape) shape_signature:[-1, 1, 7680, 1], type:INT8\n",
            "  T#73(double_cnn_bi_lstm_4/conv1d_32/Conv1D/Conv2D1) shape_signature:[-1, 1, 1280, 64], type:INT8\n",
            "  T#74(double_cnn_bi_lstm_4/conv1d_32/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#75(double_cnn_bi_lstm_4/conv1d_32/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#76(double_cnn_bi_lstm_4/conv1d_32/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#77(double_cnn_bi_lstm_4/conv1d_32/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 1280, 64], type:INT8\n",
            "  T#78(double_cnn_bi_lstm_4/conv1d_32/Conv1D/Squeeze) shape_signature:[-1, -1, 1280, 64], type:INT8\n",
            "  T#79(double_cnn_bi_lstm_4/batch_normalization_32/FusedBatchNormV32) shape_signature:[-1, -1, 1280, 64], type:INT8\n",
            "  T#80(double_cnn_bi_lstm_4/re_lu_32/Relu;double_cnn_bi_lstm_4/batch_normalization_32/FusedBatchNormV3) shape_signature:[-1, -1, 1280, 64], type:INT8\n",
            "  T#81(double_cnn_bi_lstm_4/max_pooling2d_16/MaxPool) shape_signature:[-1, -1, 160, 64], type:INT8\n",
            "  T#82(double_cnn_bi_lstm_4/conv1d_33/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 160, 64], type:INT8\n",
            "  T#83(double_cnn_bi_lstm_4/conv1d_33/Conv1D/Reshape) shape_signature:[-1, 1, 160, 64], type:INT8\n",
            "  T#84(double_cnn_bi_lstm_4/conv1d_33/Conv1D/Conv2D1) shape_signature:[-1, 1, 160, 128], type:INT8\n",
            "  T#85(double_cnn_bi_lstm_4/conv1d_33/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#86(double_cnn_bi_lstm_4/conv1d_33/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#87(double_cnn_bi_lstm_4/conv1d_33/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#88(double_cnn_bi_lstm_4/conv1d_33/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 160, 128], type:INT8\n",
            "  T#89(double_cnn_bi_lstm_4/conv1d_33/Conv1D/Squeeze) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#90(double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#91(double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#92(double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#93(double_cnn_bi_lstm_4/conv1d_33/Relu;double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/Reshape_1;double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/BiasAdd;double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/Reshape/shape;double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/Reshape;double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#94(double_cnn_bi_lstm_4/conv1d_33/Relu;double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#95(double_cnn_bi_lstm_4/batch_normalization_33/FusedBatchNormV32) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#96(double_cnn_bi_lstm_4/re_lu_33/Relu;double_cnn_bi_lstm_4/batch_normalization_33/FusedBatchNormV3) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#97(double_cnn_bi_lstm_4/conv1d_34/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 160, 128], type:INT8\n",
            "  T#98(double_cnn_bi_lstm_4/conv1d_34/Conv1D/Reshape) shape_signature:[-1, 1, 160, 128], type:INT8\n",
            "  T#99(double_cnn_bi_lstm_4/conv1d_34/Conv1D/Conv2D1) shape_signature:[-1, 1, 160, 128], type:INT8\n",
            "  T#100(double_cnn_bi_lstm_4/conv1d_34/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#101(double_cnn_bi_lstm_4/conv1d_34/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#102(double_cnn_bi_lstm_4/conv1d_34/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#103(double_cnn_bi_lstm_4/conv1d_34/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 160, 128], type:INT8\n",
            "  T#104(double_cnn_bi_lstm_4/conv1d_34/Conv1D/Squeeze) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#105(double_cnn_bi_lstm_4/conv1d_34/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#106(double_cnn_bi_lstm_4/conv1d_34/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#107(double_cnn_bi_lstm_4/conv1d_34/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#108(double_cnn_bi_lstm_4/conv1d_34/Relu;double_cnn_bi_lstm_4/conv1d_34/squeeze_batch_dims/Reshape_1;double_cnn_bi_lstm_4/conv1d_34/squeeze_batch_dims/BiasAdd;double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/Reshape/shape;double_cnn_bi_lstm_4/conv1d_34/squeeze_batch_dims/Reshape;double_cnn_bi_lstm_4/conv1d_34/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#109(double_cnn_bi_lstm_4/conv1d_34/Relu;double_cnn_bi_lstm_4/conv1d_34/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#110(double_cnn_bi_lstm_4/batch_normalization_34/FusedBatchNormV32) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#111(double_cnn_bi_lstm_4/re_lu_34/Relu;double_cnn_bi_lstm_4/batch_normalization_34/FusedBatchNormV3) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#112(double_cnn_bi_lstm_4/conv1d_35/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 160, 128], type:INT8\n",
            "  T#113(double_cnn_bi_lstm_4/conv1d_35/Conv1D/Reshape) shape_signature:[-1, 1, 160, 128], type:INT8\n",
            "  T#114(double_cnn_bi_lstm_4/conv1d_35/Conv1D/Conv2D1) shape_signature:[-1, 1, 160, 128], type:INT8\n",
            "  T#115(double_cnn_bi_lstm_4/conv1d_35/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#116(double_cnn_bi_lstm_4/conv1d_35/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#117(double_cnn_bi_lstm_4/conv1d_35/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#118(double_cnn_bi_lstm_4/conv1d_35/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 160, 128], type:INT8\n",
            "  T#119(double_cnn_bi_lstm_4/conv1d_35/Conv1D/Squeeze) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#120(double_cnn_bi_lstm_4/conv1d_35/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#121(double_cnn_bi_lstm_4/conv1d_35/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#122(double_cnn_bi_lstm_4/conv1d_35/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#123(double_cnn_bi_lstm_4/conv1d_35/Relu;double_cnn_bi_lstm_4/conv1d_35/squeeze_batch_dims/Reshape_1;double_cnn_bi_lstm_4/conv1d_35/squeeze_batch_dims/BiasAdd;double_cnn_bi_lstm_4/conv1d_33/squeeze_batch_dims/Reshape/shape;double_cnn_bi_lstm_4/conv1d_35/squeeze_batch_dims/Reshape;double_cnn_bi_lstm_4/conv1d_35/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#124(double_cnn_bi_lstm_4/conv1d_35/Relu;double_cnn_bi_lstm_4/conv1d_35/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#125(double_cnn_bi_lstm_4/batch_normalization_35/FusedBatchNormV32) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#126(double_cnn_bi_lstm_4/re_lu_35/Relu;double_cnn_bi_lstm_4/batch_normalization_35/FusedBatchNormV3) shape_signature:[-1, -1, 160, 128], type:INT8\n",
            "  T#127(double_cnn_bi_lstm_4/max_pooling2d_17/MaxPool) shape_signature:[-1, -1, 40, 128], type:INT8\n",
            "  T#128(double_cnn_bi_lstm_4/flatten_8/Reshape) shape_signature:[-1, 5120], type:INT8\n",
            "  T#129(double_cnn_bi_lstm_4/conv1d_36/Conv1D/Conv2D21) shape_signature:[-1, 1, 154, 64], type:INT8\n",
            "  T#130(double_cnn_bi_lstm_4/conv1d_36/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#131(double_cnn_bi_lstm_4/conv1d_36/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 154, 64], type:INT8\n",
            "  T#132(double_cnn_bi_lstm_4/conv1d_36/Conv1D/Squeeze) shape_signature:[-1, -1, 154, 64], type:INT8\n",
            "  T#133(double_cnn_bi_lstm_4/batch_normalization_36/FusedBatchNormV32) shape_signature:[-1, -1, 154, 64], type:INT8\n",
            "  T#134(double_cnn_bi_lstm_4/re_lu_36/Relu;double_cnn_bi_lstm_4/batch_normalization_36/FusedBatchNormV3) shape_signature:[-1, -1, 154, 64], type:INT8\n",
            "  T#135(double_cnn_bi_lstm_4/max_pooling2d_18/MaxPool) shape_signature:[-1, -1, 39, 64], type:INT8\n",
            "  T#136(double_cnn_bi_lstm_4/conv1d_37/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 39, 64], type:INT8\n",
            "  T#137(double_cnn_bi_lstm_4/conv1d_37/Conv1D/Reshape) shape_signature:[-1, 1, 39, 64], type:INT8\n",
            "  T#138(double_cnn_bi_lstm_4/conv1d_37/Conv1D/Conv2D1) shape_signature:[-1, 1, 39, 128], type:INT8\n",
            "  T#139(double_cnn_bi_lstm_4/conv1d_37/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#140(double_cnn_bi_lstm_4/conv1d_37/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#141(double_cnn_bi_lstm_4/conv1d_37/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#142(double_cnn_bi_lstm_4/conv1d_37/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 39, 128], type:INT8\n",
            "  T#143(double_cnn_bi_lstm_4/conv1d_37/Conv1D/Squeeze) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#144(double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#145(double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#146(double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#147(double_cnn_bi_lstm_4/conv1d_37/Relu;double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/Reshape_1;double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/BiasAdd;double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/Reshape/shape;double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/Reshape;double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#148(double_cnn_bi_lstm_4/conv1d_37/Relu;double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#149(double_cnn_bi_lstm_4/batch_normalization_37/FusedBatchNormV32) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#150(double_cnn_bi_lstm_4/re_lu_37/Relu;double_cnn_bi_lstm_4/batch_normalization_37/FusedBatchNormV3) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#151(double_cnn_bi_lstm_4/conv1d_38/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 39, 128], type:INT8\n",
            "  T#152(double_cnn_bi_lstm_4/conv1d_38/Conv1D/Reshape) shape_signature:[-1, 1, 39, 128], type:INT8\n",
            "  T#153(double_cnn_bi_lstm_4/conv1d_38/Conv1D/Conv2D1) shape_signature:[-1, 1, 39, 128], type:INT8\n",
            "  T#154(double_cnn_bi_lstm_4/conv1d_38/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#155(double_cnn_bi_lstm_4/conv1d_38/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#156(double_cnn_bi_lstm_4/conv1d_38/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#157(double_cnn_bi_lstm_4/conv1d_38/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 39, 128], type:INT8\n",
            "  T#158(double_cnn_bi_lstm_4/conv1d_38/Conv1D/Squeeze) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#159(double_cnn_bi_lstm_4/conv1d_38/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#160(double_cnn_bi_lstm_4/conv1d_38/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#161(double_cnn_bi_lstm_4/conv1d_38/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#162(double_cnn_bi_lstm_4/conv1d_38/Relu;double_cnn_bi_lstm_4/conv1d_38/squeeze_batch_dims/Reshape_1;double_cnn_bi_lstm_4/conv1d_38/squeeze_batch_dims/BiasAdd;double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/Reshape/shape;double_cnn_bi_lstm_4/conv1d_38/squeeze_batch_dims/Reshape;double_cnn_bi_lstm_4/conv1d_38/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#163(double_cnn_bi_lstm_4/conv1d_38/Relu;double_cnn_bi_lstm_4/conv1d_38/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#164(double_cnn_bi_lstm_4/batch_normalization_38/FusedBatchNormV32) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#165(double_cnn_bi_lstm_4/re_lu_38/Relu;double_cnn_bi_lstm_4/batch_normalization_38/FusedBatchNormV3) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#166(double_cnn_bi_lstm_4/conv1d_39/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 39, 128], type:INT8\n",
            "  T#167(double_cnn_bi_lstm_4/conv1d_39/Conv1D/Reshape) shape_signature:[-1, 1, 39, 128], type:INT8\n",
            "  T#168(double_cnn_bi_lstm_4/conv1d_39/Conv1D/Conv2D21) shape_signature:[-1, 1, 39, 128], type:INT8\n",
            "  T#169(double_cnn_bi_lstm_4/conv1d_39/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#170(double_cnn_bi_lstm_4/conv1d_39/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#171(double_cnn_bi_lstm_4/conv1d_39/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#172(double_cnn_bi_lstm_4/conv1d_39/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 39, 128], type:INT8\n",
            "  T#173(double_cnn_bi_lstm_4/conv1d_39/Conv1D/Squeeze) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#174(double_cnn_bi_lstm_4/conv1d_39/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#175(double_cnn_bi_lstm_4/conv1d_39/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#176(double_cnn_bi_lstm_4/conv1d_39/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#177(double_cnn_bi_lstm_4/conv1d_39/Relu;double_cnn_bi_lstm_4/conv1d_39/squeeze_batch_dims/Reshape_1;double_cnn_bi_lstm_4/conv1d_39/squeeze_batch_dims/BiasAdd;double_cnn_bi_lstm_4/conv1d_37/squeeze_batch_dims/Reshape/shape;double_cnn_bi_lstm_4/conv1d_39/squeeze_batch_dims/Reshape;double_cnn_bi_lstm_4/conv1d_39/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#178(double_cnn_bi_lstm_4/conv1d_39/Relu;double_cnn_bi_lstm_4/conv1d_39/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#179(double_cnn_bi_lstm_4/batch_normalization_39/FusedBatchNormV32) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#180(double_cnn_bi_lstm_4/re_lu_39/Relu;double_cnn_bi_lstm_4/batch_normalization_39/FusedBatchNormV3) shape_signature:[-1, -1, 39, 128], type:INT8\n",
            "  T#181(double_cnn_bi_lstm_4/max_pooling2d_19/MaxPool) shape_signature:[-1, -1, 20, 128], type:INT8\n",
            "  T#182(double_cnn_bi_lstm_4/flatten_8/Reshape_1) shape_signature:[-1, 2560], type:INT8\n",
            "  T#183(double_cnn_bi_lstm_4/concatenate/concat) shape_signature:[-1, 7680], type:INT8\n",
            "  T#184(double_cnn_bi_lstm_4/reshape/Shape) shape:[2], type:INT32\n",
            "  T#185(double_cnn_bi_lstm_4/reshape/strided_slice) shape:[], type:INT32\n",
            "  T#186(double_cnn_bi_lstm_4/reshape/Reshape/shape) shape:[3], type:INT32\n",
            "  T#187(double_cnn_bi_lstm_4/reshape/Reshape) shape_signature:[-1, 60, 128], type:INT8\n",
            "  T#188(double_cnn_bi_lstm_4/bidirectional_8/backward_lstm_8/Shape) shape:[3], type:INT32\n",
            "  T#189(double_cnn_bi_lstm_4/bidirectional_8/backward_lstm_8/strided_slice) shape:[], type:INT32\n",
            "  T#190(double_cnn_bi_lstm_4/bidirectional_8/backward_lstm_8/zeros/packed) shape:[2], type:INT32\n",
            "  T#191(double_cnn_bi_lstm_4/bidirectional_8/backward_lstm_8/zeros) shape_signature:[-1, 128], type:INT8\n",
            "  T#192(tfl.dequantize) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#193(transpose) shape_signature:[60, -1, 128], type:INT8\n",
            "  T#194(transpose1) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#195(ReverseV2) shape_signature:[60, -1, 128], type:INT8\n",
            "  T#196(tfl.dequantize1) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#197(while;while1;while2;while3;while4;while5) shape:[], type:INT32\n",
            "  T#198(while;while1;while2;while3;while4;while51) shape:[], type:INT32\n",
            "  T#199(while;while1;while2;while3;while4;while52) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#200(while;while1;while2;while3;while4;while53) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#201(while;while1;while2;while3;while4;while54) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#202(while;while1;while2;while3;while4;while55) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#203(tfl.quantize1) shape_signature:[60, -1, 128], type:INT8\n",
            "  T#204(transpose_1) shape_signature:[-1, 60, 128], type:INT8\n",
            "  T#205(double_cnn_bi_lstm_4/bidirectional_8/ReverseV2) shape_signature:[-1, 60, 128], type:INT8\n",
            "  T#206(while6;while7;while8;while9;while10;while11) shape:[], type:INT32\n",
            "  T#207(while6;while7;while8;while9;while10;while111) shape:[], type:INT32\n",
            "  T#208(while6;while7;while8;while9;while10;while112) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#209(while6;while7;while8;while9;while10;while113) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#210(while6;while7;while8;while9;while10;while114) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#211(while6;while7;while8;while9;while10;while115) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#212(transpose_11) shape_signature:[-1, 60, 128], type:FLOAT32\n",
            "  T#213(transpose_111) shape_signature:[-1, 60, 128], type:INT8\n",
            "  T#214(double_cnn_bi_lstm_4/bidirectional_8/concat) shape_signature:[-1, 60, 256], type:INT8\n",
            "  T#215(double_cnn_bi_lstm_4/bidirectional_9/backward_lstm_9/Shape) shape:[3], type:INT32\n",
            "  T#216(double_cnn_bi_lstm_4/bidirectional_9/backward_lstm_9/strided_slice) shape:[], type:INT32\n",
            "  T#217(double_cnn_bi_lstm_4/bidirectional_9/backward_lstm_9/zeros/packed) shape:[2], type:INT32\n",
            "  T#218(double_cnn_bi_lstm_4/bidirectional_9/backward_lstm_9/zeros) shape_signature:[-1, 128], type:INT8\n",
            "  T#219(tfl.dequantize2) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#220(transpose11) shape_signature:[60, -1, 256], type:INT8\n",
            "  T#221(transpose12) shape_signature:[60, -1, 256], type:FLOAT32\n",
            "  T#222(ReverseV21) shape_signature:[60, -1, 256], type:INT8\n",
            "  T#223(tfl.dequantize3) shape_signature:[60, -1, 256], type:FLOAT32\n",
            "  T#224(while12;while13;while14;while15;while16;while17) shape:[], type:INT32\n",
            "  T#225(while12;while13;while14;while15;while16;while171) shape:[], type:INT32\n",
            "  T#226(while12;while13;while14;while15;while16;while172) shape_signature:[1, -1, 128], type:FLOAT32\n",
            "  T#227(while12;while13;while14;while15;while16;while173) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#228(while12;while13;while14;while15;while16;while174) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#229(while12;while13;while14;while15;while16;while175) shape_signature:[60, -1, 256], type:FLOAT32\n",
            "  T#230(strided_slice_23) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#231(strided_slice_231) shape_signature:[-1, 128], type:INT8\n",
            "  T#232(while18;while19;while20;while21;while22;while23) shape:[], type:INT32\n",
            "  T#233(while18;while19;while20;while21;while22;while231) shape:[], type:INT32\n",
            "  T#234(while18;while19;while20;while21;while22;while232) shape_signature:[1, -1, 128], type:FLOAT32\n",
            "  T#235(while18;while19;while20;while21;while22;while233) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#236(while18;while19;while20;while21;while22;while234) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#237(while18;while19;while20;while21;while22;while235) shape_signature:[60, -1, 256], type:FLOAT32\n",
            "  T#238(strided_slice_24) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#239(strided_slice_241) shape_signature:[-1, 128], type:INT8\n",
            "  T#240(double_cnn_bi_lstm_4/bidirectional_9/concat) shape_signature:[-1, 256], type:INT8\n",
            "  T#241(double_cnn_bi_lstm_4/dense_4/MatMul;double_cnn_bi_lstm_4/dense_4/BiasAdd) shape_signature:[-1, 5], type:INT8\n",
            "  T#242(StatefulPartitionedCall:01) shape_signature:[-1, 5], type:INT8\n",
            "  T#243(StatefulPartitionedCall:0) shape_signature:[-1, 5], type:FLOAT32\n",
            "\n",
            "Subgraph#1 while_cond(T#1_0, T#1_1, T#1_2, T#1_3, T#1_4, T#1_5) -> [T#1_7]\n",
            "  Op#0 LESS(T#1_1, T#1_6[60]) -> [T#1_7]\n",
            "\n",
            "Tensors of Subgraph#1\n",
            "  T#1_0(arg0) shape:[], type:INT32\n",
            "  T#1_1(arg1) shape:[], type:INT32\n",
            "  T#1_2(arg2) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#1_3(arg3) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#1_4(arg4) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#1_5(arg5) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#1_6(double_cnn_bi_lstm_4/reshape/Reshape/shape/11) shape:[], type:INT32 RO 4 bytes, buffer: 22, data:[60]\n",
            "  T#1_7(while/Less) shape:[], type:BOOL\n",
            "\n",
            "Subgraph#2 while_body(T#2_0, T#2_1, T#2_2, T#2_3, T#2_4, T#2_5) -> [T#2_50, T#2_20, T#2_49, T#2_40, T#2_37, T#2_5]\n",
            "  Op#0 QUANTIZE(T#2_2) -> [T#2_13]\n",
            "  Op#1 QUANTIZE(T#2_3) -> [T#2_17]\n",
            "  Op#2 QUANTIZE(T#2_4) -> [T#2_18]\n",
            "  Op#3 QUANTIZE(T#2_5) -> [T#2_19]\n",
            "  Op#4 ADD(T#2_1, T#2_6[1]) -> [T#2_20]\n",
            "  Op#5 FULLY_CONNECTED(T#2_17, T#2_16, T#-1) -> [T#2_21]\n",
            "  Op#6 GATHER(T#2_19, T#2_1) -> [T#2_22]\n",
            "  Op#7 FULLY_CONNECTED(T#2_22, T#2_15, T#-1) -> [T#2_23]\n",
            "  Op#8 ADD(T#2_23, T#2_21) -> [T#2_24]\n",
            "  Op#9 ADD(T#2_24, T#2_14) -> [T#2_25]\n",
            "  Op#10 SPLIT(T#2_6[1], T#2_25) -> [T#2_26, T#2_27, T#2_28, T#2_29]\n",
            "  Op#11 LOGISTIC(T#2_26) -> [T#2_30]\n",
            "  Op#12 LOGISTIC(T#2_27) -> [T#2_31]\n",
            "  Op#13 MUL(T#2_31, T#2_18) -> [T#2_32]\n",
            "  Op#14 LOGISTIC(T#2_29) -> [T#2_33]\n",
            "  Op#15 TANH(T#2_28) -> [T#2_34]\n",
            "  Op#16 MUL(T#2_30, T#2_34) -> [T#2_35]\n",
            "  Op#17 ADD(T#2_32, T#2_35) -> [T#2_36]\n",
            "  Op#18 DEQUANTIZE(T#2_36) -> [T#2_37]\n",
            "  Op#19 TANH(T#2_36) -> [T#2_38]\n",
            "  Op#20 MUL(T#2_33, T#2_38) -> [T#2_39]\n",
            "  Op#21 DEQUANTIZE(T#2_39) -> [T#2_40]\n",
            "  Op#22 RESHAPE(T#2_1, T#2_7[1]) -> [T#2_41]\n",
            "  Op#23 CONCATENATION(T#2_41, T#2_8[-1, -1]) -> [T#2_42]\n",
            "  Op#24 SLICE(T#2_13, T#2_9[0, 0, 0], T#2_42) -> [T#2_43]\n",
            "  Op#25 RESHAPE(T#2_20, T#2_7[1]) -> [T#2_44]\n",
            "  Op#26 CONCATENATION(T#2_44, T#2_10[0, 0]) -> [T#2_45]\n",
            "  Op#27 SLICE(T#2_13, T#2_45, T#2_11[-1, -1, -1]) -> [T#2_46]\n",
            "  Op#28 EXPAND_DIMS(T#2_39, T#2_12[0]) -> [T#2_47]\n",
            "  Op#29 CONCATENATION(T#2_43, T#2_47, T#2_46) -> [T#2_48]\n",
            "  Op#30 DEQUANTIZE(T#2_48) -> [T#2_49]\n",
            "  Op#31 ADD(T#2_0, T#2_6[1]) -> [T#2_50]\n",
            "\n",
            "Tensors of Subgraph#2\n",
            "  T#2_0(arg0) shape:[], type:INT32\n",
            "  T#2_1(arg1) shape:[], type:INT32\n",
            "  T#2_2(arg2) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#2_3(arg3) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#2_4(arg4) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#2_5(arg5) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#2_6(double_cnn_bi_lstm_4/bidirectional_9/concat/axis1) shape:[], type:INT32 RO 4 bytes, buffer: 8, data:[1]\n",
            "  T#2_7(double_cnn_bi_lstm_4/bidirectional_8/ReverseV2/axis1) shape:[1], type:INT32 RO 4 bytes, buffer: 8, data:[1]\n",
            "  T#2_8(while/TensorArrayV2Write/TensorListSetItem1) shape:[2], type:INT32 RO 8 bytes, buffer: 261, data:[-1, -1]\n",
            "  T#2_9(while/TensorArrayV2Write/TensorListSetItem3) shape:[3], type:INT32 RO 12 bytes, buffer: 262, data:[0, 0, 0]\n",
            "  T#2_10(while/TensorArrayV2Write/TensorListSetItem2) shape:[2], type:INT32 RO 8 bytes, buffer: 263, data:[0, 0]\n",
            "  T#2_11(while/TensorArrayV2Write/TensorListSetItem) shape:[3], type:INT32 RO 12 bytes, buffer: 264, data:[-1, -1, -1]\n",
            "  T#2_12(time1) shape:[], type:INT32 RO 4 bytes, buffer: 6, data:[0]\n",
            "  T#2_13(tfl.quantize2) shape_signature:[60, -1, 128], type:INT8\n",
            "  T#2_14(double_cnn_bi_lstm_4/bidirectional_8/backward_lstm_8/Read_2/ReadVariableOp1) shape:[512], type:INT8 RO 512 bytes, buffer: 267, data:[., ., ., ., ., ...]\n",
            "  T#2_15(while/MatMul) shape:[512, 128], type:INT8 RO 65536 bytes, buffer: 268, data:[., ., ., ., ., ...]\n",
            "  T#2_16(while/MatMul_11) shape:[512, 128], type:INT8 RO 65536 bytes, buffer: 269, data:[., ., ., ., ., ...]\n",
            "  T#2_17(tfl.quantize3) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_18(tfl.quantize4) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_19(tfl.quantize5) shape_signature:[60, -1, 128], type:INT8\n",
            "  T#2_20(while/add_2) shape:[], type:INT32\n",
            "  T#2_21(while/MatMul_12) shape_signature:[-1, 512], type:INT8\n",
            "  T#2_22(while/TensorArrayV2Read/TensorListGetItem;while/TensorArrayV2Write/TensorListSetItem) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_23(while/MatMul1) shape_signature:[-1, 512], type:INT8\n",
            "  T#2_24(while/add) shape_signature:[-1, 512], type:INT8\n",
            "  T#2_25(while/BiasAdd) shape_signature:[-1, 512], type:INT8\n",
            "  T#2_26(while/split;while/split1;while/split2;while/split3) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_27(while/split;while/split1;while/split2;while/split31) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_28(while/split;while/split1;while/split2;while/split32) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_29(while/split;while/split1;while/split2;while/split33) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_30(while/Sigmoid) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_31(while/Sigmoid_1) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_32(while/mul) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_33(while/Sigmoid_2) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_34(while/Tanh) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_35(while/mul_1) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_36(while/add_1) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_37(tfl.dequantize4) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#2_38(while/Tanh_1) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_39(while/mul_2) shape_signature:[-1, 128], type:INT8\n",
            "  T#2_40(while/mul_21) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#2_41(while/TensorArrayV2Write/TensorListSetItem4) shape:[1], type:INT32\n",
            "  T#2_42(while/TensorArrayV2Write/TensorListSetItem5) shape:[3], type:INT32\n",
            "  T#2_43(while/TensorArrayV2Write/TensorListSetItem6) shape_signature:[-1, -1, -1], type:INT8\n",
            "  T#2_44(while/TensorArrayV2Write/TensorListSetItem7) shape:[1], type:INT32\n",
            "  T#2_45(while/TensorArrayV2Write/TensorListSetItem8) shape:[3], type:INT32\n",
            "  T#2_46(while/TensorArrayV2Write/TensorListSetItem9) shape_signature:[-1, -1, 128], type:INT8\n",
            "  T#2_47(while/TensorArrayV2Write/TensorListSetItem10) shape_signature:[1, -1, 128], type:INT8\n",
            "  T#2_48(while/TensorArrayV2Write/TensorListSetItem11) shape_signature:[60, -1, 128], type:INT8\n",
            "  T#2_49(tfl.dequantize5) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#2_50(while/add_3) shape:[], type:INT32\n",
            "\n",
            "Subgraph#3 while1_cond(T#3_0, T#3_1, T#3_2, T#3_3, T#3_4, T#3_5) -> [T#3_7]\n",
            "  Op#0 LESS(T#3_1, T#3_6[60]) -> [T#3_7]\n",
            "\n",
            "Tensors of Subgraph#3\n",
            "  T#3_0(arg0) shape:[], type:INT32\n",
            "  T#3_1(arg1) shape:[], type:INT32\n",
            "  T#3_2(arg2) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#3_3(arg3) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#3_4(arg4) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#3_5(arg5) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#3_6(double_cnn_bi_lstm_4/reshape/Reshape/shape/12) shape:[], type:INT32 RO 4 bytes, buffer: 22, data:[60]\n",
            "  T#3_7(while/Less1) shape:[], type:BOOL\n",
            "\n",
            "Subgraph#4 while1_body(T#4_0, T#4_1, T#4_2, T#4_3, T#4_4, T#4_5) -> [T#4_50, T#4_20, T#4_49, T#4_40, T#4_37, T#4_5]\n",
            "  Op#0 QUANTIZE(T#4_2) -> [T#4_13]\n",
            "  Op#1 QUANTIZE(T#4_3) -> [T#4_17]\n",
            "  Op#2 QUANTIZE(T#4_4) -> [T#4_18]\n",
            "  Op#3 QUANTIZE(T#4_5) -> [T#4_19]\n",
            "  Op#4 ADD(T#4_1, T#4_6[1]) -> [T#4_20]\n",
            "  Op#5 FULLY_CONNECTED(T#4_17, T#4_16, T#-1) -> [T#4_21]\n",
            "  Op#6 GATHER(T#4_19, T#4_1) -> [T#4_22]\n",
            "  Op#7 FULLY_CONNECTED(T#4_22, T#4_15, T#-1) -> [T#4_23]\n",
            "  Op#8 ADD(T#4_23, T#4_21) -> [T#4_24]\n",
            "  Op#9 ADD(T#4_24, T#4_14) -> [T#4_25]\n",
            "  Op#10 SPLIT(T#4_6[1], T#4_25) -> [T#4_26, T#4_27, T#4_28, T#4_29]\n",
            "  Op#11 LOGISTIC(T#4_26) -> [T#4_30]\n",
            "  Op#12 LOGISTIC(T#4_27) -> [T#4_31]\n",
            "  Op#13 MUL(T#4_31, T#4_18) -> [T#4_32]\n",
            "  Op#14 LOGISTIC(T#4_29) -> [T#4_33]\n",
            "  Op#15 TANH(T#4_28) -> [T#4_34]\n",
            "  Op#16 MUL(T#4_30, T#4_34) -> [T#4_35]\n",
            "  Op#17 ADD(T#4_32, T#4_35) -> [T#4_36]\n",
            "  Op#18 DEQUANTIZE(T#4_36) -> [T#4_37]\n",
            "  Op#19 TANH(T#4_36) -> [T#4_38]\n",
            "  Op#20 MUL(T#4_33, T#4_38) -> [T#4_39]\n",
            "  Op#21 DEQUANTIZE(T#4_39) -> [T#4_40]\n",
            "  Op#22 RESHAPE(T#4_1, T#4_7[1]) -> [T#4_41]\n",
            "  Op#23 CONCATENATION(T#4_41, T#4_8[-1, -1]) -> [T#4_42]\n",
            "  Op#24 SLICE(T#4_13, T#4_9[0, 0, 0], T#4_42) -> [T#4_43]\n",
            "  Op#25 RESHAPE(T#4_20, T#4_7[1]) -> [T#4_44]\n",
            "  Op#26 CONCATENATION(T#4_44, T#4_10[0, 0]) -> [T#4_45]\n",
            "  Op#27 SLICE(T#4_13, T#4_45, T#4_11[-1, -1, -1]) -> [T#4_46]\n",
            "  Op#28 EXPAND_DIMS(T#4_39, T#4_12[0]) -> [T#4_47]\n",
            "  Op#29 CONCATENATION(T#4_43, T#4_47, T#4_46) -> [T#4_48]\n",
            "  Op#30 DEQUANTIZE(T#4_48) -> [T#4_49]\n",
            "  Op#31 ADD(T#4_0, T#4_6[1]) -> [T#4_50]\n",
            "\n",
            "Tensors of Subgraph#4\n",
            "  T#4_0(arg0) shape:[], type:INT32\n",
            "  T#4_1(arg1) shape:[], type:INT32\n",
            "  T#4_2(arg2) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#4_3(arg3) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#4_4(arg4) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#4_5(arg5) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#4_6(double_cnn_bi_lstm_4/bidirectional_9/concat/axis2) shape:[], type:INT32 RO 4 bytes, buffer: 8, data:[1]\n",
            "  T#4_7(double_cnn_bi_lstm_4/bidirectional_8/ReverseV2/axis2) shape:[1], type:INT32 RO 4 bytes, buffer: 8, data:[1]\n",
            "  T#4_8(while/TensorArrayV2Write/TensorListSetItem13) shape:[2], type:INT32 RO 8 bytes, buffer: 261, data:[-1, -1]\n",
            "  T#4_9(while/TensorArrayV2Write/TensorListSetItem15) shape:[3], type:INT32 RO 12 bytes, buffer: 262, data:[0, 0, 0]\n",
            "  T#4_10(while/TensorArrayV2Write/TensorListSetItem14) shape:[2], type:INT32 RO 8 bytes, buffer: 263, data:[0, 0]\n",
            "  T#4_11(while/TensorArrayV2Write/TensorListSetItem12) shape:[3], type:INT32 RO 12 bytes, buffer: 264, data:[-1, -1, -1]\n",
            "  T#4_12(time2) shape:[], type:INT32 RO 4 bytes, buffer: 6, data:[0]\n",
            "  T#4_13(tfl.quantize6) shape_signature:[60, -1, 128], type:INT8\n",
            "  T#4_14(double_cnn_bi_lstm_4/bidirectional_8/forward_lstm_8/Read_2/ReadVariableOp1) shape:[512], type:INT8 RO 512 bytes, buffer: 326, data:[., ., ., ., ., ...]\n",
            "  T#4_15(while/MatMul2) shape:[512, 128], type:INT8 RO 65536 bytes, buffer: 327, data:[., ., ., ., ., ...]\n",
            "  T#4_16(while/MatMul_14) shape:[512, 128], type:INT8 RO 65536 bytes, buffer: 328, data:[., ., ., \n",
            ", ., ...]\n",
            "  T#4_17(tfl.quantize7) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_18(tfl.quantize8) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_19(tfl.quantize9) shape_signature:[60, -1, 128], type:INT8\n",
            "  T#4_20(while/add_21) shape:[], type:INT32\n",
            "  T#4_21(while/MatMul_15) shape_signature:[-1, 512], type:INT8\n",
            "  T#4_22(while/TensorArrayV2Read/TensorListGetItem;while/TensorArrayV2Write/TensorListSetItem1) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_23(while/MatMul3) shape_signature:[-1, 512], type:INT8\n",
            "  T#4_24(while/add1) shape_signature:[-1, 512], type:INT8\n",
            "  T#4_25(while/BiasAdd1) shape_signature:[-1, 512], type:INT8\n",
            "  T#4_26(while/split4;while/split5;while/split6;while/split7) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_27(while/split4;while/split5;while/split6;while/split71) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_28(while/split4;while/split5;while/split6;while/split72) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_29(while/split4;while/split5;while/split6;while/split73) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_30(while/Sigmoid1) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_31(while/Sigmoid_11) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_32(while/mul1) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_33(while/Sigmoid_21) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_34(while/Tanh1) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_35(while/mul_11) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_36(while/add_11) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_37(tfl.dequantize6) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#4_38(while/Tanh_11) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_39(while/mul_211) shape_signature:[-1, 128], type:INT8\n",
            "  T#4_40(while/mul_212) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#4_41(while/TensorArrayV2Write/TensorListSetItem16) shape:[1], type:INT32\n",
            "  T#4_42(while/TensorArrayV2Write/TensorListSetItem17) shape:[3], type:INT32\n",
            "  T#4_43(while/TensorArrayV2Write/TensorListSetItem18) shape_signature:[-1, -1, -1], type:INT8\n",
            "  T#4_44(while/TensorArrayV2Write/TensorListSetItem19) shape:[1], type:INT32\n",
            "  T#4_45(while/TensorArrayV2Write/TensorListSetItem20) shape:[3], type:INT32\n",
            "  T#4_46(while/TensorArrayV2Write/TensorListSetItem21) shape_signature:[-1, -1, 128], type:INT8\n",
            "  T#4_47(while/TensorArrayV2Write/TensorListSetItem22) shape_signature:[1, -1, 128], type:INT8\n",
            "  T#4_48(while/TensorArrayV2Write/TensorListSetItem23) shape_signature:[60, -1, 128], type:INT8\n",
            "  T#4_49(tfl.dequantize7) shape_signature:[60, -1, 128], type:FLOAT32\n",
            "  T#4_50(while/add_31) shape:[], type:INT32\n",
            "\n",
            "Subgraph#5 while2_cond(T#5_0, T#5_1, T#5_2, T#5_3, T#5_4, T#5_5) -> [T#5_7]\n",
            "  Op#0 LESS(T#5_1, T#5_6[60]) -> [T#5_7]\n",
            "\n",
            "Tensors of Subgraph#5\n",
            "  T#5_0(arg0) shape:[], type:INT32\n",
            "  T#5_1(arg1) shape:[], type:INT32\n",
            "  T#5_2(arg2) shape_signature:[1, -1, 128], type:FLOAT32\n",
            "  T#5_3(arg3) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#5_4(arg4) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#5_5(arg5) shape_signature:[60, -1, 256], type:FLOAT32\n",
            "  T#5_6(double_cnn_bi_lstm_4/reshape/Reshape/shape/13) shape:[], type:INT32 RO 4 bytes, buffer: 22, data:[60]\n",
            "  T#5_7(while/Less2) shape:[], type:BOOL\n",
            "\n",
            "Subgraph#6 while2_body(T#6_0, T#6_1, T#6_2, T#6_3, T#6_4, T#6_5) -> [T#6_45, T#6_19, T#6_44, T#6_39, T#6_36, T#6_5]\n",
            "  Op#0 QUANTIZE(T#6_2) -> [T#6_15]\n",
            "  Op#1 QUANTIZE(T#6_3) -> [T#6_16]\n",
            "  Op#2 QUANTIZE(T#6_4) -> [T#6_17]\n",
            "  Op#3 QUANTIZE(T#6_5) -> [T#6_18]\n",
            "  Op#4 ADD(T#6_1, T#6_6[1]) -> [T#6_19]\n",
            "  Op#5 FULLY_CONNECTED(T#6_16, T#6_14, T#-1) -> [T#6_20]\n",
            "  Op#6 GATHER(T#6_18, T#6_1) -> [T#6_21]\n",
            "  Op#7 FULLY_CONNECTED(T#6_21, T#6_13, T#-1) -> [T#6_22]\n",
            "  Op#8 ADD(T#6_22, T#6_20) -> [T#6_23]\n",
            "  Op#9 ADD(T#6_23, T#6_12) -> [T#6_24]\n",
            "  Op#10 SPLIT(T#6_6[1], T#6_24) -> [T#6_25, T#6_26, T#6_27, T#6_28]\n",
            "  Op#11 LOGISTIC(T#6_25) -> [T#6_29]\n",
            "  Op#12 LOGISTIC(T#6_26) -> [T#6_30]\n",
            "  Op#13 MUL(T#6_30, T#6_17) -> [T#6_31]\n",
            "  Op#14 LOGISTIC(T#6_28) -> [T#6_32]\n",
            "  Op#15 TANH(T#6_27) -> [T#6_33]\n",
            "  Op#16 MUL(T#6_29, T#6_33) -> [T#6_34]\n",
            "  Op#17 ADD(T#6_31, T#6_34) -> [T#6_35]\n",
            "  Op#18 DEQUANTIZE(T#6_35) -> [T#6_36]\n",
            "  Op#19 TANH(T#6_35) -> [T#6_37]\n",
            "  Op#20 MUL(T#6_32, T#6_37) -> [T#6_38]\n",
            "  Op#21 DEQUANTIZE(T#6_38) -> [T#6_39]\n",
            "  Op#22 SLICE(T#6_15, T#6_11[0, 0, 0], T#6_10[0, -1, -1]) -> [T#6_40]\n",
            "  Op#23 SLICE(T#6_15, T#6_9[1, 0, 0], T#6_8[-1, -1, -1]) -> [T#6_41]\n",
            "  Op#24 EXPAND_DIMS(T#6_38, T#6_7[0]) -> [T#6_42]\n",
            "  Op#25 CONCATENATION(T#6_40, T#6_42, T#6_41) -> [T#6_43]\n",
            "  Op#26 DEQUANTIZE(T#6_43) -> [T#6_44]\n",
            "  Op#27 ADD(T#6_0, T#6_6[1]) -> [T#6_45]\n",
            "\n",
            "Tensors of Subgraph#6\n",
            "  T#6_0(arg0) shape:[], type:INT32\n",
            "  T#6_1(arg1) shape:[], type:INT32\n",
            "  T#6_2(arg2) shape_signature:[1, -1, 128], type:FLOAT32\n",
            "  T#6_3(arg3) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#6_4(arg4) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#6_5(arg5) shape_signature:[60, -1, 256], type:FLOAT32\n",
            "  T#6_6(double_cnn_bi_lstm_4/bidirectional_9/concat/axis3) shape:[], type:INT32 RO 4 bytes, buffer: 8, data:[1]\n",
            "  T#6_7(time3) shape:[], type:INT32 RO 4 bytes, buffer: 6, data:[0]\n",
            "  T#6_8(while/TensorArrayV2Write/TensorListSetItem24) shape:[3], type:INT32 RO 12 bytes, buffer: 264, data:[-1, -1, -1]\n",
            "  T#6_9(while/TensorArrayV2Write/TensorListSetItem25) shape:[3], type:INT32 RO 12 bytes, buffer: 380, data:[1, 0, 0]\n",
            "  T#6_10(while/TensorArrayV2Write/TensorListSetItem26) shape:[3], type:INT32 RO 12 bytes, buffer: 381, data:[0, -1, -1]\n",
            "  T#6_11(while/TensorArrayV2Write/TensorListSetItem27) shape:[3], type:INT32 RO 12 bytes, buffer: 262, data:[0, 0, 0]\n",
            "  T#6_12(double_cnn_bi_lstm_4/bidirectional_9/backward_lstm_9/Read_2/ReadVariableOp1) shape:[512], type:INT8 RO 512 bytes, buffer: 383, data:[., ., ., ., ., ...]\n",
            "  T#6_13(while/MatMul4) shape:[512, 256], type:INT8 RO 131072 bytes, buffer: 384, data:[., ., ., ., ., ...]\n",
            "  T#6_14(while/MatMul_17) shape:[512, 128], type:INT8 RO 65536 bytes, buffer: 385, data:[., ., ., ., ., ...]\n",
            "  T#6_15(tfl.quantize10) shape_signature:[1, -1, 128], type:INT8\n",
            "  T#6_16(tfl.quantize11) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_17(tfl.quantize12) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_18(tfl.quantize13) shape_signature:[60, -1, 256], type:INT8\n",
            "  T#6_19(while/add_22) shape:[], type:INT32\n",
            "  T#6_20(while/MatMul_18) shape_signature:[-1, 512], type:INT8\n",
            "  T#6_21(while/TensorArrayV2Read/TensorListGetItem;time) shape_signature:[-1, 256], type:INT8\n",
            "  T#6_22(while/MatMul5) shape_signature:[-1, 512], type:INT8\n",
            "  T#6_23(while/add2) shape_signature:[-1, 512], type:INT8\n",
            "  T#6_24(while/BiasAdd2) shape_signature:[-1, 512], type:INT8\n",
            "  T#6_25(while/split8;while/split9;while/split10;while/split11) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_26(while/split8;while/split9;while/split10;while/split111) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_27(while/split8;while/split9;while/split10;while/split112) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_28(while/split8;while/split9;while/split10;while/split113) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_29(while/Sigmoid2) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_30(while/Sigmoid_12) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_31(while/mul2) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_32(while/Sigmoid_22) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_33(while/Tanh2) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_34(while/mul_12) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_35(while/add_12) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_36(tfl.dequantize8) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#6_37(while/Tanh_12) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_38(while/mul_22) shape_signature:[-1, 128], type:INT8\n",
            "  T#6_39(while/mul_221) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#6_40(while/TensorArrayV2Write/TensorListSetItem28) shape_signature:[0, -1, 128], type:INT8\n",
            "  T#6_41(while/TensorArrayV2Write/TensorListSetItem29) shape_signature:[0, -1, 128], type:INT8\n",
            "  T#6_42(while/TensorArrayV2Write/TensorListSetItem30) shape_signature:[1, -1, 128], type:INT8\n",
            "  T#6_43(while/TensorArrayV2Write/TensorListSetItem31) shape_signature:[1, -1, 128], type:INT8\n",
            "  T#6_44(tfl.dequantize9) shape_signature:[1, -1, 128], type:FLOAT32\n",
            "  T#6_45(while/add_32) shape:[], type:INT32\n",
            "\n",
            "Subgraph#7 while3_cond(T#7_0, T#7_1, T#7_2, T#7_3, T#7_4, T#7_5) -> [T#7_7]\n",
            "  Op#0 LESS(T#7_1, T#7_6[60]) -> [T#7_7]\n",
            "\n",
            "Tensors of Subgraph#7\n",
            "  T#7_0(arg0) shape:[], type:INT32\n",
            "  T#7_1(arg1) shape:[], type:INT32\n",
            "  T#7_2(arg2) shape_signature:[1, -1, 128], type:FLOAT32\n",
            "  T#7_3(arg3) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#7_4(arg4) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#7_5(arg5) shape_signature:[60, -1, 256], type:FLOAT32\n",
            "  T#7_6(double_cnn_bi_lstm_4/reshape/Reshape/shape/14) shape:[], type:INT32 RO 4 bytes, buffer: 22, data:[60]\n",
            "  T#7_7(while/Less3) shape:[], type:BOOL\n",
            "\n",
            "Subgraph#8 while3_body(T#8_0, T#8_1, T#8_2, T#8_3, T#8_4, T#8_5) -> [T#8_45, T#8_19, T#8_44, T#8_39, T#8_36, T#8_5]\n",
            "  Op#0 QUANTIZE(T#8_2) -> [T#8_15]\n",
            "  Op#1 QUANTIZE(T#8_3) -> [T#8_16]\n",
            "  Op#2 QUANTIZE(T#8_4) -> [T#8_17]\n",
            "  Op#3 QUANTIZE(T#8_5) -> [T#8_18]\n",
            "  Op#4 ADD(T#8_1, T#8_6[1]) -> [T#8_19]\n",
            "  Op#5 FULLY_CONNECTED(T#8_16, T#8_14, T#-1) -> [T#8_20]\n",
            "  Op#6 GATHER(T#8_18, T#8_1) -> [T#8_21]\n",
            "  Op#7 FULLY_CONNECTED(T#8_21, T#8_13, T#-1) -> [T#8_22]\n",
            "  Op#8 ADD(T#8_22, T#8_20) -> [T#8_23]\n",
            "  Op#9 ADD(T#8_23, T#8_12) -> [T#8_24]\n",
            "  Op#10 SPLIT(T#8_6[1], T#8_24) -> [T#8_25, T#8_26, T#8_27, T#8_28]\n",
            "  Op#11 LOGISTIC(T#8_25) -> [T#8_29]\n",
            "  Op#12 LOGISTIC(T#8_26) -> [T#8_30]\n",
            "  Op#13 MUL(T#8_30, T#8_17) -> [T#8_31]\n",
            "  Op#14 LOGISTIC(T#8_28) -> [T#8_32]\n",
            "  Op#15 TANH(T#8_27) -> [T#8_33]\n",
            "  Op#16 MUL(T#8_29, T#8_33) -> [T#8_34]\n",
            "  Op#17 ADD(T#8_31, T#8_34) -> [T#8_35]\n",
            "  Op#18 DEQUANTIZE(T#8_35) -> [T#8_36]\n",
            "  Op#19 TANH(T#8_35) -> [T#8_37]\n",
            "  Op#20 MUL(T#8_32, T#8_37) -> [T#8_38]\n",
            "  Op#21 DEQUANTIZE(T#8_38) -> [T#8_39]\n",
            "  Op#22 SLICE(T#8_15, T#8_11[0, 0, 0], T#8_10[0, -1, -1]) -> [T#8_40]\n",
            "  Op#23 SLICE(T#8_15, T#8_9[1, 0, 0], T#8_8[-1, -1, -1]) -> [T#8_41]\n",
            "  Op#24 EXPAND_DIMS(T#8_38, T#8_7[0]) -> [T#8_42]\n",
            "  Op#25 CONCATENATION(T#8_40, T#8_42, T#8_41) -> [T#8_43]\n",
            "  Op#26 DEQUANTIZE(T#8_43) -> [T#8_44]\n",
            "  Op#27 ADD(T#8_0, T#8_6[1]) -> [T#8_45]\n",
            "\n",
            "Tensors of Subgraph#8\n",
            "  T#8_0(arg0) shape:[], type:INT32\n",
            "  T#8_1(arg1) shape:[], type:INT32\n",
            "  T#8_2(arg2) shape_signature:[1, -1, 128], type:FLOAT32\n",
            "  T#8_3(arg3) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#8_4(arg4) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#8_5(arg5) shape_signature:[60, -1, 256], type:FLOAT32\n",
            "  T#8_6(double_cnn_bi_lstm_4/bidirectional_9/concat/axis4) shape:[], type:INT32 RO 4 bytes, buffer: 8, data:[1]\n",
            "  T#8_7(time4) shape:[], type:INT32 RO 4 bytes, buffer: 6, data:[0]\n",
            "  T#8_8(while/TensorArrayV2Write/TensorListSetItem32) shape:[3], type:INT32 RO 12 bytes, buffer: 264, data:[-1, -1, -1]\n",
            "  T#8_9(while/TensorArrayV2Write/TensorListSetItem33) shape:[3], type:INT32 RO 12 bytes, buffer: 380, data:[1, 0, 0]\n",
            "  T#8_10(while/TensorArrayV2Write/TensorListSetItem34) shape:[3], type:INT32 RO 12 bytes, buffer: 381, data:[0, -1, -1]\n",
            "  T#8_11(while/TensorArrayV2Write/TensorListSetItem35) shape:[3], type:INT32 RO 12 bytes, buffer: 262, data:[0, 0, 0]\n",
            "  T#8_12(double_cnn_bi_lstm_4/bidirectional_9/forward_lstm_9/Read_2/ReadVariableOp1) shape:[512], type:INT8 RO 512 bytes, buffer: 437, data:[., ., ., ., ., ...]\n",
            "  T#8_13(while/MatMul6) shape:[512, 256], type:INT8 RO 131072 bytes, buffer: 438, data:[., ., ., ., ., ...]\n",
            "  T#8_14(while/MatMul_110) shape:[512, 128], type:INT8 RO 65536 bytes, buffer: 439, data:[., D, ., ), ., ...]\n",
            "  T#8_15(tfl.quantize14) shape_signature:[1, -1, 128], type:INT8\n",
            "  T#8_16(tfl.quantize15) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_17(tfl.quantize16) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_18(tfl.quantize17) shape_signature:[60, -1, 256], type:INT8\n",
            "  T#8_19(while/add_23) shape:[], type:INT32\n",
            "  T#8_20(while/MatMul_111) shape_signature:[-1, 512], type:INT8\n",
            "  T#8_21(while/TensorArrayV2Read/TensorListGetItem;time1) shape_signature:[-1, 256], type:INT8\n",
            "  T#8_22(while/MatMul7) shape_signature:[-1, 512], type:INT8\n",
            "  T#8_23(while/add3) shape_signature:[-1, 512], type:INT8\n",
            "  T#8_24(while/BiasAdd3) shape_signature:[-1, 512], type:INT8\n",
            "  T#8_25(while/split12;while/split13;while/split14;while/split15) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_26(while/split12;while/split13;while/split14;while/split151) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_27(while/split12;while/split13;while/split14;while/split152) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_28(while/split12;while/split13;while/split14;while/split153) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_29(while/Sigmoid3) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_30(while/Sigmoid_13) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_31(while/mul3) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_32(while/Sigmoid_23) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_33(while/Tanh3) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_34(while/mul_13) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_35(while/add_13) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_36(tfl.dequantize10) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#8_37(while/Tanh_13) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_38(while/mul_23) shape_signature:[-1, 128], type:INT8\n",
            "  T#8_39(while/mul_231) shape_signature:[-1, 128], type:FLOAT32\n",
            "  T#8_40(while/TensorArrayV2Write/TensorListSetItem36) shape_signature:[0, -1, 128], type:INT8\n",
            "  T#8_41(while/TensorArrayV2Write/TensorListSetItem37) shape_signature:[0, -1, 128], type:INT8\n",
            "  T#8_42(while/TensorArrayV2Write/TensorListSetItem38) shape_signature:[1, -1, 128], type:INT8\n",
            "  T#8_43(while/TensorArrayV2Write/TensorListSetItem39) shape_signature:[1, -1, 128], type:INT8\n",
            "  T#8_44(tfl.dequantize11) shape_signature:[1, -1, 128], type:FLOAT32\n",
            "  T#8_45(while/add_33) shape:[], type:INT32\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Your TFLite model has '1' signature_def(s).\n",
            "\n",
            "Signature#0 key: 'serving_default'\n",
            "- Subgraph: Subgraph#0\n",
            "- Inputs: \n",
            "    'input_1' : T#0\n",
            "- Outputs: \n",
            "    'output_1' : T#243\n",
            "\n",
            "---------------------------------------------------------------\n",
            "              Model size:    1393848 bytes\n",
            "    Non-data buffer size:      98451 bytes (07.06 %)\n",
            "  Total data buffer size:    1295397 bytes (92.94 %)\n",
            "          - Subgraph#0  :     641158 bytes (46.00 %)\n",
            "          - Subgraph#1  :          4 bytes (00.00 %)\n",
            "          - Subgraph#2  :     131636 bytes (09.44 %)\n",
            "          - Subgraph#3  :          4 bytes (00.00 %)\n",
            "          - Subgraph#4  :     131636 bytes (09.44 %)\n",
            "          - Subgraph#5  :          4 bytes (00.00 %)\n",
            "          - Subgraph#6  :     197176 bytes (14.15 %)\n",
            "          - Subgraph#7  :          4 bytes (00.00 %)\n",
            "          - Subgraph#8  :     197176 bytes (14.15 %)\n",
            "    (Zero value buffers):      31513 bytes (02.26 %)\n",
            "\n",
            "* Buffers of TFLite model are mostly used for constant tensors.\n",
            "  And zero value buffers are buffers filled with zeros.\n",
            "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
            "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.lite.experimental.Analyzer.analyze(model_path='ltrybest_doubleCNNmodel.tflite')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkPdJfE8_ZyK",
        "outputId": "adc0d05f-1060-4ffd-da78-ce201a1c8311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ltrybest_doubleCNNmodel.tflite ===\n",
            "\n",
            "Your TFLite model has '1' subgraph(s). In the subgraph description below,\n",
            "T# represents the Tensor numbers. For example, in Subgraph#0, the QUANTIZE op takes\n",
            "tensor #0 as input and produces tensor #59 as output.\n",
            "\n",
            "Subgraph#0 main(T#0) -> [T#175]\n",
            "  Op#0 QUANTIZE(T#0) -> [T#59]\n",
            "  Op#1 EXPAND_DIMS(T#59, T#1[-3]) -> [T#60]\n",
            "  Op#2 RESHAPE(T#60, T#2[-1, 1, 7680, 1]) -> [T#61]\n",
            "  Op#3 CONV_2D(T#61, T#58, T#56[0, 0, 0, 0, 0, ...]) -> [T#62]\n",
            "  Op#4 SHAPE(T#60) -> [T#63]\n",
            "  Op#5 STRIDED_SLICE(T#63, T#3[0], T#4[-3], T#5[1]) -> [T#64]\n",
            "  Op#6 CONCATENATION(T#64, T#6[1, 2560, 8]) -> [T#65]\n",
            "  Op#7 RESHAPE(T#62, T#65) -> [T#66]\n",
            "  Op#8 SQUEEZE(T#66) -> [T#67]\n",
            "  Op#9 MUL(T#67, T#55) -> [T#68]\n",
            "  Op#10 ADD(T#68, T#54) -> [T#69]\n",
            "  Op#11 MAX_POOL_2D(T#69) -> [T#70]\n",
            "  Op#12 EXPAND_DIMS(T#70, T#1[-3]) -> [T#71]\n",
            "  Op#13 RESHAPE(T#71, T#7[-1, 1, 640, 8]) -> [T#72]\n",
            "  Op#14 CONV_2D(T#72, T#53, T#47[0, 0, 0, 0, 0, ...]) -> [T#73]\n",
            "  Op#15 SHAPE(T#71) -> [T#74]\n",
            "  Op#16 STRIDED_SLICE(T#74, T#3[0], T#4[-3], T#5[1]) -> [T#75]\n",
            "  Op#17 CONCATENATION(T#75, T#8[1, 640, 16]) -> [T#76]\n",
            "  Op#18 RESHAPE(T#73, T#76) -> [T#77]\n",
            "  Op#19 SQUEEZE(T#77) -> [T#78]\n",
            "  Op#20 SHAPE(T#78) -> [T#79]\n",
            "  Op#21 STRIDED_SLICE(T#79, T#3[0], T#9[-2], T#5[1]) -> [T#80]\n",
            "  Op#22 CONCATENATION(T#80, T#10[640, 16]) -> [T#81]\n",
            "  Op#23 ADD(T#78, T#46) -> [T#82]\n",
            "  Op#24 RESHAPE(T#82, T#81) -> [T#83]\n",
            "  Op#25 MUL(T#83, T#45) -> [T#84]\n",
            "  Op#26 ADD(T#84, T#44) -> [T#85]\n",
            "  Op#27 EXPAND_DIMS(T#85, T#1[-3]) -> [T#86]\n",
            "  Op#28 RESHAPE(T#86, T#11[-1, 1, 640, 16]) -> [T#87]\n",
            "  Op#29 CONV_2D(T#87, T#43, T#48[0, 0, 0, 0, 0, ...]) -> [T#88]\n",
            "  Op#30 SHAPE(T#86) -> [T#89]\n",
            "  Op#31 STRIDED_SLICE(T#89, T#3[0], T#4[-3], T#5[1]) -> [T#90]\n",
            "  Op#32 CONCATENATION(T#90, T#8[1, 640, 16]) -> [T#91]\n",
            "  Op#33 RESHAPE(T#88, T#91) -> [T#92]\n",
            "  Op#34 SQUEEZE(T#92) -> [T#93]\n",
            "  Op#35 SHAPE(T#93) -> [T#94]\n",
            "  Op#36 STRIDED_SLICE(T#94, T#3[0], T#9[-2], T#5[1]) -> [T#95]\n",
            "  Op#37 CONCATENATION(T#95, T#10[640, 16]) -> [T#96]\n",
            "  Op#38 ADD(T#93, T#42) -> [T#97]\n",
            "  Op#39 RESHAPE(T#97, T#96) -> [T#98]\n",
            "  Op#40 MUL(T#98, T#41) -> [T#99]\n",
            "  Op#41 ADD(T#99, T#40) -> [T#100]\n",
            "  Op#42 EXPAND_DIMS(T#100, T#1[-3]) -> [T#101]\n",
            "  Op#43 RESHAPE(T#101, T#11[-1, 1, 640, 16]) -> [T#102]\n",
            "  Op#44 CONV_2D(T#102, T#39, T#49[0, 0, 0, 0, 0, ...]) -> [T#103]\n",
            "  Op#45 SHAPE(T#101) -> [T#104]\n",
            "  Op#46 STRIDED_SLICE(T#104, T#3[0], T#4[-3], T#5[1]) -> [T#105]\n",
            "  Op#47 CONCATENATION(T#105, T#8[1, 640, 16]) -> [T#106]\n",
            "  Op#48 RESHAPE(T#103, T#106) -> [T#107]\n",
            "  Op#49 SQUEEZE(T#107) -> [T#108]\n",
            "  Op#50 SHAPE(T#108) -> [T#109]\n",
            "  Op#51 STRIDED_SLICE(T#109, T#3[0], T#9[-2], T#5[1]) -> [T#110]\n",
            "  Op#52 CONCATENATION(T#110, T#10[640, 16]) -> [T#111]\n",
            "  Op#53 ADD(T#108, T#38) -> [T#112]\n",
            "  Op#54 RESHAPE(T#112, T#111) -> [T#113]\n",
            "  Op#55 MUL(T#113, T#37) -> [T#114]\n",
            "  Op#56 ADD(T#114, T#36) -> [T#115]\n",
            "  Op#57 MAX_POOL_2D(T#115) -> [T#116]\n",
            "  Op#58 RESHAPE(T#116, T#12[-1, 5120]) -> [T#117]\n",
            "  Op#59 CONV_2D(T#61, T#35, T#57[0, 0, 0, 0, 0, ...]) -> [T#118]\n",
            "  Op#60 CONCATENATION(T#64, T#13[1, 154, 8]) -> [T#119]\n",
            "  Op#61 RESHAPE(T#118, T#119) -> [T#120]\n",
            "  Op#62 SQUEEZE(T#120) -> [T#121]\n",
            "  Op#63 MUL(T#121, T#34) -> [T#122]\n",
            "  Op#64 ADD(T#122, T#33) -> [T#123]\n",
            "  Op#65 MAX_POOL_2D(T#123) -> [T#124]\n",
            "  Op#66 EXPAND_DIMS(T#124, T#1[-3]) -> [T#125]\n",
            "  Op#67 RESHAPE(T#125, T#14[-1, 1, 39, 8]) -> [T#126]\n",
            "  Op#68 CONV_2D(T#126, T#32, T#50[0, 0, 0, 0, 0, ...]) -> [T#127]\n",
            "  Op#69 SHAPE(T#125) -> [T#128]\n",
            "  Op#70 STRIDED_SLICE(T#128, T#3[0], T#4[-3], T#5[1]) -> [T#129]\n",
            "  Op#71 CONCATENATION(T#129, T#15[1, 39, 16]) -> [T#130]\n",
            "  Op#72 RESHAPE(T#127, T#130) -> [T#131]\n",
            "  Op#73 SQUEEZE(T#131) -> [T#132]\n",
            "  Op#74 SHAPE(T#132) -> [T#133]\n",
            "  Op#75 STRIDED_SLICE(T#133, T#3[0], T#9[-2], T#5[1]) -> [T#134]\n",
            "  Op#76 CONCATENATION(T#134, T#16[39, 16]) -> [T#135]\n",
            "  Op#77 ADD(T#132, T#31) -> [T#136]\n",
            "  Op#78 RESHAPE(T#136, T#135) -> [T#137]\n",
            "  Op#79 MUL(T#137, T#30) -> [T#138]\n",
            "  Op#80 ADD(T#138, T#29) -> [T#139]\n",
            "  Op#81 EXPAND_DIMS(T#139, T#1[-3]) -> [T#140]\n",
            "  Op#82 RESHAPE(T#140, T#17[-1, 1, 39, 16]) -> [T#141]\n",
            "  Op#83 CONV_2D(T#141, T#28, T#51[0, 0, 0, 0, 0, ...]) -> [T#142]\n",
            "  Op#84 SHAPE(T#140) -> [T#143]\n",
            "  Op#85 STRIDED_SLICE(T#143, T#3[0], T#4[-3], T#5[1]) -> [T#144]\n",
            "  Op#86 CONCATENATION(T#144, T#15[1, 39, 16]) -> [T#145]\n",
            "  Op#87 RESHAPE(T#142, T#145) -> [T#146]\n",
            "  Op#88 SQUEEZE(T#146) -> [T#147]\n",
            "  Op#89 SHAPE(T#147) -> [T#148]\n",
            "  Op#90 STRIDED_SLICE(T#148, T#3[0], T#9[-2], T#5[1]) -> [T#149]\n",
            "  Op#91 CONCATENATION(T#149, T#16[39, 16]) -> [T#150]\n",
            "  Op#92 ADD(T#147, T#27) -> [T#151]\n",
            "  Op#93 RESHAPE(T#151, T#150) -> [T#152]\n",
            "  Op#94 MUL(T#152, T#26) -> [T#153]\n",
            "  Op#95 ADD(T#153, T#25) -> [T#154]\n",
            "  Op#96 EXPAND_DIMS(T#154, T#1[-3]) -> [T#155]\n",
            "  Op#97 RESHAPE(T#155, T#17[-1, 1, 39, 16]) -> [T#156]\n",
            "  Op#98 CONV_2D(T#156, T#24, T#52[0, 0, 0, 0, 0, ...]) -> [T#157]\n",
            "  Op#99 SHAPE(T#155) -> [T#158]\n",
            "  Op#100 STRIDED_SLICE(T#158, T#3[0], T#4[-3], T#5[1]) -> [T#159]\n",
            "  Op#101 CONCATENATION(T#159, T#15[1, 39, 16]) -> [T#160]\n",
            "  Op#102 RESHAPE(T#157, T#160) -> [T#161]\n",
            "  Op#103 SQUEEZE(T#161) -> [T#162]\n",
            "  Op#104 SHAPE(T#162) -> [T#163]\n",
            "  Op#105 STRIDED_SLICE(T#163, T#3[0], T#9[-2], T#5[1]) -> [T#164]\n",
            "  Op#106 CONCATENATION(T#164, T#16[39, 16]) -> [T#165]\n",
            "  Op#107 ADD(T#162, T#23) -> [T#166]\n",
            "  Op#108 RESHAPE(T#166, T#165) -> [T#167]\n",
            "  Op#109 MUL(T#167, T#22) -> [T#168]\n",
            "  Op#110 ADD(T#168, T#21) -> [T#169]\n",
            "  Op#111 MAX_POOL_2D(T#169) -> [T#170]\n",
            "  Op#112 RESHAPE(T#170, T#18[-1, 320]) -> [T#171]\n",
            "  Op#113 CONCATENATION(T#117, T#171) -> [T#172]\n",
            "  Op#114 FULLY_CONNECTED(T#172, T#20, T#19[227, -587, 1076, -4886, 2675]) -> [T#173]\n",
            "  Op#115 SOFTMAX(T#173) -> [T#174]\n",
            "  Op#116 DEQUANTIZE(T#174) -> [T#175]\n",
            "\n",
            "Tensors of Subgraph#0\n",
            "  T#0(serving_default_input_1:0) shape_signature:[-1, 1, 7680, 1], type:FLOAT32\n",
            "  T#1(double_cnn_2/conv1d_16/Conv1D/ExpandDims/dim) shape:[], type:INT32 RO 4 bytes, buffer: 2, data:[-3]\n",
            "  T#2(double_cnn_2/conv1d_16/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 3, data:[-1, 1, 7680, 1]\n",
            "  T#3(double_cnn_2/conv1d_16/Conv1D/strided_slice/stack) shape:[1], type:INT32 RO 4 bytes, buffer: 4, data:[0]\n",
            "  T#4(double_cnn_2/conv1d_16/Conv1D/strided_slice/stack_1) shape:[1], type:INT32 RO 4 bytes, buffer: 2, data:[-3]\n",
            "  T#5(double_cnn_2/conv1d_16/Conv1D/strided_slice/stack_2) shape:[1], type:INT32 RO 4 bytes, buffer: 6, data:[1]\n",
            "  T#6(double_cnn_2/conv1d_16/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 7, data:[1, 2560, 8]\n",
            "  T#7(double_cnn_2/conv1d_17/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 8, data:[-1, 1, 640, 8]\n",
            "  T#8(double_cnn_2/conv1d_17/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 9, data:[1, 640, 16]\n",
            "  T#9(double_cnn_2/conv1d_17/squeeze_batch_dims/strided_slice/stack_1) shape:[1], type:INT32 RO 4 bytes, buffer: 10, data:[-2]\n",
            "  T#10(double_cnn_2/conv1d_17/squeeze_batch_dims/concat/values_1) shape:[2], type:INT32 RO 8 bytes, buffer: 11, data:[640, 16]\n",
            "  T#11(double_cnn_2/conv1d_18/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 12, data:[-1, 1, 640, 16]\n",
            "  T#12(double_cnn_2/flatten_4/Const) shape:[2], type:INT32 RO 8 bytes, buffer: 13, data:[-1, 5120]\n",
            "  T#13(double_cnn_2/conv1d_20/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 14, data:[1, 154, 8]\n",
            "  T#14(double_cnn_2/conv1d_21/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 15, data:[-1, 1, 39, 8]\n",
            "  T#15(double_cnn_2/conv1d_21/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 16, data:[1, 39, 16]\n",
            "  T#16(double_cnn_2/conv1d_21/squeeze_batch_dims/concat/values_1) shape:[2], type:INT32 RO 8 bytes, buffer: 17, data:[39, 16]\n",
            "  T#17(double_cnn_2/conv1d_22/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 18, data:[-1, 1, 39, 16]\n",
            "  T#18(double_cnn_2/flatten_4/Const_1) shape:[2], type:INT32 RO 8 bytes, buffer: 19, data:[-1, 320]\n",
            "  T#19(double_cnn_2/dense_2/BiasAdd/ReadVariableOp) shape:[5], type:INT32 RO 20 bytes, buffer: 20, data:[227, -587, 1076, -4886, 2675]\n",
            "  T#20(double_cnn_2/dense_2/MatMul) shape:[5, 5440], type:INT8 RO 27200 bytes, buffer: 21, data:[., ., ., ., ., ...]\n",
            "  T#21(double_cnn_2/batch_normalization_23/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 22, data:[-, ., ., ., ., ...]\n",
            "  T#22(double_cnn_2/batch_normalization_23/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 23, data:[J, ^, (, G, +, ...]\n",
            "  T#23(double_cnn_2/conv1d_23/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 24, data:[., ., ., ^, >, ...]\n",
            "  T#24(double_cnn_2/conv1d_23/Conv1D/Conv2D1) shape:[16, 1, 4, 16], type:INT8 RO 1024 bytes, buffer: 25, data:[., ., ., ., ., ...]\n",
            "  T#25(double_cnn_2/batch_normalization_22/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 26, data:[., ., ., ., g, ...]\n",
            "  T#26(double_cnn_2/batch_normalization_22/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 27, data:[., ., ., ., ., ...]\n",
            "  T#27(double_cnn_2/conv1d_22/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 28, data:[., ., ., ., ., ...]\n",
            "  T#28(double_cnn_2/conv1d_22/Conv1D/Conv2D) shape:[16, 1, 4, 16], type:INT8 RO 1024 bytes, buffer: 29, data:[+, ., ., ., ., ...]\n",
            "  T#29(double_cnn_2/batch_normalization_21/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 30, data:[}, ., y, 6, \\, ...]\n",
            "  T#30(double_cnn_2/batch_normalization_21/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 31, data:[8, >, 2, X, (, ...]\n",
            "  T#31(double_cnn_2/conv1d_21/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 32, data:[., $, ., ., ., ...]\n",
            "  T#32(double_cnn_2/conv1d_21/Conv1D/Conv2D) shape:[16, 1, 4, 8], type:INT8 RO 512 bytes, buffer: 33, data:[., (, ., ., ., ...]\n",
            "  T#33(double_cnn_2/batch_normalization_20/FusedBatchNormV31) shape:[8], type:INT8 RO 8 bytes, buffer: 34, data:[., ., ., P, ., ...]\n",
            "  T#34(double_cnn_2/batch_normalization_20/FusedBatchNormV3) shape:[8], type:INT8 RO 8 bytes, buffer: 35, data:[., ., ., ., ., ...]\n",
            "  T#35(double_cnn_2/conv1d_20/Conv1D/Conv2D1) shape:[8, 1, 200, 1], type:INT8 RO 1600 bytes, buffer: 36, data:[S, 0, ', -, <, ...]\n",
            "  T#36(double_cnn_2/batch_normalization_19/ReadVariableOp_1) shape:[16], type:INT8 RO 16 bytes, buffer: 37, data:[., y, ., ., ., ...]\n",
            "  T#37(double_cnn_2/batch_normalization_19/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 38, data:[t, w, l, z, u, ...]\n",
            "  T#38(double_cnn_2/conv1d_19/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 39, data:[<, ., ), ., ., ...]\n",
            "  T#39(double_cnn_2/conv1d_19/Conv1D/Conv2D) shape:[16, 1, 6, 16], type:INT8 RO 1536 bytes, buffer: 40, data:[8, ., ], ., ., ...]\n",
            "  T#40(double_cnn_2/batch_normalization_18/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 41, data:[w, y, v, p, ., ...]\n",
            "  T#41(double_cnn_2/batch_normalization_18/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 42, data:[}, {, ., x, u, ...]\n",
            "  T#42(double_cnn_2/conv1d_18/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 43, data:[., ., ., ., ., ...]\n",
            "  T#43(double_cnn_2/conv1d_18/Conv1D/Conv2D) shape:[16, 1, 6, 16], type:INT8 RO 1536 bytes, buffer: 44, data:[T, P, ., ., P, ...]\n",
            "  T#44(double_cnn_2/batch_normalization_17/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 45, data:[w, ., ., x, x, ...]\n",
            "  T#45(double_cnn_2/batch_normalization_17/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 46, data:[~, q, u, }, ~, ...]\n",
            "  T#46(double_cnn_2/conv1d_17/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 47, data:[., ., X, ., ., ...]\n",
            "  T#47(double_cnn_2/conv1d_23/Conv1D/Conv2D) shape:[16], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#48(double_cnn_2/conv1d_23/Conv1D/Conv2D2) shape:[16], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#49(double_cnn_2/conv1d_23/Conv1D/Conv2D3) shape:[16], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#50(double_cnn_2/conv1d_23/Conv1D/Conv2D4) shape:[16], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#51(double_cnn_2/conv1d_23/Conv1D/Conv2D5) shape:[16], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#52(double_cnn_2/conv1d_23/Conv1D/Conv2D6) shape:[16], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#53(double_cnn_2/conv1d_17/Conv1D/Conv2D) shape:[16, 1, 6, 8], type:INT8 RO 768 bytes, buffer: 54, data:[,, ., ., ., ., ...]\n",
            "  T#54(double_cnn_2/batch_normalization_16/FusedBatchNormV31) shape:[8], type:INT8 RO 8 bytes, buffer: 55, data:[., ., ., ., ., ...]\n",
            "  T#55(double_cnn_2/batch_normalization_16/FusedBatchNormV3) shape:[8], type:INT8 RO 8 bytes, buffer: 56, data:[r, ., y, ~, s, ...]\n",
            "  T#56(double_cnn_2/conv1d_20/Conv1D/Conv2D) shape:[8], type:INT32 RO 32 bytes, buffer: 57, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#57(double_cnn_2/conv1d_20/Conv1D/Conv2D2) shape:[8], type:INT32 RO 32 bytes, buffer: 57, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#58(double_cnn_2/conv1d_16/Conv1D/Conv2D) shape:[8, 1, 25, 1], type:INT8 RO 200 bytes, buffer: 59, data:[., ., ., ., ., ...]\n",
            "  T#59(tfl.quantize) shape_signature:[-1, 1, 7680, 1], type:INT8\n",
            "  T#60(double_cnn_2/conv1d_16/Conv1D/ExpandDims) shape_signature:[-1, 1, 1, 7680, 1], type:INT8\n",
            "  T#61(double_cnn_2/conv1d_16/Conv1D/Reshape) shape_signature:[-1, 1, 7680, 1], type:INT8\n",
            "  T#62(double_cnn_2/conv1d_16/Conv1D/Conv2D1) shape_signature:[-1, 1, 2560, 8], type:INT8\n",
            "  T#63(double_cnn_2/conv1d_16/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#64(double_cnn_2/conv1d_16/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#65(double_cnn_2/conv1d_16/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#66(double_cnn_2/conv1d_16/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 2560, 8], type:INT8\n",
            "  T#67(double_cnn_2/conv1d_16/Conv1D/Squeeze) shape_signature:[-1, -1, 2560, 8], type:INT8\n",
            "  T#68(double_cnn_2/batch_normalization_16/FusedBatchNormV32) shape_signature:[-1, -1, 2560, 8], type:INT8\n",
            "  T#69(double_cnn_2/re_lu_16/Relu;double_cnn_2/batch_normalization_16/FusedBatchNormV3) shape_signature:[-1, -1, 2560, 8], type:INT8\n",
            "  T#70(double_cnn_2/max_pooling2d_8/MaxPool) shape_signature:[-1, -1, 640, 8], type:INT8\n",
            "  T#71(double_cnn_2/conv1d_17/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 640, 8], type:INT8\n",
            "  T#72(double_cnn_2/conv1d_17/Conv1D/Reshape) shape_signature:[-1, 1, 640, 8], type:INT8\n",
            "  T#73(double_cnn_2/conv1d_17/Conv1D/Conv2D1) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#74(double_cnn_2/conv1d_17/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#75(double_cnn_2/conv1d_17/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#76(double_cnn_2/conv1d_17/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#77(double_cnn_2/conv1d_17/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#78(double_cnn_2/conv1d_17/Conv1D/Squeeze) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#79(double_cnn_2/conv1d_17/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#80(double_cnn_2/conv1d_17/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#81(double_cnn_2/conv1d_17/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#82(double_cnn_2/conv1d_17/Relu;double_cnn_2/conv1d_17/squeeze_batch_dims/Reshape_1;double_cnn_2/conv1d_17/squeeze_batch_dims/BiasAdd;double_cnn_2/conv1d_17/squeeze_batch_dims/Reshape/shape;double_cnn_2/conv1d_17/squeeze_batch_dims/Reshape;double_cnn_2/conv1d_17/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#83(double_cnn_2/conv1d_17/Relu;double_cnn_2/conv1d_17/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#84(double_cnn_2/batch_normalization_17/FusedBatchNormV32) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#85(double_cnn_2/re_lu_17/Relu;double_cnn_2/batch_normalization_17/FusedBatchNormV3) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#86(double_cnn_2/conv1d_18/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#87(double_cnn_2/conv1d_18/Conv1D/Reshape) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#88(double_cnn_2/conv1d_18/Conv1D/Conv2D1) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#89(double_cnn_2/conv1d_18/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#90(double_cnn_2/conv1d_18/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#91(double_cnn_2/conv1d_18/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#92(double_cnn_2/conv1d_18/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#93(double_cnn_2/conv1d_18/Conv1D/Squeeze) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#94(double_cnn_2/conv1d_18/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#95(double_cnn_2/conv1d_18/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#96(double_cnn_2/conv1d_18/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#97(double_cnn_2/conv1d_18/Relu;double_cnn_2/conv1d_18/squeeze_batch_dims/Reshape_1;double_cnn_2/conv1d_18/squeeze_batch_dims/BiasAdd;double_cnn_2/conv1d_17/squeeze_batch_dims/Reshape/shape;double_cnn_2/conv1d_18/squeeze_batch_dims/Reshape;double_cnn_2/conv1d_18/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#98(double_cnn_2/conv1d_18/Relu;double_cnn_2/conv1d_18/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#99(double_cnn_2/batch_normalization_18/FusedBatchNormV32) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#100(double_cnn_2/re_lu_18/Relu;double_cnn_2/batch_normalization_18/FusedBatchNormV3) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#101(double_cnn_2/conv1d_19/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#102(double_cnn_2/conv1d_19/Conv1D/Reshape) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#103(double_cnn_2/conv1d_19/Conv1D/Conv2D1) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#104(double_cnn_2/conv1d_19/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#105(double_cnn_2/conv1d_19/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#106(double_cnn_2/conv1d_19/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#107(double_cnn_2/conv1d_19/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#108(double_cnn_2/conv1d_19/Conv1D/Squeeze) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#109(double_cnn_2/conv1d_19/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#110(double_cnn_2/conv1d_19/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#111(double_cnn_2/conv1d_19/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#112(double_cnn_2/conv1d_19/Relu;double_cnn_2/conv1d_19/squeeze_batch_dims/Reshape_1;double_cnn_2/conv1d_19/squeeze_batch_dims/BiasAdd;double_cnn_2/conv1d_17/squeeze_batch_dims/Reshape/shape;double_cnn_2/conv1d_19/squeeze_batch_dims/Reshape;double_cnn_2/conv1d_19/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#113(double_cnn_2/conv1d_19/Relu;double_cnn_2/conv1d_19/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#114(double_cnn_2/batch_normalization_19/FusedBatchNormV31) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#115(double_cnn_2/re_lu_19/Relu;double_cnn_2/batch_normalization_19/FusedBatchNormV3) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#116(double_cnn_2/max_pooling2d_9/MaxPool) shape_signature:[-1, -1, 320, 16], type:INT8\n",
            "  T#117(double_cnn_2/flatten_4/Reshape) shape_signature:[-1, 5120], type:INT8\n",
            "  T#118(double_cnn_2/conv1d_20/Conv1D/Conv2D21) shape_signature:[-1, 1, 154, 8], type:INT8\n",
            "  T#119(double_cnn_2/conv1d_20/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#120(double_cnn_2/conv1d_20/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 154, 8], type:INT8\n",
            "  T#121(double_cnn_2/conv1d_20/Conv1D/Squeeze) shape_signature:[-1, -1, 154, 8], type:INT8\n",
            "  T#122(double_cnn_2/batch_normalization_20/FusedBatchNormV32) shape_signature:[-1, -1, 154, 8], type:INT8\n",
            "  T#123(double_cnn_2/re_lu_20/Relu;double_cnn_2/batch_normalization_20/FusedBatchNormV3) shape_signature:[-1, -1, 154, 8], type:INT8\n",
            "  T#124(double_cnn_2/max_pooling2d_10/MaxPool) shape_signature:[-1, -1, 39, 8], type:INT8\n",
            "  T#125(double_cnn_2/conv1d_21/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 39, 8], type:INT8\n",
            "  T#126(double_cnn_2/conv1d_21/Conv1D/Reshape) shape_signature:[-1, 1, 39, 8], type:INT8\n",
            "  T#127(double_cnn_2/conv1d_21/Conv1D/Conv2D1) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#128(double_cnn_2/conv1d_21/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#129(double_cnn_2/conv1d_21/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#130(double_cnn_2/conv1d_21/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#131(double_cnn_2/conv1d_21/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#132(double_cnn_2/conv1d_21/Conv1D/Squeeze) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#133(double_cnn_2/conv1d_21/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#134(double_cnn_2/conv1d_21/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#135(double_cnn_2/conv1d_21/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#136(double_cnn_2/conv1d_21/Relu;double_cnn_2/conv1d_21/squeeze_batch_dims/Reshape_1;double_cnn_2/conv1d_21/squeeze_batch_dims/BiasAdd;double_cnn_2/conv1d_21/squeeze_batch_dims/Reshape/shape;double_cnn_2/conv1d_21/squeeze_batch_dims/Reshape;double_cnn_2/conv1d_21/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#137(double_cnn_2/conv1d_21/Relu;double_cnn_2/conv1d_21/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#138(double_cnn_2/batch_normalization_21/FusedBatchNormV32) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#139(double_cnn_2/re_lu_21/Relu;double_cnn_2/batch_normalization_21/FusedBatchNormV3) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#140(double_cnn_2/conv1d_22/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#141(double_cnn_2/conv1d_22/Conv1D/Reshape) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#142(double_cnn_2/conv1d_22/Conv1D/Conv2D1) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#143(double_cnn_2/conv1d_22/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#144(double_cnn_2/conv1d_22/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#145(double_cnn_2/conv1d_22/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#146(double_cnn_2/conv1d_22/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#147(double_cnn_2/conv1d_22/Conv1D/Squeeze) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#148(double_cnn_2/conv1d_22/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#149(double_cnn_2/conv1d_22/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#150(double_cnn_2/conv1d_22/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#151(double_cnn_2/conv1d_22/Relu;double_cnn_2/conv1d_22/squeeze_batch_dims/Reshape_1;double_cnn_2/conv1d_22/squeeze_batch_dims/BiasAdd;double_cnn_2/conv1d_21/squeeze_batch_dims/Reshape/shape;double_cnn_2/conv1d_22/squeeze_batch_dims/Reshape;double_cnn_2/conv1d_22/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#152(double_cnn_2/conv1d_22/Relu;double_cnn_2/conv1d_22/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#153(double_cnn_2/batch_normalization_22/FusedBatchNormV32) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#154(double_cnn_2/re_lu_22/Relu;double_cnn_2/batch_normalization_22/FusedBatchNormV3) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#155(double_cnn_2/conv1d_23/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#156(double_cnn_2/conv1d_23/Conv1D/Reshape) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#157(double_cnn_2/conv1d_23/Conv1D/Conv2D21) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#158(double_cnn_2/conv1d_23/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#159(double_cnn_2/conv1d_23/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#160(double_cnn_2/conv1d_23/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#161(double_cnn_2/conv1d_23/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#162(double_cnn_2/conv1d_23/Conv1D/Squeeze) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#163(double_cnn_2/conv1d_23/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#164(double_cnn_2/conv1d_23/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#165(double_cnn_2/conv1d_23/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#166(double_cnn_2/conv1d_23/Relu;double_cnn_2/conv1d_23/squeeze_batch_dims/Reshape_1;double_cnn_2/conv1d_23/squeeze_batch_dims/BiasAdd;double_cnn_2/conv1d_21/squeeze_batch_dims/Reshape/shape;double_cnn_2/conv1d_23/squeeze_batch_dims/Reshape;double_cnn_2/conv1d_23/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#167(double_cnn_2/conv1d_23/Relu;double_cnn_2/conv1d_23/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#168(double_cnn_2/batch_normalization_23/FusedBatchNormV32) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#169(double_cnn_2/re_lu_23/Relu;double_cnn_2/batch_normalization_23/FusedBatchNormV3) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#170(double_cnn_2/max_pooling2d_11/MaxPool) shape_signature:[-1, -1, 20, 16], type:INT8\n",
            "  T#171(double_cnn_2/flatten_4/Reshape_1) shape_signature:[-1, 320], type:INT8\n",
            "  T#172(double_cnn_2/concatenate/concat) shape_signature:[-1, 5440], type:INT8\n",
            "  T#173(double_cnn_2/dense_2/MatMul;double_cnn_2/dense_2/BiasAdd) shape_signature:[-1, 5], type:INT8\n",
            "  T#174(StatefulPartitionedCall:01) shape_signature:[-1, 5], type:INT8\n",
            "  T#175(StatefulPartitionedCall:0) shape_signature:[-1, 5], type:FLOAT32\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Your TFLite model has '1' signature_def(s).\n",
            "\n",
            "Signature#0 key: 'serving_default'\n",
            "- Subgraph: Subgraph#0\n",
            "- Inputs: \n",
            "    'input_1' : T#0\n",
            "- Outputs: \n",
            "    'output_1' : T#175\n",
            "\n",
            "---------------------------------------------------------------\n",
            "              Model size:      72512 bytes\n",
            "    Non-data buffer size:      36396 bytes (50.19 %)\n",
            "  Total data buffer size:      36116 bytes (49.81 %)\n",
            "    (Zero value buffers):        300 bytes (00.41 %)\n",
            "\n",
            "* Buffers of TFLite model are mostly used for constant tensors.\n",
            "  And zero value buffers are buffers filled with zeros.\n",
            "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
            "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.lite.experimental.Analyzer.analyze(model_path='lfinalbest_doubleCNNmodel_3.tflite')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq4ApUsnDT26",
        "outputId": "ba5c5310-0007-4c6e-e1d6-91f07fb491f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== lfinalbest_doubleCNNmodel_3.tflite ===\n",
            "\n",
            "Your TFLite model has '1' subgraph(s). In the subgraph description below,\n",
            "T# represents the Tensor numbers. For example, in Subgraph#0, the QUANTIZE op takes\n",
            "tensor #0 as input and produces tensor #59 as output.\n",
            "\n",
            "Subgraph#0 main(T#0) -> [T#175]\n",
            "  Op#0 QUANTIZE(T#0) -> [T#59]\n",
            "  Op#1 EXPAND_DIMS(T#59, T#1[-3]) -> [T#60]\n",
            "  Op#2 RESHAPE(T#60, T#2[-1, 1, 7680, 1]) -> [T#61]\n",
            "  Op#3 CONV_2D(T#61, T#58, T#56[0, 0, 0, 0, 0, ...]) -> [T#62]\n",
            "  Op#4 SHAPE(T#60) -> [T#63]\n",
            "  Op#5 STRIDED_SLICE(T#63, T#3[0], T#4[-3], T#5[1]) -> [T#64]\n",
            "  Op#6 CONCATENATION(T#64, T#6[1, 2560, 8]) -> [T#65]\n",
            "  Op#7 RESHAPE(T#62, T#65) -> [T#66]\n",
            "  Op#8 SQUEEZE(T#66) -> [T#67]\n",
            "  Op#9 MUL(T#67, T#55) -> [T#68]\n",
            "  Op#10 ADD(T#68, T#54) -> [T#69]\n",
            "  Op#11 MAX_POOL_2D(T#69) -> [T#70]\n",
            "  Op#12 EXPAND_DIMS(T#70, T#1[-3]) -> [T#71]\n",
            "  Op#13 RESHAPE(T#71, T#7[-1, 1, 640, 8]) -> [T#72]\n",
            "  Op#14 CONV_2D(T#72, T#53, T#47[0, 0, 0, 0, 0, ...]) -> [T#73]\n",
            "  Op#15 SHAPE(T#71) -> [T#74]\n",
            "  Op#16 STRIDED_SLICE(T#74, T#3[0], T#4[-3], T#5[1]) -> [T#75]\n",
            "  Op#17 CONCATENATION(T#75, T#8[1, 640, 16]) -> [T#76]\n",
            "  Op#18 RESHAPE(T#73, T#76) -> [T#77]\n",
            "  Op#19 SQUEEZE(T#77) -> [T#78]\n",
            "  Op#20 SHAPE(T#78) -> [T#79]\n",
            "  Op#21 STRIDED_SLICE(T#79, T#3[0], T#9[-2], T#5[1]) -> [T#80]\n",
            "  Op#22 CONCATENATION(T#80, T#10[640, 16]) -> [T#81]\n",
            "  Op#23 ADD(T#78, T#46) -> [T#82]\n",
            "  Op#24 RESHAPE(T#82, T#81) -> [T#83]\n",
            "  Op#25 MUL(T#83, T#45) -> [T#84]\n",
            "  Op#26 ADD(T#84, T#44) -> [T#85]\n",
            "  Op#27 EXPAND_DIMS(T#85, T#1[-3]) -> [T#86]\n",
            "  Op#28 RESHAPE(T#86, T#11[-1, 1, 640, 16]) -> [T#87]\n",
            "  Op#29 CONV_2D(T#87, T#43, T#48[0, 0, 0, 0, 0, ...]) -> [T#88]\n",
            "  Op#30 SHAPE(T#86) -> [T#89]\n",
            "  Op#31 STRIDED_SLICE(T#89, T#3[0], T#4[-3], T#5[1]) -> [T#90]\n",
            "  Op#32 CONCATENATION(T#90, T#8[1, 640, 16]) -> [T#91]\n",
            "  Op#33 RESHAPE(T#88, T#91) -> [T#92]\n",
            "  Op#34 SQUEEZE(T#92) -> [T#93]\n",
            "  Op#35 SHAPE(T#93) -> [T#94]\n",
            "  Op#36 STRIDED_SLICE(T#94, T#3[0], T#9[-2], T#5[1]) -> [T#95]\n",
            "  Op#37 CONCATENATION(T#95, T#10[640, 16]) -> [T#96]\n",
            "  Op#38 ADD(T#93, T#42) -> [T#97]\n",
            "  Op#39 RESHAPE(T#97, T#96) -> [T#98]\n",
            "  Op#40 MUL(T#98, T#41) -> [T#99]\n",
            "  Op#41 ADD(T#99, T#40) -> [T#100]\n",
            "  Op#42 EXPAND_DIMS(T#100, T#1[-3]) -> [T#101]\n",
            "  Op#43 RESHAPE(T#101, T#11[-1, 1, 640, 16]) -> [T#102]\n",
            "  Op#44 CONV_2D(T#102, T#39, T#49[0, 0, 0, 0, 0, ...]) -> [T#103]\n",
            "  Op#45 SHAPE(T#101) -> [T#104]\n",
            "  Op#46 STRIDED_SLICE(T#104, T#3[0], T#4[-3], T#5[1]) -> [T#105]\n",
            "  Op#47 CONCATENATION(T#105, T#8[1, 640, 16]) -> [T#106]\n",
            "  Op#48 RESHAPE(T#103, T#106) -> [T#107]\n",
            "  Op#49 SQUEEZE(T#107) -> [T#108]\n",
            "  Op#50 SHAPE(T#108) -> [T#109]\n",
            "  Op#51 STRIDED_SLICE(T#109, T#3[0], T#9[-2], T#5[1]) -> [T#110]\n",
            "  Op#52 CONCATENATION(T#110, T#10[640, 16]) -> [T#111]\n",
            "  Op#53 ADD(T#108, T#38) -> [T#112]\n",
            "  Op#54 RESHAPE(T#112, T#111) -> [T#113]\n",
            "  Op#55 MUL(T#113, T#37) -> [T#114]\n",
            "  Op#56 ADD(T#114, T#36) -> [T#115]\n",
            "  Op#57 MAX_POOL_2D(T#115) -> [T#116]\n",
            "  Op#58 RESHAPE(T#116, T#12[-1, 5120]) -> [T#117]\n",
            "  Op#59 CONV_2D(T#61, T#35, T#57[0, 0, 0, 0, 0, ...]) -> [T#118]\n",
            "  Op#60 CONCATENATION(T#64, T#13[1, 154, 8]) -> [T#119]\n",
            "  Op#61 RESHAPE(T#118, T#119) -> [T#120]\n",
            "  Op#62 SQUEEZE(T#120) -> [T#121]\n",
            "  Op#63 MUL(T#121, T#34) -> [T#122]\n",
            "  Op#64 ADD(T#122, T#33) -> [T#123]\n",
            "  Op#65 MAX_POOL_2D(T#123) -> [T#124]\n",
            "  Op#66 EXPAND_DIMS(T#124, T#1[-3]) -> [T#125]\n",
            "  Op#67 RESHAPE(T#125, T#14[-1, 1, 39, 8]) -> [T#126]\n",
            "  Op#68 CONV_2D(T#126, T#32, T#50[0, 0, 0, 0, 0, ...]) -> [T#127]\n",
            "  Op#69 SHAPE(T#125) -> [T#128]\n",
            "  Op#70 STRIDED_SLICE(T#128, T#3[0], T#4[-3], T#5[1]) -> [T#129]\n",
            "  Op#71 CONCATENATION(T#129, T#15[1, 39, 16]) -> [T#130]\n",
            "  Op#72 RESHAPE(T#127, T#130) -> [T#131]\n",
            "  Op#73 SQUEEZE(T#131) -> [T#132]\n",
            "  Op#74 SHAPE(T#132) -> [T#133]\n",
            "  Op#75 STRIDED_SLICE(T#133, T#3[0], T#9[-2], T#5[1]) -> [T#134]\n",
            "  Op#76 CONCATENATION(T#134, T#16[39, 16]) -> [T#135]\n",
            "  Op#77 ADD(T#132, T#31) -> [T#136]\n",
            "  Op#78 RESHAPE(T#136, T#135) -> [T#137]\n",
            "  Op#79 MUL(T#137, T#30) -> [T#138]\n",
            "  Op#80 ADD(T#138, T#29) -> [T#139]\n",
            "  Op#81 EXPAND_DIMS(T#139, T#1[-3]) -> [T#140]\n",
            "  Op#82 RESHAPE(T#140, T#17[-1, 1, 39, 16]) -> [T#141]\n",
            "  Op#83 CONV_2D(T#141, T#28, T#51[0, 0, 0, 0, 0, ...]) -> [T#142]\n",
            "  Op#84 SHAPE(T#140) -> [T#143]\n",
            "  Op#85 STRIDED_SLICE(T#143, T#3[0], T#4[-3], T#5[1]) -> [T#144]\n",
            "  Op#86 CONCATENATION(T#144, T#15[1, 39, 16]) -> [T#145]\n",
            "  Op#87 RESHAPE(T#142, T#145) -> [T#146]\n",
            "  Op#88 SQUEEZE(T#146) -> [T#147]\n",
            "  Op#89 SHAPE(T#147) -> [T#148]\n",
            "  Op#90 STRIDED_SLICE(T#148, T#3[0], T#9[-2], T#5[1]) -> [T#149]\n",
            "  Op#91 CONCATENATION(T#149, T#16[39, 16]) -> [T#150]\n",
            "  Op#92 ADD(T#147, T#27) -> [T#151]\n",
            "  Op#93 RESHAPE(T#151, T#150) -> [T#152]\n",
            "  Op#94 MUL(T#152, T#26) -> [T#153]\n",
            "  Op#95 ADD(T#153, T#25) -> [T#154]\n",
            "  Op#96 EXPAND_DIMS(T#154, T#1[-3]) -> [T#155]\n",
            "  Op#97 RESHAPE(T#155, T#17[-1, 1, 39, 16]) -> [T#156]\n",
            "  Op#98 CONV_2D(T#156, T#24, T#52[0, 0, 0, 0, 0, ...]) -> [T#157]\n",
            "  Op#99 SHAPE(T#155) -> [T#158]\n",
            "  Op#100 STRIDED_SLICE(T#158, T#3[0], T#4[-3], T#5[1]) -> [T#159]\n",
            "  Op#101 CONCATENATION(T#159, T#15[1, 39, 16]) -> [T#160]\n",
            "  Op#102 RESHAPE(T#157, T#160) -> [T#161]\n",
            "  Op#103 SQUEEZE(T#161) -> [T#162]\n",
            "  Op#104 SHAPE(T#162) -> [T#163]\n",
            "  Op#105 STRIDED_SLICE(T#163, T#3[0], T#9[-2], T#5[1]) -> [T#164]\n",
            "  Op#106 CONCATENATION(T#164, T#16[39, 16]) -> [T#165]\n",
            "  Op#107 ADD(T#162, T#23) -> [T#166]\n",
            "  Op#108 RESHAPE(T#166, T#165) -> [T#167]\n",
            "  Op#109 MUL(T#167, T#22) -> [T#168]\n",
            "  Op#110 ADD(T#168, T#21) -> [T#169]\n",
            "  Op#111 MAX_POOL_2D(T#169) -> [T#170]\n",
            "  Op#112 RESHAPE(T#170, T#18[-1, 320]) -> [T#171]\n",
            "  Op#113 CONCATENATION(T#117, T#171) -> [T#172]\n",
            "  Op#114 FULLY_CONNECTED(T#172, T#20, T#19[-144, -86, 298, -446, 503]) -> [T#173]\n",
            "  Op#115 SOFTMAX(T#173) -> [T#174]\n",
            "  Op#116 DEQUANTIZE(T#174) -> [T#175]\n",
            "\n",
            "Tensors of Subgraph#0\n",
            "  T#0(serving_default_input_1:0) shape_signature:[-1, 1, 7680, 1], type:FLOAT32\n",
            "  T#1(double_cnn/conv1d/Conv1D/ExpandDims/dim) shape:[], type:INT32 RO 4 bytes, buffer: 2, data:[-3]\n",
            "  T#2(double_cnn/conv1d/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 3, data:[-1, 1, 7680, 1]\n",
            "  T#3(double_cnn/conv1d/Conv1D/strided_slice/stack) shape:[1], type:INT32 RO 4 bytes, buffer: 4, data:[0]\n",
            "  T#4(double_cnn/conv1d/Conv1D/strided_slice/stack_1) shape:[1], type:INT32 RO 4 bytes, buffer: 2, data:[-3]\n",
            "  T#5(double_cnn/conv1d/Conv1D/strided_slice/stack_2) shape:[1], type:INT32 RO 4 bytes, buffer: 6, data:[1]\n",
            "  T#6(double_cnn/conv1d/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 7, data:[1, 2560, 8]\n",
            "  T#7(double_cnn/conv1d_1/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 8, data:[-1, 1, 640, 8]\n",
            "  T#8(double_cnn/conv1d_1/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 9, data:[1, 640, 16]\n",
            "  T#9(double_cnn/conv1d_1/squeeze_batch_dims/strided_slice/stack_1) shape:[1], type:INT32 RO 4 bytes, buffer: 10, data:[-2]\n",
            "  T#10(double_cnn/conv1d_1/squeeze_batch_dims/concat/values_1) shape:[2], type:INT32 RO 8 bytes, buffer: 11, data:[640, 16]\n",
            "  T#11(double_cnn/conv1d_2/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 12, data:[-1, 1, 640, 16]\n",
            "  T#12(double_cnn/flatten/Const) shape:[2], type:INT32 RO 8 bytes, buffer: 13, data:[-1, 5120]\n",
            "  T#13(double_cnn/conv1d_4/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 14, data:[1, 154, 8]\n",
            "  T#14(double_cnn/conv1d_5/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 15, data:[-1, 1, 39, 8]\n",
            "  T#15(double_cnn/conv1d_5/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 16, data:[1, 39, 16]\n",
            "  T#16(double_cnn/conv1d_5/squeeze_batch_dims/concat/values_1) shape:[2], type:INT32 RO 8 bytes, buffer: 17, data:[39, 16]\n",
            "  T#17(double_cnn/conv1d_6/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 18, data:[-1, 1, 39, 16]\n",
            "  T#18(double_cnn/flatten/Const_1) shape:[2], type:INT32 RO 8 bytes, buffer: 19, data:[-1, 320]\n",
            "  T#19(double_cnn/dense/BiasAdd/ReadVariableOp) shape:[5], type:INT32 RO 20 bytes, buffer: 20, data:[-144, -86, 298, -446, 503]\n",
            "  T#20(double_cnn/dense/MatMul) shape:[5, 5440], type:INT8 RO 27200 bytes, buffer: 21, data:[., \n",
            ", ., ., ., ...]\n",
            "  T#21(double_cnn/batch_normalization_7/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 22, data:[., ., ., ., 3, ...]\n",
            "  T#22(double_cnn/batch_normalization_7/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 23, data:[., ., ., ., j, ...]\n",
            "  T#23(double_cnn/conv1d_7/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 24, data:[., ., 0, ., u, ...]\n",
            "  T#24(double_cnn/conv1d_7/Conv1D/Conv2D1) shape:[16, 1, 4, 16], type:INT8 RO 1024 bytes, buffer: 25, data:[., ., D, *, M, ...]\n",
            "  T#25(double_cnn/batch_normalization_6/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 26, data:[x, ., ., ., ., ...]\n",
            "  T#26(double_cnn/batch_normalization_6/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 27, data:[}, ., ?, !, L, ...]\n",
            "  T#27(double_cnn/conv1d_6/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 28, data:[., `, /, 5, \\, ...]\n",
            "  T#28(double_cnn/conv1d_6/Conv1D/Conv2D) shape:[16, 1, 4, 16], type:INT8 RO 1024 bytes, buffer: 29, data:[., ., ., ., ., ...]\n",
            "  T#29(double_cnn/batch_normalization_5/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 30, data:[., W, ., ., 8, ...]\n",
            "  T#30(double_cnn/batch_normalization_5/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 31, data:[b, X, h, j, N, ...]\n",
            "  T#31(double_cnn/conv1d_5/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 32, data:[w, ., ., N, M, ...]\n",
            "  T#32(double_cnn/conv1d_5/Conv1D/Conv2D) shape:[16, 1, 4, 8], type:INT8 RO 512 bytes, buffer: 33, data:[h, ., <, ., ., ...]\n",
            "  T#33(double_cnn/batch_normalization_4/FusedBatchNormV31) shape:[8], type:INT8 RO 8 bytes, buffer: 34, data:[W, _, ., J, ., ...]\n",
            "  T#34(double_cnn/batch_normalization_4/FusedBatchNormV3) shape:[8], type:INT8 RO 8 bytes, buffer: 35, data:[., c, ^, d, 1, ...]\n",
            "  T#35(double_cnn/conv1d_4/Conv1D/Conv2D1) shape:[8, 1, 200, 1], type:INT8 RO 1600 bytes, buffer: 36, data:[P, ), ., ., ., ...]\n",
            "  T#36(double_cnn/batch_normalization_3/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 37, data:[., ., J, k, R, ...]\n",
            "  T#37(double_cnn/batch_normalization_3/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 38, data:[L, ., J, b, O, ...]\n",
            "  T#38(double_cnn/conv1d_3/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 39, data:[o, ., ., ., ., ...]\n",
            "  T#39(double_cnn/conv1d_3/Conv1D/Conv2D) shape:[16, 1, 6, 16], type:INT8 RO 1536 bytes, buffer: 40, data:[., ., ., ., ., ...]\n",
            "  T#40(double_cnn/batch_normalization_2/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 41, data:[u, |, t, z, r, ...]\n",
            "  T#41(double_cnn/batch_normalization_2/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 42, data:[l, }, n, ., p, ...]\n",
            "  T#42(double_cnn/conv1d_2/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 43, data:[., ., ., ., ., ...]\n",
            "  T#43(double_cnn/conv1d_2/Conv1D/Conv2D) shape:[16, 1, 6, 16], type:INT8 RO 1536 bytes, buffer: 44, data:[9, ., ., ., ., ...]\n",
            "  T#44(double_cnn/batch_normalization_1/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 45, data:[g, y, s, ., ., ...]\n",
            "  T#45(double_cnn/batch_normalization_1/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 46, data:[p, u, m, ., i, ...]\n",
            "  T#46(double_cnn/conv1d_1/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 47, data:[., ., ., ., l, ...]\n",
            "  T#47(double_cnn/conv1d_7/Conv1D/Conv2D) shape:[16], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#48(double_cnn/conv1d_7/Conv1D/Conv2D2) shape:[16], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#49(double_cnn/conv1d_7/Conv1D/Conv2D3) shape:[16], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#50(double_cnn/conv1d_7/Conv1D/Conv2D4) shape:[16], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#51(double_cnn/conv1d_7/Conv1D/Conv2D5) shape:[16], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#52(double_cnn/conv1d_7/Conv1D/Conv2D6) shape:[16], type:INT32 RO 64 bytes, buffer: 48, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#53(double_cnn/conv1d_1/Conv1D/Conv2D) shape:[16, 1, 6, 8], type:INT8 RO 768 bytes, buffer: 54, data:[., ., 7, ., ., ...]\n",
            "  T#54(double_cnn/batch_normalization/FusedBatchNormV31) shape:[8], type:INT8 RO 8 bytes, buffer: 55, data:[., ., Y, ., ., ...]\n",
            "  T#55(double_cnn/batch_normalization/FusedBatchNormV3) shape:[8], type:INT8 RO 8 bytes, buffer: 56, data:[>, >, ., <, =, ...]\n",
            "  T#56(double_cnn/conv1d_4/Conv1D/Conv2D) shape:[8], type:INT32 RO 32 bytes, buffer: 57, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#57(double_cnn/conv1d_4/Conv1D/Conv2D2) shape:[8], type:INT32 RO 32 bytes, buffer: 57, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#58(double_cnn/conv1d/Conv1D/Conv2D) shape:[8, 1, 25, 1], type:INT8 RO 200 bytes, buffer: 59, data:[., ., ., ., ., ...]\n",
            "  T#59(tfl.quantize) shape_signature:[-1, 1, 7680, 1], type:INT8\n",
            "  T#60(double_cnn/conv1d/Conv1D/ExpandDims) shape_signature:[-1, 1, 1, 7680, 1], type:INT8\n",
            "  T#61(double_cnn/conv1d/Conv1D/Reshape) shape_signature:[-1, 1, 7680, 1], type:INT8\n",
            "  T#62(double_cnn/conv1d/Conv1D/Conv2D1) shape_signature:[-1, 1, 2560, 8], type:INT8\n",
            "  T#63(double_cnn/conv1d/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#64(double_cnn/conv1d/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#65(double_cnn/conv1d/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#66(double_cnn/conv1d/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 2560, 8], type:INT8\n",
            "  T#67(double_cnn/conv1d/Conv1D/Squeeze) shape_signature:[-1, -1, 2560, 8], type:INT8\n",
            "  T#68(double_cnn/batch_normalization/FusedBatchNormV32) shape_signature:[-1, -1, 2560, 8], type:INT8\n",
            "  T#69(double_cnn/re_lu/Relu;double_cnn/batch_normalization/FusedBatchNormV3) shape_signature:[-1, -1, 2560, 8], type:INT8\n",
            "  T#70(double_cnn/max_pooling2d/MaxPool) shape_signature:[-1, -1, 640, 8], type:INT8\n",
            "  T#71(double_cnn/conv1d_1/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 640, 8], type:INT8\n",
            "  T#72(double_cnn/conv1d_1/Conv1D/Reshape) shape_signature:[-1, 1, 640, 8], type:INT8\n",
            "  T#73(double_cnn/conv1d_1/Conv1D/Conv2D1) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#74(double_cnn/conv1d_1/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#75(double_cnn/conv1d_1/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#76(double_cnn/conv1d_1/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#77(double_cnn/conv1d_1/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#78(double_cnn/conv1d_1/Conv1D/Squeeze) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#79(double_cnn/conv1d_1/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#80(double_cnn/conv1d_1/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#81(double_cnn/conv1d_1/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#82(double_cnn/conv1d_1/Relu;double_cnn/conv1d_1/squeeze_batch_dims/Reshape_1;double_cnn/conv1d_1/squeeze_batch_dims/BiasAdd;double_cnn/conv1d_1/squeeze_batch_dims/Reshape/shape;double_cnn/conv1d_1/squeeze_batch_dims/Reshape;double_cnn/conv1d_1/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#83(double_cnn/conv1d_1/Relu;double_cnn/conv1d_1/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#84(double_cnn/batch_normalization_1/FusedBatchNormV32) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#85(double_cnn/re_lu_1/Relu;double_cnn/batch_normalization_1/FusedBatchNormV3) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#86(double_cnn/conv1d_2/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#87(double_cnn/conv1d_2/Conv1D/Reshape) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#88(double_cnn/conv1d_2/Conv1D/Conv2D1) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#89(double_cnn/conv1d_2/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#90(double_cnn/conv1d_2/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#91(double_cnn/conv1d_2/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#92(double_cnn/conv1d_2/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#93(double_cnn/conv1d_2/Conv1D/Squeeze) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#94(double_cnn/conv1d_2/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#95(double_cnn/conv1d_2/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#96(double_cnn/conv1d_2/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#97(double_cnn/conv1d_2/Relu;double_cnn/conv1d_2/squeeze_batch_dims/Reshape_1;double_cnn/conv1d_2/squeeze_batch_dims/BiasAdd;double_cnn/conv1d_1/squeeze_batch_dims/Reshape/shape;double_cnn/conv1d_2/squeeze_batch_dims/Reshape;double_cnn/conv1d_2/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#98(double_cnn/conv1d_2/Relu;double_cnn/conv1d_2/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#99(double_cnn/batch_normalization_2/FusedBatchNormV32) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#100(double_cnn/re_lu_2/Relu;double_cnn/batch_normalization_2/FusedBatchNormV3) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#101(double_cnn/conv1d_3/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#102(double_cnn/conv1d_3/Conv1D/Reshape) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#103(double_cnn/conv1d_3/Conv1D/Conv2D1) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#104(double_cnn/conv1d_3/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#105(double_cnn/conv1d_3/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#106(double_cnn/conv1d_3/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#107(double_cnn/conv1d_3/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#108(double_cnn/conv1d_3/Conv1D/Squeeze) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#109(double_cnn/conv1d_3/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#110(double_cnn/conv1d_3/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#111(double_cnn/conv1d_3/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#112(double_cnn/conv1d_3/Relu;double_cnn/conv1d_3/squeeze_batch_dims/Reshape_1;double_cnn/conv1d_3/squeeze_batch_dims/BiasAdd;double_cnn/conv1d_1/squeeze_batch_dims/Reshape/shape;double_cnn/conv1d_3/squeeze_batch_dims/Reshape;double_cnn/conv1d_3/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#113(double_cnn/conv1d_3/Relu;double_cnn/conv1d_3/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#114(double_cnn/batch_normalization_3/FusedBatchNormV32) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#115(double_cnn/re_lu_3/Relu;double_cnn/batch_normalization_3/FusedBatchNormV3) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#116(double_cnn/max_pooling2d_1/MaxPool) shape_signature:[-1, -1, 320, 16], type:INT8\n",
            "  T#117(double_cnn/flatten/Reshape) shape_signature:[-1, 5120], type:INT8\n",
            "  T#118(double_cnn/conv1d_4/Conv1D/Conv2D21) shape_signature:[-1, 1, 154, 8], type:INT8\n",
            "  T#119(double_cnn/conv1d_4/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#120(double_cnn/conv1d_4/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 154, 8], type:INT8\n",
            "  T#121(double_cnn/conv1d_4/Conv1D/Squeeze) shape_signature:[-1, -1, 154, 8], type:INT8\n",
            "  T#122(double_cnn/batch_normalization_4/FusedBatchNormV32) shape_signature:[-1, -1, 154, 8], type:INT8\n",
            "  T#123(double_cnn/re_lu_4/Relu;double_cnn/batch_normalization_4/FusedBatchNormV3) shape_signature:[-1, -1, 154, 8], type:INT8\n",
            "  T#124(double_cnn/max_pooling2d_2/MaxPool) shape_signature:[-1, -1, 39, 8], type:INT8\n",
            "  T#125(double_cnn/conv1d_5/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 39, 8], type:INT8\n",
            "  T#126(double_cnn/conv1d_5/Conv1D/Reshape) shape_signature:[-1, 1, 39, 8], type:INT8\n",
            "  T#127(double_cnn/conv1d_5/Conv1D/Conv2D1) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#128(double_cnn/conv1d_5/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#129(double_cnn/conv1d_5/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#130(double_cnn/conv1d_5/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#131(double_cnn/conv1d_5/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#132(double_cnn/conv1d_5/Conv1D/Squeeze) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#133(double_cnn/conv1d_5/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#134(double_cnn/conv1d_5/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#135(double_cnn/conv1d_5/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#136(double_cnn/conv1d_5/Relu;double_cnn/conv1d_5/squeeze_batch_dims/Reshape_1;double_cnn/conv1d_5/squeeze_batch_dims/BiasAdd;double_cnn/conv1d_5/squeeze_batch_dims/Reshape/shape;double_cnn/conv1d_5/squeeze_batch_dims/Reshape;double_cnn/conv1d_5/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#137(double_cnn/conv1d_5/Relu;double_cnn/conv1d_5/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#138(double_cnn/batch_normalization_5/FusedBatchNormV32) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#139(double_cnn/re_lu_5/Relu;double_cnn/batch_normalization_5/FusedBatchNormV3) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#140(double_cnn/conv1d_6/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#141(double_cnn/conv1d_6/Conv1D/Reshape) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#142(double_cnn/conv1d_6/Conv1D/Conv2D1) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#143(double_cnn/conv1d_6/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#144(double_cnn/conv1d_6/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#145(double_cnn/conv1d_6/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#146(double_cnn/conv1d_6/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#147(double_cnn/conv1d_6/Conv1D/Squeeze) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#148(double_cnn/conv1d_6/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#149(double_cnn/conv1d_6/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#150(double_cnn/conv1d_6/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#151(double_cnn/conv1d_6/Relu;double_cnn/conv1d_6/squeeze_batch_dims/Reshape_1;double_cnn/conv1d_6/squeeze_batch_dims/BiasAdd;double_cnn/conv1d_5/squeeze_batch_dims/Reshape/shape;double_cnn/conv1d_6/squeeze_batch_dims/Reshape;double_cnn/conv1d_6/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#152(double_cnn/conv1d_6/Relu;double_cnn/conv1d_6/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#153(double_cnn/batch_normalization_6/FusedBatchNormV32) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#154(double_cnn/re_lu_6/Relu;double_cnn/batch_normalization_6/FusedBatchNormV3) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#155(double_cnn/conv1d_7/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#156(double_cnn/conv1d_7/Conv1D/Reshape) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#157(double_cnn/conv1d_7/Conv1D/Conv2D21) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#158(double_cnn/conv1d_7/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#159(double_cnn/conv1d_7/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#160(double_cnn/conv1d_7/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#161(double_cnn/conv1d_7/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#162(double_cnn/conv1d_7/Conv1D/Squeeze) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#163(double_cnn/conv1d_7/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#164(double_cnn/conv1d_7/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#165(double_cnn/conv1d_7/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#166(double_cnn/conv1d_7/Relu;double_cnn/conv1d_7/squeeze_batch_dims/Reshape_1;double_cnn/conv1d_7/squeeze_batch_dims/BiasAdd;double_cnn/conv1d_5/squeeze_batch_dims/Reshape/shape;double_cnn/conv1d_7/squeeze_batch_dims/Reshape;double_cnn/conv1d_7/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#167(double_cnn/conv1d_7/Relu;double_cnn/conv1d_7/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#168(double_cnn/batch_normalization_7/FusedBatchNormV32) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#169(double_cnn/re_lu_7/Relu;double_cnn/batch_normalization_7/FusedBatchNormV3) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#170(double_cnn/max_pooling2d_3/MaxPool) shape_signature:[-1, -1, 20, 16], type:INT8\n",
            "  T#171(double_cnn/flatten/Reshape_1) shape_signature:[-1, 320], type:INT8\n",
            "  T#172(double_cnn/concatenate/concat) shape_signature:[-1, 5440], type:INT8\n",
            "  T#173(double_cnn/dense/MatMul;double_cnn/dense/BiasAdd) shape_signature:[-1, 5], type:INT8\n",
            "  T#174(StatefulPartitionedCall:01) shape_signature:[-1, 5], type:INT8\n",
            "  T#175(StatefulPartitionedCall:0) shape_signature:[-1, 5], type:FLOAT32\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Your TFLite model has '1' signature_def(s).\n",
            "\n",
            "Signature#0 key: 'serving_default'\n",
            "- Subgraph: Subgraph#0\n",
            "- Inputs: \n",
            "    'input_1' : T#0\n",
            "- Outputs: \n",
            "    'output_1' : T#175\n",
            "\n",
            "---------------------------------------------------------------\n",
            "              Model size:      71752 bytes\n",
            "    Non-data buffer size:      35636 bytes (49.67 %)\n",
            "  Total data buffer size:      36116 bytes (50.33 %)\n",
            "    (Zero value buffers):        100 bytes (00.14 %)\n",
            "\n",
            "* Buffers of TFLite model are mostly used for constant tensors.\n",
            "  And zero value buffers are buffers filled with zeros.\n",
            "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
            "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Works but has error message while predicting"
      ],
      "metadata": {
        "id": "OmpD_qXYfFB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.lite.experimental.Analyzer.analyze(model_path='ltestbest_2CNNsBiLSTMmodel_2.tflite')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PAO56YVVaH5",
        "outputId": "f40ab3c2-82ef-4361-f7af-2446fdc64cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ltestbest_2CNNsBiLSTMmodel_2.tflite ===\n",
            "\n",
            "Your TFLite model has '5' subgraph(s). In the subgraph description below,\n",
            "T# represents the Tensor numbers. For example, in Subgraph#0, the QUANTIZE op takes\n",
            "tensor #0 as input and produces tensor #71 as output.\n",
            "\n",
            "Subgraph#0 main(T#0) -> [T#219]\n",
            "  Op#0 QUANTIZE(T#0) -> [T#71]\n",
            "  Op#1 EXPAND_DIMS(T#71, T#3[-3]) -> [T#72]\n",
            "  Op#2 RESHAPE(T#72, T#4[-1, 1, 7680, 1]) -> [T#73]\n",
            "  Op#3 CONV_2D(T#73, T#70, T#68[0, 0, 0, 0, 0, ...]) -> [T#74]\n",
            "  Op#4 SHAPE(T#72) -> [T#75]\n",
            "  Op#5 STRIDED_SLICE(T#75, T#5[0], T#6[-3], T#7[1]) -> [T#76]\n",
            "  Op#6 CONCATENATION(T#76, T#8[1, 2560, 8]) -> [T#77]\n",
            "  Op#7 RESHAPE(T#74, T#77) -> [T#78]\n",
            "  Op#8 SQUEEZE(T#78) -> [T#79]\n",
            "  Op#9 MUL(T#79, T#67) -> [T#80]\n",
            "  Op#10 ADD(T#80, T#66) -> [T#81]\n",
            "  Op#11 MAX_POOL_2D(T#81) -> [T#82]\n",
            "  Op#12 EXPAND_DIMS(T#82, T#3[-3]) -> [T#83]\n",
            "  Op#13 RESHAPE(T#83, T#9[-1, 1, 640, 8]) -> [T#84]\n",
            "  Op#14 CONV_2D(T#84, T#65, T#59[0, 0, 0, 0, 0, ...]) -> [T#85]\n",
            "  Op#15 SHAPE(T#83) -> [T#86]\n",
            "  Op#16 STRIDED_SLICE(T#86, T#5[0], T#6[-3], T#7[1]) -> [T#87]\n",
            "  Op#17 CONCATENATION(T#87, T#10[1, 640, 16]) -> [T#88]\n",
            "  Op#18 RESHAPE(T#85, T#88) -> [T#89]\n",
            "  Op#19 SQUEEZE(T#89) -> [T#90]\n",
            "  Op#20 SHAPE(T#90) -> [T#91]\n",
            "  Op#21 STRIDED_SLICE(T#91, T#5[0], T#11[-2], T#7[1]) -> [T#92]\n",
            "  Op#22 CONCATENATION(T#92, T#12[640, 16]) -> [T#93]\n",
            "  Op#23 ADD(T#90, T#58) -> [T#94]\n",
            "  Op#24 RESHAPE(T#94, T#93) -> [T#95]\n",
            "  Op#25 MUL(T#95, T#57) -> [T#96]\n",
            "  Op#26 ADD(T#96, T#56) -> [T#97]\n",
            "  Op#27 EXPAND_DIMS(T#97, T#3[-3]) -> [T#98]\n",
            "  Op#28 RESHAPE(T#98, T#13[-1, 1, 640, 16]) -> [T#99]\n",
            "  Op#29 CONV_2D(T#99, T#55, T#60[0, 0, 0, 0, 0, ...]) -> [T#100]\n",
            "  Op#30 SHAPE(T#98) -> [T#101]\n",
            "  Op#31 STRIDED_SLICE(T#101, T#5[0], T#6[-3], T#7[1]) -> [T#102]\n",
            "  Op#32 CONCATENATION(T#102, T#10[1, 640, 16]) -> [T#103]\n",
            "  Op#33 RESHAPE(T#100, T#103) -> [T#104]\n",
            "  Op#34 SQUEEZE(T#104) -> [T#105]\n",
            "  Op#35 SHAPE(T#105) -> [T#106]\n",
            "  Op#36 STRIDED_SLICE(T#106, T#5[0], T#11[-2], T#7[1]) -> [T#107]\n",
            "  Op#37 CONCATENATION(T#107, T#12[640, 16]) -> [T#108]\n",
            "  Op#38 ADD(T#105, T#54) -> [T#109]\n",
            "  Op#39 RESHAPE(T#109, T#108) -> [T#110]\n",
            "  Op#40 MUL(T#110, T#53) -> [T#111]\n",
            "  Op#41 ADD(T#111, T#52) -> [T#112]\n",
            "  Op#42 EXPAND_DIMS(T#112, T#3[-3]) -> [T#113]\n",
            "  Op#43 RESHAPE(T#113, T#13[-1, 1, 640, 16]) -> [T#114]\n",
            "  Op#44 CONV_2D(T#114, T#51, T#61[0, 0, 0, 0, 0, ...]) -> [T#115]\n",
            "  Op#45 SHAPE(T#113) -> [T#116]\n",
            "  Op#46 STRIDED_SLICE(T#116, T#5[0], T#6[-3], T#7[1]) -> [T#117]\n",
            "  Op#47 CONCATENATION(T#117, T#10[1, 640, 16]) -> [T#118]\n",
            "  Op#48 RESHAPE(T#115, T#118) -> [T#119]\n",
            "  Op#49 SQUEEZE(T#119) -> [T#120]\n",
            "  Op#50 SHAPE(T#120) -> [T#121]\n",
            "  Op#51 STRIDED_SLICE(T#121, T#5[0], T#11[-2], T#7[1]) -> [T#122]\n",
            "  Op#52 CONCATENATION(T#122, T#12[640, 16]) -> [T#123]\n",
            "  Op#53 ADD(T#120, T#50) -> [T#124]\n",
            "  Op#54 RESHAPE(T#124, T#123) -> [T#125]\n",
            "  Op#55 MUL(T#125, T#49) -> [T#126]\n",
            "  Op#56 ADD(T#126, T#48) -> [T#127]\n",
            "  Op#57 MAX_POOL_2D(T#127) -> [T#128]\n",
            "  Op#58 RESHAPE(T#128, T#14[-1, 5120]) -> [T#129]\n",
            "  Op#59 CONV_2D(T#73, T#47, T#69[0, 0, 0, 0, 0, ...]) -> [T#130]\n",
            "  Op#60 CONCATENATION(T#76, T#15[1, 154, 8]) -> [T#131]\n",
            "  Op#61 RESHAPE(T#130, T#131) -> [T#132]\n",
            "  Op#62 SQUEEZE(T#132) -> [T#133]\n",
            "  Op#63 MUL(T#133, T#46) -> [T#134]\n",
            "  Op#64 ADD(T#134, T#45) -> [T#135]\n",
            "  Op#65 MAX_POOL_2D(T#135) -> [T#136]\n",
            "  Op#66 EXPAND_DIMS(T#136, T#3[-3]) -> [T#137]\n",
            "  Op#67 RESHAPE(T#137, T#16[-1, 1, 39, 8]) -> [T#138]\n",
            "  Op#68 CONV_2D(T#138, T#44, T#62[0, 0, 0, 0, 0, ...]) -> [T#139]\n",
            "  Op#69 SHAPE(T#137) -> [T#140]\n",
            "  Op#70 STRIDED_SLICE(T#140, T#5[0], T#6[-3], T#7[1]) -> [T#141]\n",
            "  Op#71 CONCATENATION(T#141, T#17[1, 39, 16]) -> [T#142]\n",
            "  Op#72 RESHAPE(T#139, T#142) -> [T#143]\n",
            "  Op#73 SQUEEZE(T#143) -> [T#144]\n",
            "  Op#74 SHAPE(T#144) -> [T#145]\n",
            "  Op#75 STRIDED_SLICE(T#145, T#5[0], T#11[-2], T#7[1]) -> [T#146]\n",
            "  Op#76 CONCATENATION(T#146, T#18[39, 16]) -> [T#147]\n",
            "  Op#77 ADD(T#144, T#43) -> [T#148]\n",
            "  Op#78 RESHAPE(T#148, T#147) -> [T#149]\n",
            "  Op#79 MUL(T#149, T#42) -> [T#150]\n",
            "  Op#80 ADD(T#150, T#41) -> [T#151]\n",
            "  Op#81 EXPAND_DIMS(T#151, T#3[-3]) -> [T#152]\n",
            "  Op#82 RESHAPE(T#152, T#19[-1, 1, 39, 16]) -> [T#153]\n",
            "  Op#83 CONV_2D(T#153, T#40, T#63[0, 0, 0, 0, 0, ...]) -> [T#154]\n",
            "  Op#84 SHAPE(T#152) -> [T#155]\n",
            "  Op#85 STRIDED_SLICE(T#155, T#5[0], T#6[-3], T#7[1]) -> [T#156]\n",
            "  Op#86 CONCATENATION(T#156, T#17[1, 39, 16]) -> [T#157]\n",
            "  Op#87 RESHAPE(T#154, T#157) -> [T#158]\n",
            "  Op#88 SQUEEZE(T#158) -> [T#159]\n",
            "  Op#89 SHAPE(T#159) -> [T#160]\n",
            "  Op#90 STRIDED_SLICE(T#160, T#5[0], T#11[-2], T#7[1]) -> [T#161]\n",
            "  Op#91 CONCATENATION(T#161, T#18[39, 16]) -> [T#162]\n",
            "  Op#92 ADD(T#159, T#39) -> [T#163]\n",
            "  Op#93 RESHAPE(T#163, T#162) -> [T#164]\n",
            "  Op#94 MUL(T#164, T#38) -> [T#165]\n",
            "  Op#95 ADD(T#165, T#37) -> [T#166]\n",
            "  Op#96 EXPAND_DIMS(T#166, T#3[-3]) -> [T#167]\n",
            "  Op#97 RESHAPE(T#167, T#19[-1, 1, 39, 16]) -> [T#168]\n",
            "  Op#98 CONV_2D(T#168, T#36, T#64[0, 0, 0, 0, 0, ...]) -> [T#169]\n",
            "  Op#99 SHAPE(T#167) -> [T#170]\n",
            "  Op#100 STRIDED_SLICE(T#170, T#5[0], T#6[-3], T#7[1]) -> [T#171]\n",
            "  Op#101 CONCATENATION(T#171, T#17[1, 39, 16]) -> [T#172]\n",
            "  Op#102 RESHAPE(T#169, T#172) -> [T#173]\n",
            "  Op#103 SQUEEZE(T#173) -> [T#174]\n",
            "  Op#104 SHAPE(T#174) -> [T#175]\n",
            "  Op#105 STRIDED_SLICE(T#175, T#5[0], T#11[-2], T#7[1]) -> [T#176]\n",
            "  Op#106 CONCATENATION(T#176, T#18[39, 16]) -> [T#177]\n",
            "  Op#107 ADD(T#174, T#35) -> [T#178]\n",
            "  Op#108 RESHAPE(T#178, T#177) -> [T#179]\n",
            "  Op#109 MUL(T#179, T#34) -> [T#180]\n",
            "  Op#110 ADD(T#180, T#33) -> [T#181]\n",
            "  Op#111 MAX_POOL_2D(T#181) -> [T#182]\n",
            "  Op#112 RESHAPE(T#182, T#20[-1, 320]) -> [T#183]\n",
            "  Op#113 CONCATENATION(T#129, T#183) -> [T#184]\n",
            "  Op#114 SHAPE(T#184) -> [T#185]\n",
            "  Op#115 STRIDED_SLICE(T#185, T#5[0], T#7[1], T#7[1]) -> [T#186]\n",
            "  Op#116 PACK(T#186, T#21[64], T#22[85]) -> [T#187]\n",
            "  Op#117 RESHAPE(T#184, T#187) -> [T#188]\n",
            "  Op#118 SHAPE(T#188) -> [T#189]\n",
            "  Op#119 STRIDED_SLICE(T#189, T#5[0], T#7[1], T#7[1]) -> [T#190]\n",
            "  Op#120 PACK(T#190, T#23[16]) -> [T#191]\n",
            "  Op#121 FILL(T#191, T#31) -> [T#192]\n",
            "  Op#122 DEQUANTIZE(T#192) -> [T#193]\n",
            "  Op#123 TRANSPOSE(T#188, T#24[1, 0, 2]) -> [T#194]\n",
            "  Op#124 DEQUANTIZE(T#194) -> [T#195]\n",
            "  Op#125 WHILE(T#25[0], T#25[0], T#2, T#193, T#193, T#195, Cond: Subgraph#1, Body: Subgraph#2) -> [T#196, T#197, T#198, T#199, T#200, T#201]\n",
            "  Op#126 QUANTIZE(T#198) -> [T#202]\n",
            "  Op#127 TRANSPOSE(T#202, T#24[1, 0, 2]) -> [T#203]\n",
            "  Op#128 SHAPE(T#203) -> [T#204]\n",
            "  Op#129 STRIDED_SLICE(T#204, T#5[0], T#7[1], T#7[1]) -> [T#205]\n",
            "  Op#130 PACK(T#205, T#23[16]) -> [T#206]\n",
            "  Op#131 FILL(T#206, T#32) -> [T#207]\n",
            "  Op#132 DEQUANTIZE(T#207) -> [T#208]\n",
            "  Op#133 WHILE(T#25[0], T#25[0], T#1, T#208, T#208, T#198, Cond: Subgraph#3, Body: Subgraph#4) -> [T#209, T#210, T#211, T#212, T#213, T#214]\n",
            "  Op#134 QUANTIZE(T#211) -> [T#215]\n",
            "  Op#135 STRIDED_SLICE(T#215, T#26[-1, 0, 0], T#27[0, 0, 16], T#28[1, 1, 1]) -> [T#216]\n",
            "  Op#136 FULLY_CONNECTED(T#216, T#30, T#29[-3376, -3159, 3402, -2586, 1908]) -> [T#217]\n",
            "  Op#137 SOFTMAX(T#217) -> [T#218]\n",
            "  Op#138 DEQUANTIZE(T#218) -> [T#219]\n",
            "\n",
            "Tensors of Subgraph#0\n",
            "  T#0(serving_default_input_1:0) shape_signature:[-1, 1, 7680, 1], type:FLOAT32\n",
            "  T#1(TensorArrayV2_1) shape:[1, 1, 16], type:FLOAT32 RO 64 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#2(TensorArrayV2_11) shape:[64, 1, 16], type:FLOAT32 RO 4096 bytes, buffer: 3, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#3(double_cnn_bi_lstm_2/conv1d_16/Conv1D/ExpandDims/dim) shape:[], type:INT32 RO 4 bytes, buffer: 4, data:[-3]\n",
            "  T#4(double_cnn_bi_lstm_2/conv1d_16/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 5, data:[-1, 1, 7680, 1]\n",
            "  T#5(double_cnn_bi_lstm_2/conv1d_16/Conv1D/strided_slice/stack) shape:[1], type:INT32 RO 4 bytes, buffer: 6, data:[0]\n",
            "  T#6(double_cnn_bi_lstm_2/conv1d_16/Conv1D/strided_slice/stack_1) shape:[1], type:INT32 RO 4 bytes, buffer: 4, data:[-3]\n",
            "  T#7(double_cnn_bi_lstm_2/conv1d_16/Conv1D/strided_slice/stack_2) shape:[1], type:INT32 RO 4 bytes, buffer: 8, data:[1]\n",
            "  T#8(double_cnn_bi_lstm_2/conv1d_16/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 9, data:[1, 2560, 8]\n",
            "  T#9(double_cnn_bi_lstm_2/conv1d_17/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 10, data:[-1, 1, 640, 8]\n",
            "  T#10(double_cnn_bi_lstm_2/conv1d_17/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 11, data:[1, 640, 16]\n",
            "  T#11(double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/strided_slice/stack_1) shape:[1], type:INT32 RO 4 bytes, buffer: 12, data:[-2]\n",
            "  T#12(double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/concat/values_1) shape:[2], type:INT32 RO 8 bytes, buffer: 13, data:[640, 16]\n",
            "  T#13(double_cnn_bi_lstm_2/conv1d_18/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 14, data:[-1, 1, 640, 16]\n",
            "  T#14(double_cnn_bi_lstm_2/flatten_4/Const) shape:[2], type:INT32 RO 8 bytes, buffer: 15, data:[-1, 5120]\n",
            "  T#15(double_cnn_bi_lstm_2/conv1d_20/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 16, data:[1, 154, 8]\n",
            "  T#16(double_cnn_bi_lstm_2/conv1d_21/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 17, data:[-1, 1, 39, 8]\n",
            "  T#17(double_cnn_bi_lstm_2/conv1d_21/Conv1D/concat/values_1) shape:[3], type:INT32 RO 12 bytes, buffer: 18, data:[1, 39, 16]\n",
            "  T#18(double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/concat/values_1) shape:[2], type:INT32 RO 8 bytes, buffer: 19, data:[39, 16]\n",
            "  T#19(double_cnn_bi_lstm_2/conv1d_22/Conv1D/Reshape/shape) shape:[4], type:INT32 RO 16 bytes, buffer: 20, data:[-1, 1, 39, 16]\n",
            "  T#20(double_cnn_bi_lstm_2/flatten_4/Const_1) shape:[2], type:INT32 RO 8 bytes, buffer: 21, data:[-1, 320]\n",
            "  T#21(double_cnn_bi_lstm_2/reshape/Reshape/shape/1) shape:[], type:INT32 RO 4 bytes, buffer: 22, data:[64]\n",
            "  T#22(double_cnn_bi_lstm_2/reshape/Reshape/shape/2) shape:[], type:INT32 RO 4 bytes, buffer: 23, data:[85]\n",
            "  T#23(double_cnn_bi_lstm_2/lstm_4/zeros/packed/1) shape:[], type:INT32 RO 4 bytes, buffer: 24, data:[16]\n",
            "  T#24(transpose/perm) shape:[3], type:INT32 RO 12 bytes, buffer: 25, data:[1, 0, 2]\n",
            "  T#25(time) shape:[], type:INT32 RO 4 bytes, buffer: 6, data:[0]\n",
            "  T#26(strided_slice_2) shape:[3], type:INT32 RO 12 bytes, buffer: 27, data:[-1, 0, 0]\n",
            "  T#27(strided_slice_21) shape:[3], type:INT32 RO 12 bytes, buffer: 28, data:[0, 0, 16]\n",
            "  T#28(strided_slice_22) shape:[3], type:INT32 RO 12 bytes, buffer: 29, data:[1, 1, 1]\n",
            "  T#29(double_cnn_bi_lstm_2/dense_5/BiasAdd/ReadVariableOp) shape:[5], type:INT32 RO 20 bytes, buffer: 30, data:[-3376, -3159, 3402, -2586, 1908]\n",
            "  T#30(double_cnn_bi_lstm_2/dense_5/MatMul) shape:[5, 16], type:INT8 RO 80 bytes, buffer: 31, data:[., ., ., ;, ., ...]\n",
            "  T#31(double_cnn_bi_lstm_2/lstm_4/zeros/Const) shape:[], type:INT8 RO 1 bytes, buffer: 32, data:[.]\n",
            "  T#32(double_cnn_bi_lstm_2/lstm_4/zeros/Const1) shape:[], type:INT8 RO 1 bytes, buffer: 32, data:[.]\n",
            "  T#33(double_cnn_bi_lstm_2/batch_normalization_23/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 34, data:[., ., ., ., ., ...]\n",
            "  T#34(double_cnn_bi_lstm_2/batch_normalization_23/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 35, data:[., ., ., 0, ., ...]\n",
            "  T#35(double_cnn_bi_lstm_2/conv1d_23/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 36, data:[., ., ., ., ., ...]\n",
            "  T#36(double_cnn_bi_lstm_2/conv1d_23/Conv1D/Conv2D1) shape:[16, 1, 4, 16], type:INT8 RO 1024 bytes, buffer: 37, data:[., ., 6, \n",
            ", =, ...]\n",
            "  T#37(double_cnn_bi_lstm_2/batch_normalization_22/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 38, data:[., ., ., u, ., ...]\n",
            "  T#38(double_cnn_bi_lstm_2/batch_normalization_22/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 39, data:[., ., ., H, ., ...]\n",
            "  T#39(double_cnn_bi_lstm_2/conv1d_22/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 40, data:[_, ., ., ., Q, ...]\n",
            "  T#40(double_cnn_bi_lstm_2/conv1d_22/Conv1D/Conv2D) shape:[16, 1, 4, 16], type:INT8 RO 1024 bytes, buffer: 41, data:[., ., ., ., ., ...]\n",
            "  T#41(double_cnn_bi_lstm_2/batch_normalization_21/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 42, data:[a, j, v, $, t, ...]\n",
            "  T#42(double_cnn_bi_lstm_2/batch_normalization_21/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 43, data:[_, p, u, q, |, ...]\n",
            "  T#43(double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 44, data:[., ., ., ., ., ...]\n",
            "  T#44(double_cnn_bi_lstm_2/conv1d_21/Conv1D/Conv2D) shape:[16, 1, 4, 8], type:INT8 RO 512 bytes, buffer: 45, data:[., ., 2, ., ., ...]\n",
            "  T#45(double_cnn_bi_lstm_2/batch_normalization_20/FusedBatchNormV31) shape:[8], type:INT8 RO 8 bytes, buffer: 46, data:[., =, H, ., u, ...]\n",
            "  T#46(double_cnn_bi_lstm_2/batch_normalization_20/FusedBatchNormV3) shape:[8], type:INT8 RO 8 bytes, buffer: 47, data:[_, <, >, <, ., ...]\n",
            "  T#47(double_cnn_bi_lstm_2/conv1d_20/Conv1D/Conv2D1) shape:[8, 1, 200, 1], type:INT8 RO 1600 bytes, buffer: 48, data:[., ., ., ., ., ...]\n",
            "  T#48(double_cnn_bi_lstm_2/batch_normalization_19/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 49, data:[*, K, D, !, ., ...]\n",
            "  T#49(double_cnn_bi_lstm_2/batch_normalization_19/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 50, data:[., ., ., G, ., ...]\n",
            "  T#50(double_cnn_bi_lstm_2/conv1d_19/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 51, data:[., 7, ., ., ., ...]\n",
            "  T#51(double_cnn_bi_lstm_2/conv1d_19/Conv1D/Conv2D) shape:[16, 1, 6, 16], type:INT8 RO 1536 bytes, buffer: 52, data:[., ., ., ., ., ...]\n",
            "  T#52(double_cnn_bi_lstm_2/batch_normalization_18/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 53, data:[~, p, ., z, ., ...]\n",
            "  T#53(double_cnn_bi_lstm_2/batch_normalization_18/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 54, data:[k, q, K, l, ., ...]\n",
            "  T#54(double_cnn_bi_lstm_2/conv1d_18/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 55, data:[., ., !, ., S, ...]\n",
            "  T#55(double_cnn_bi_lstm_2/conv1d_18/Conv1D/Conv2D) shape:[16, 1, 6, 16], type:INT8 RO 1536 bytes, buffer: 56, data:[., ., ., ., 3, ...]\n",
            "  T#56(double_cnn_bi_lstm_2/batch_normalization_17/FusedBatchNormV31) shape:[16], type:INT8 RO 16 bytes, buffer: 57, data:[l, ., ., r, m, ...]\n",
            "  T#57(double_cnn_bi_lstm_2/batch_normalization_17/FusedBatchNormV3) shape:[16], type:INT8 RO 16 bytes, buffer: 58, data:[r, m, ^, q, q, ...]\n",
            "  T#58(double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape:[16], type:INT8 RO 16 bytes, buffer: 59, data:[., r, 4, ., ., ...]\n",
            "  T#59(double_cnn_bi_lstm_2/conv1d_23/Conv1D/Conv2D) shape:[16], type:INT32 RO 64 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#60(double_cnn_bi_lstm_2/conv1d_23/Conv1D/Conv2D2) shape:[16], type:INT32 RO 64 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#61(double_cnn_bi_lstm_2/conv1d_23/Conv1D/Conv2D3) shape:[16], type:INT32 RO 64 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#62(double_cnn_bi_lstm_2/conv1d_23/Conv1D/Conv2D4) shape:[16], type:INT32 RO 64 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#63(double_cnn_bi_lstm_2/conv1d_23/Conv1D/Conv2D5) shape:[16], type:INT32 RO 64 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#64(double_cnn_bi_lstm_2/conv1d_23/Conv1D/Conv2D6) shape:[16], type:INT32 RO 64 bytes, buffer: 2, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#65(double_cnn_bi_lstm_2/conv1d_17/Conv1D/Conv2D) shape:[16, 1, 6, 8], type:INT8 RO 768 bytes, buffer: 66, data:[., ., =, ., ., ...]\n",
            "  T#66(double_cnn_bi_lstm_2/batch_normalization_16/FusedBatchNormV31) shape:[8], type:INT8 RO 8 bytes, buffer: 67, data:[., 6, ., e, ., ...]\n",
            "  T#67(double_cnn_bi_lstm_2/batch_normalization_16/FusedBatchNormV3) shape:[8], type:INT8 RO 8 bytes, buffer: 68, data:[J, 7, 7, ., 6, ...]\n",
            "  T#68(double_cnn_bi_lstm_2/conv1d_20/Conv1D/Conv2D) shape:[8], type:INT32 RO 32 bytes, buffer: 69, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#69(double_cnn_bi_lstm_2/conv1d_20/Conv1D/Conv2D2) shape:[8], type:INT32 RO 32 bytes, buffer: 69, data:[0, 0, 0, 0, 0, ...]\n",
            "  T#70(double_cnn_bi_lstm_2/conv1d_16/Conv1D/Conv2D) shape:[8, 1, 25, 1], type:INT8 RO 200 bytes, buffer: 71, data:[., ., ., ., ., ...]\n",
            "  T#71(tfl.quantize) shape_signature:[-1, 1, 7680, 1], type:INT8\n",
            "  T#72(double_cnn_bi_lstm_2/conv1d_16/Conv1D/ExpandDims) shape_signature:[-1, 1, 1, 7680, 1], type:INT8\n",
            "  T#73(double_cnn_bi_lstm_2/conv1d_16/Conv1D/Reshape) shape_signature:[-1, 1, 7680, 1], type:INT8\n",
            "  T#74(double_cnn_bi_lstm_2/conv1d_16/Conv1D/Conv2D1) shape_signature:[-1, 1, 2560, 8], type:INT8\n",
            "  T#75(double_cnn_bi_lstm_2/conv1d_16/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#76(double_cnn_bi_lstm_2/conv1d_16/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#77(double_cnn_bi_lstm_2/conv1d_16/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#78(double_cnn_bi_lstm_2/conv1d_16/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 2560, 8], type:INT8\n",
            "  T#79(double_cnn_bi_lstm_2/conv1d_16/Conv1D/Squeeze) shape_signature:[-1, -1, 2560, 8], type:INT8\n",
            "  T#80(double_cnn_bi_lstm_2/batch_normalization_16/FusedBatchNormV32) shape_signature:[-1, -1, 2560, 8], type:INT8\n",
            "  T#81(double_cnn_bi_lstm_2/re_lu_16/Relu;double_cnn_bi_lstm_2/batch_normalization_16/FusedBatchNormV3) shape_signature:[-1, -1, 2560, 8], type:INT8\n",
            "  T#82(double_cnn_bi_lstm_2/max_pooling2d_8/MaxPool) shape_signature:[-1, -1, 640, 8], type:INT8\n",
            "  T#83(double_cnn_bi_lstm_2/conv1d_17/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 640, 8], type:INT8\n",
            "  T#84(double_cnn_bi_lstm_2/conv1d_17/Conv1D/Reshape) shape_signature:[-1, 1, 640, 8], type:INT8\n",
            "  T#85(double_cnn_bi_lstm_2/conv1d_17/Conv1D/Conv2D1) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#86(double_cnn_bi_lstm_2/conv1d_17/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#87(double_cnn_bi_lstm_2/conv1d_17/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#88(double_cnn_bi_lstm_2/conv1d_17/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#89(double_cnn_bi_lstm_2/conv1d_17/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#90(double_cnn_bi_lstm_2/conv1d_17/Conv1D/Squeeze) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#91(double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#92(double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#93(double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#94(double_cnn_bi_lstm_2/conv1d_17/Relu;double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/Reshape_1;double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/BiasAdd;double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/Reshape/shape;double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/Reshape;double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#95(double_cnn_bi_lstm_2/conv1d_17/Relu;double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#96(double_cnn_bi_lstm_2/batch_normalization_17/FusedBatchNormV32) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#97(double_cnn_bi_lstm_2/re_lu_17/Relu;double_cnn_bi_lstm_2/batch_normalization_17/FusedBatchNormV3) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#98(double_cnn_bi_lstm_2/conv1d_18/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#99(double_cnn_bi_lstm_2/conv1d_18/Conv1D/Reshape) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#100(double_cnn_bi_lstm_2/conv1d_18/Conv1D/Conv2D1) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#101(double_cnn_bi_lstm_2/conv1d_18/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#102(double_cnn_bi_lstm_2/conv1d_18/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#103(double_cnn_bi_lstm_2/conv1d_18/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#104(double_cnn_bi_lstm_2/conv1d_18/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#105(double_cnn_bi_lstm_2/conv1d_18/Conv1D/Squeeze) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#106(double_cnn_bi_lstm_2/conv1d_18/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#107(double_cnn_bi_lstm_2/conv1d_18/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#108(double_cnn_bi_lstm_2/conv1d_18/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#109(double_cnn_bi_lstm_2/conv1d_18/Relu;double_cnn_bi_lstm_2/conv1d_18/squeeze_batch_dims/Reshape_1;double_cnn_bi_lstm_2/conv1d_18/squeeze_batch_dims/BiasAdd;double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/Reshape/shape;double_cnn_bi_lstm_2/conv1d_18/squeeze_batch_dims/Reshape;double_cnn_bi_lstm_2/conv1d_18/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#110(double_cnn_bi_lstm_2/conv1d_18/Relu;double_cnn_bi_lstm_2/conv1d_18/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#111(double_cnn_bi_lstm_2/batch_normalization_18/FusedBatchNormV32) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#112(double_cnn_bi_lstm_2/re_lu_18/Relu;double_cnn_bi_lstm_2/batch_normalization_18/FusedBatchNormV3) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#113(double_cnn_bi_lstm_2/conv1d_19/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#114(double_cnn_bi_lstm_2/conv1d_19/Conv1D/Reshape) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#115(double_cnn_bi_lstm_2/conv1d_19/Conv1D/Conv2D1) shape_signature:[-1, 1, 640, 16], type:INT8\n",
            "  T#116(double_cnn_bi_lstm_2/conv1d_19/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#117(double_cnn_bi_lstm_2/conv1d_19/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#118(double_cnn_bi_lstm_2/conv1d_19/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#119(double_cnn_bi_lstm_2/conv1d_19/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 640, 16], type:INT8\n",
            "  T#120(double_cnn_bi_lstm_2/conv1d_19/Conv1D/Squeeze) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#121(double_cnn_bi_lstm_2/conv1d_19/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#122(double_cnn_bi_lstm_2/conv1d_19/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#123(double_cnn_bi_lstm_2/conv1d_19/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#124(double_cnn_bi_lstm_2/conv1d_19/Relu;double_cnn_bi_lstm_2/conv1d_19/squeeze_batch_dims/Reshape_1;double_cnn_bi_lstm_2/conv1d_19/squeeze_batch_dims/BiasAdd;double_cnn_bi_lstm_2/conv1d_17/squeeze_batch_dims/Reshape/shape;double_cnn_bi_lstm_2/conv1d_19/squeeze_batch_dims/Reshape;double_cnn_bi_lstm_2/conv1d_19/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#125(double_cnn_bi_lstm_2/conv1d_19/Relu;double_cnn_bi_lstm_2/conv1d_19/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#126(double_cnn_bi_lstm_2/batch_normalization_19/FusedBatchNormV32) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#127(double_cnn_bi_lstm_2/re_lu_19/Relu;double_cnn_bi_lstm_2/batch_normalization_19/FusedBatchNormV3) shape_signature:[-1, -1, 640, 16], type:INT8\n",
            "  T#128(double_cnn_bi_lstm_2/max_pooling2d_9/MaxPool) shape_signature:[-1, -1, 320, 16], type:INT8\n",
            "  T#129(double_cnn_bi_lstm_2/flatten_4/Reshape) shape_signature:[-1, 5120], type:INT8\n",
            "  T#130(double_cnn_bi_lstm_2/conv1d_20/Conv1D/Conv2D21) shape_signature:[-1, 1, 154, 8], type:INT8\n",
            "  T#131(double_cnn_bi_lstm_2/conv1d_20/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#132(double_cnn_bi_lstm_2/conv1d_20/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 154, 8], type:INT8\n",
            "  T#133(double_cnn_bi_lstm_2/conv1d_20/Conv1D/Squeeze) shape_signature:[-1, -1, 154, 8], type:INT8\n",
            "  T#134(double_cnn_bi_lstm_2/batch_normalization_20/FusedBatchNormV32) shape_signature:[-1, -1, 154, 8], type:INT8\n",
            "  T#135(double_cnn_bi_lstm_2/re_lu_20/Relu;double_cnn_bi_lstm_2/batch_normalization_20/FusedBatchNormV3) shape_signature:[-1, -1, 154, 8], type:INT8\n",
            "  T#136(double_cnn_bi_lstm_2/max_pooling2d_10/MaxPool) shape_signature:[-1, -1, 39, 8], type:INT8\n",
            "  T#137(double_cnn_bi_lstm_2/conv1d_21/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 39, 8], type:INT8\n",
            "  T#138(double_cnn_bi_lstm_2/conv1d_21/Conv1D/Reshape) shape_signature:[-1, 1, 39, 8], type:INT8\n",
            "  T#139(double_cnn_bi_lstm_2/conv1d_21/Conv1D/Conv2D1) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#140(double_cnn_bi_lstm_2/conv1d_21/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#141(double_cnn_bi_lstm_2/conv1d_21/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#142(double_cnn_bi_lstm_2/conv1d_21/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#143(double_cnn_bi_lstm_2/conv1d_21/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#144(double_cnn_bi_lstm_2/conv1d_21/Conv1D/Squeeze) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#145(double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#146(double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#147(double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#148(double_cnn_bi_lstm_2/conv1d_21/Relu;double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/Reshape_1;double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/BiasAdd;double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/Reshape/shape;double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/Reshape;double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#149(double_cnn_bi_lstm_2/conv1d_21/Relu;double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#150(double_cnn_bi_lstm_2/batch_normalization_21/FusedBatchNormV32) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#151(double_cnn_bi_lstm_2/re_lu_21/Relu;double_cnn_bi_lstm_2/batch_normalization_21/FusedBatchNormV3) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#152(double_cnn_bi_lstm_2/conv1d_22/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#153(double_cnn_bi_lstm_2/conv1d_22/Conv1D/Reshape) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#154(double_cnn_bi_lstm_2/conv1d_22/Conv1D/Conv2D1) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#155(double_cnn_bi_lstm_2/conv1d_22/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#156(double_cnn_bi_lstm_2/conv1d_22/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#157(double_cnn_bi_lstm_2/conv1d_22/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#158(double_cnn_bi_lstm_2/conv1d_22/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#159(double_cnn_bi_lstm_2/conv1d_22/Conv1D/Squeeze) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#160(double_cnn_bi_lstm_2/conv1d_22/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#161(double_cnn_bi_lstm_2/conv1d_22/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#162(double_cnn_bi_lstm_2/conv1d_22/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#163(double_cnn_bi_lstm_2/conv1d_22/Relu;double_cnn_bi_lstm_2/conv1d_22/squeeze_batch_dims/Reshape_1;double_cnn_bi_lstm_2/conv1d_22/squeeze_batch_dims/BiasAdd;double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/Reshape/shape;double_cnn_bi_lstm_2/conv1d_22/squeeze_batch_dims/Reshape;double_cnn_bi_lstm_2/conv1d_22/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#164(double_cnn_bi_lstm_2/conv1d_22/Relu;double_cnn_bi_lstm_2/conv1d_22/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#165(double_cnn_bi_lstm_2/batch_normalization_22/FusedBatchNormV32) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#166(double_cnn_bi_lstm_2/re_lu_22/Relu;double_cnn_bi_lstm_2/batch_normalization_22/FusedBatchNormV3) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#167(double_cnn_bi_lstm_2/conv1d_23/Conv1D/ExpandDims) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#168(double_cnn_bi_lstm_2/conv1d_23/Conv1D/Reshape) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#169(double_cnn_bi_lstm_2/conv1d_23/Conv1D/Conv2D21) shape_signature:[-1, 1, 39, 16], type:INT8\n",
            "  T#170(double_cnn_bi_lstm_2/conv1d_23/Conv1D/Shape) shape:[5], type:INT32\n",
            "  T#171(double_cnn_bi_lstm_2/conv1d_23/Conv1D/strided_slice) shape:[2], type:INT32\n",
            "  T#172(double_cnn_bi_lstm_2/conv1d_23/Conv1D/concat) shape:[5], type:INT32\n",
            "  T#173(double_cnn_bi_lstm_2/conv1d_23/Conv1D/Reshape_1) shape_signature:[-1, -1, 1, 39, 16], type:INT8\n",
            "  T#174(double_cnn_bi_lstm_2/conv1d_23/Conv1D/Squeeze) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#175(double_cnn_bi_lstm_2/conv1d_23/squeeze_batch_dims/Shape) shape:[4], type:INT32\n",
            "  T#176(double_cnn_bi_lstm_2/conv1d_23/squeeze_batch_dims/strided_slice) shape:[2], type:INT32\n",
            "  T#177(double_cnn_bi_lstm_2/conv1d_23/squeeze_batch_dims/concat) shape:[4], type:INT32\n",
            "  T#178(double_cnn_bi_lstm_2/conv1d_23/Relu;double_cnn_bi_lstm_2/conv1d_23/squeeze_batch_dims/Reshape_1;double_cnn_bi_lstm_2/conv1d_23/squeeze_batch_dims/BiasAdd;double_cnn_bi_lstm_2/conv1d_21/squeeze_batch_dims/Reshape/shape;double_cnn_bi_lstm_2/conv1d_23/squeeze_batch_dims/Reshape;double_cnn_bi_lstm_2/conv1d_23/squeeze_batch_dims/BiasAdd/ReadVariableOp) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#179(double_cnn_bi_lstm_2/conv1d_23/Relu;double_cnn_bi_lstm_2/conv1d_23/squeeze_batch_dims/Reshape_1) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#180(double_cnn_bi_lstm_2/batch_normalization_23/FusedBatchNormV32) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#181(double_cnn_bi_lstm_2/re_lu_23/Relu;double_cnn_bi_lstm_2/batch_normalization_23/FusedBatchNormV3) shape_signature:[-1, -1, 39, 16], type:INT8\n",
            "  T#182(double_cnn_bi_lstm_2/max_pooling2d_11/MaxPool) shape_signature:[-1, -1, 20, 16], type:INT8\n",
            "  T#183(double_cnn_bi_lstm_2/flatten_4/Reshape_1) shape_signature:[-1, 320], type:INT8\n",
            "  T#184(double_cnn_bi_lstm_2/concatenate/concat) shape_signature:[-1, 5440], type:INT8\n",
            "  T#185(double_cnn_bi_lstm_2/reshape/Shape) shape:[2], type:INT32\n",
            "  T#186(double_cnn_bi_lstm_2/reshape/strided_slice) shape:[], type:INT32\n",
            "  T#187(double_cnn_bi_lstm_2/reshape/Reshape/shape) shape:[3], type:INT32\n",
            "  T#188(double_cnn_bi_lstm_2/reshape/Reshape) shape_signature:[-1, 64, 85], type:INT8\n",
            "  T#189(double_cnn_bi_lstm_2/lstm_4/Shape) shape:[3], type:INT32\n",
            "  T#190(double_cnn_bi_lstm_2/lstm_4/strided_slice) shape:[], type:INT32\n",
            "  T#191(double_cnn_bi_lstm_2/lstm_4/zeros/packed) shape:[2], type:INT32\n",
            "  T#192(double_cnn_bi_lstm_2/lstm_4/zeros) shape_signature:[-1, 16], type:INT8\n",
            "  T#193(tfl.dequantize) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#194(transpose) shape_signature:[64, -1, 85], type:INT8\n",
            "  T#195(transpose1) shape_signature:[64, -1, 85], type:FLOAT32\n",
            "  T#196(while;while1;while2;while3;while4;while5) shape:[], type:INT32\n",
            "  T#197(while;while1;while2;while3;while4;while51) shape:[], type:INT32\n",
            "  T#198(while;while1;while2;while3;while4;while52) shape_signature:[64, -1, 16], type:FLOAT32\n",
            "  T#199(while;while1;while2;while3;while4;while53) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#200(while;while1;while2;while3;while4;while54) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#201(while;while1;while2;while3;while4;while55) shape_signature:[64, -1, 85], type:FLOAT32\n",
            "  T#202(tfl.quantize1) shape_signature:[64, -1, 16], type:INT8\n",
            "  T#203(transpose_1) shape_signature:[-1, 64, 16], type:INT8\n",
            "  T#204(double_cnn_bi_lstm_2/lstm_5/Shape) shape:[3], type:INT32\n",
            "  T#205(double_cnn_bi_lstm_2/lstm_5/strided_slice) shape:[], type:INT32\n",
            "  T#206(double_cnn_bi_lstm_2/lstm_5/zeros/packed) shape:[2], type:INT32\n",
            "  T#207(double_cnn_bi_lstm_2/lstm_5/zeros) shape_signature:[-1, 16], type:INT8\n",
            "  T#208(tfl.dequantize1) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#209(while6;while7;while8;while9;while10;while11) shape:[], type:INT32\n",
            "  T#210(while6;while7;while8;while9;while10;while111) shape:[], type:INT32\n",
            "  T#211(while6;while7;while8;while9;while10;while112) shape_signature:[1, -1, 16], type:FLOAT32\n",
            "  T#212(while6;while7;while8;while9;while10;while113) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#213(while6;while7;while8;while9;while10;while114) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#214(while6;while7;while8;while9;while10;while115) shape_signature:[64, -1, 16], type:FLOAT32\n",
            "  T#215(tfl.quantize2) shape_signature:[1, -1, 16], type:INT8\n",
            "  T#216(strided_slice_23) shape_signature:[-1, 16], type:INT8\n",
            "  T#217(double_cnn_bi_lstm_2/dense_5/MatMul;double_cnn_bi_lstm_2/dense_5/BiasAdd) shape_signature:[-1, 5], type:INT8\n",
            "  T#218(StatefulPartitionedCall:01) shape_signature:[-1, 5], type:INT8\n",
            "  T#219(StatefulPartitionedCall:0) shape_signature:[-1, 5], type:FLOAT32\n",
            "\n",
            "Subgraph#1 while_cond(T#1_0, T#1_1, T#1_2, T#1_3, T#1_4, T#1_5) -> [T#1_7]\n",
            "  Op#0 LESS(T#1_1, T#1_6[64]) -> [T#1_7]\n",
            "\n",
            "Tensors of Subgraph#1\n",
            "  T#1_0(arg0) shape:[], type:INT32\n",
            "  T#1_1(arg1) shape:[], type:INT32\n",
            "  T#1_2(arg2) shape_signature:[64, -1, 16], type:FLOAT32\n",
            "  T#1_3(arg3) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#1_4(arg4) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#1_5(arg5) shape_signature:[64, -1, 85], type:FLOAT32\n",
            "  T#1_6(double_cnn_bi_lstm_2/reshape/Reshape/shape/11) shape:[], type:INT32 RO 4 bytes, buffer: 22, data:[64]\n",
            "  T#1_7(while/Less) shape:[], type:BOOL\n",
            "\n",
            "Subgraph#2 while_body(T#2_0, T#2_1, T#2_2, T#2_3, T#2_4, T#2_5) -> [T#2_50, T#2_20, T#2_49, T#2_40, T#2_37, T#2_5]\n",
            "  Op#0 QUANTIZE(T#2_2) -> [T#2_13]\n",
            "  Op#1 QUANTIZE(T#2_3) -> [T#2_17]\n",
            "  Op#2 QUANTIZE(T#2_4) -> [T#2_18]\n",
            "  Op#3 QUANTIZE(T#2_5) -> [T#2_19]\n",
            "  Op#4 ADD(T#2_1, T#2_6[1]) -> [T#2_20]\n",
            "  Op#5 FULLY_CONNECTED(T#2_17, T#2_16, T#-1) -> [T#2_21]\n",
            "  Op#6 GATHER(T#2_19, T#2_1) -> [T#2_22]\n",
            "  Op#7 FULLY_CONNECTED(T#2_22, T#2_15, T#-1) -> [T#2_23]\n",
            "  Op#8 ADD(T#2_23, T#2_21) -> [T#2_24]\n",
            "  Op#9 ADD(T#2_24, T#2_14) -> [T#2_25]\n",
            "  Op#10 SPLIT(T#2_6[1], T#2_25) -> [T#2_26, T#2_27, T#2_28, T#2_29]\n",
            "  Op#11 LOGISTIC(T#2_26) -> [T#2_30]\n",
            "  Op#12 LOGISTIC(T#2_27) -> [T#2_31]\n",
            "  Op#13 MUL(T#2_31, T#2_18) -> [T#2_32]\n",
            "  Op#14 LOGISTIC(T#2_29) -> [T#2_33]\n",
            "  Op#15 TANH(T#2_28) -> [T#2_34]\n",
            "  Op#16 MUL(T#2_30, T#2_34) -> [T#2_35]\n",
            "  Op#17 ADD(T#2_32, T#2_35) -> [T#2_36]\n",
            "  Op#18 DEQUANTIZE(T#2_36) -> [T#2_37]\n",
            "  Op#19 TANH(T#2_36) -> [T#2_38]\n",
            "  Op#20 MUL(T#2_33, T#2_38) -> [T#2_39]\n",
            "  Op#21 DEQUANTIZE(T#2_39) -> [T#2_40]\n",
            "  Op#22 RESHAPE(T#2_1, T#2_7[1]) -> [T#2_41]\n",
            "  Op#23 CONCATENATION(T#2_41, T#2_8[-1, -1]) -> [T#2_42]\n",
            "  Op#24 SLICE(T#2_13, T#2_9[0, 0, 0], T#2_42) -> [T#2_43]\n",
            "  Op#25 RESHAPE(T#2_20, T#2_7[1]) -> [T#2_44]\n",
            "  Op#26 CONCATENATION(T#2_44, T#2_10[0, 0]) -> [T#2_45]\n",
            "  Op#27 SLICE(T#2_13, T#2_45, T#2_11[-1, -1, -1]) -> [T#2_46]\n",
            "  Op#28 EXPAND_DIMS(T#2_39, T#2_12[0]) -> [T#2_47]\n",
            "  Op#29 CONCATENATION(T#2_43, T#2_47, T#2_46) -> [T#2_48]\n",
            "  Op#30 DEQUANTIZE(T#2_48) -> [T#2_49]\n",
            "  Op#31 ADD(T#2_0, T#2_6[1]) -> [T#2_50]\n",
            "\n",
            "Tensors of Subgraph#2\n",
            "  T#2_0(arg0) shape:[], type:INT32\n",
            "  T#2_1(arg1) shape:[], type:INT32\n",
            "  T#2_2(arg2) shape_signature:[64, -1, 16], type:FLOAT32\n",
            "  T#2_3(arg3) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#2_4(arg4) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#2_5(arg5) shape_signature:[64, -1, 85], type:FLOAT32\n",
            "  T#2_6(double_cnn_bi_lstm_2/concatenate/concat/axis1) shape:[], type:INT32 RO 4 bytes, buffer: 8, data:[1]\n",
            "  T#2_7(double_cnn_bi_lstm_2/conv1d_16/Conv1D/strided_slice/stack_21) shape:[1], type:INT32 RO 4 bytes, buffer: 8, data:[1]\n",
            "  T#2_8(while/TensorArrayV2Write/TensorListSetItem1) shape:[2], type:INT32 RO 8 bytes, buffer: 237, data:[-1, -1]\n",
            "  T#2_9(while/TensorArrayV2Write/TensorListSetItem3) shape:[3], type:INT32 RO 12 bytes, buffer: 238, data:[0, 0, 0]\n",
            "  T#2_10(while/TensorArrayV2Write/TensorListSetItem2) shape:[2], type:INT32 RO 8 bytes, buffer: 239, data:[0, 0]\n",
            "  T#2_11(while/TensorArrayV2Write/TensorListSetItem) shape:[3], type:INT32 RO 12 bytes, buffer: 240, data:[-1, -1, -1]\n",
            "  T#2_12(time1) shape:[], type:INT32 RO 4 bytes, buffer: 6, data:[0]\n",
            "  T#2_13(tfl.quantize3) shape_signature:[64, -1, 16], type:INT8\n",
            "  T#2_14(double_cnn_bi_lstm_2/lstm_4/Read_2/ReadVariableOp1) shape:[64], type:INT8 RO 64 bytes, buffer: 243, data:[., ., ., ., ., ...]\n",
            "  T#2_15(while/MatMul) shape:[64, 85], type:INT8 RO 5440 bytes, buffer: 244, data:[., ., ., ., ., ...]\n",
            "  T#2_16(while/MatMul_11) shape:[64, 16], type:INT8 RO 1024 bytes, buffer: 245, data:[., ., ., ., ., ...]\n",
            "  T#2_17(tfl.quantize4) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_18(tfl.quantize5) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_19(tfl.quantize6) shape_signature:[64, -1, 85], type:INT8\n",
            "  T#2_20(while/add_2) shape:[], type:INT32\n",
            "  T#2_21(while/MatMul_12) shape_signature:[-1, 64], type:INT8\n",
            "  T#2_22(while/TensorArrayV2Read/TensorListGetItem;while/TensorArrayV2Write/TensorListSetItem) shape_signature:[-1, 85], type:INT8\n",
            "  T#2_23(while/MatMul1) shape_signature:[-1, 64], type:INT8\n",
            "  T#2_24(while/add) shape_signature:[-1, 64], type:INT8\n",
            "  T#2_25(while/BiasAdd) shape_signature:[-1, 64], type:INT8\n",
            "  T#2_26(while/split;while/split1;while/split2;while/split3) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_27(while/split;while/split1;while/split2;while/split31) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_28(while/split;while/split1;while/split2;while/split32) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_29(while/split;while/split1;while/split2;while/split33) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_30(while/Sigmoid) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_31(while/Sigmoid_1) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_32(while/mul) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_33(while/Sigmoid_2) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_34(while/Tanh) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_35(while/mul_1) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_36(while/add_1) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_37(tfl.dequantize2) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#2_38(while/Tanh_1) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_39(while/mul_2) shape_signature:[-1, 16], type:INT8\n",
            "  T#2_40(while/mul_21) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#2_41(while/TensorArrayV2Write/TensorListSetItem4) shape:[1], type:INT32\n",
            "  T#2_42(while/TensorArrayV2Write/TensorListSetItem5) shape:[3], type:INT32\n",
            "  T#2_43(while/TensorArrayV2Write/TensorListSetItem6) shape_signature:[-1, -1, -1], type:INT8\n",
            "  T#2_44(while/TensorArrayV2Write/TensorListSetItem7) shape:[1], type:INT32\n",
            "  T#2_45(while/TensorArrayV2Write/TensorListSetItem8) shape:[3], type:INT32\n",
            "  T#2_46(while/TensorArrayV2Write/TensorListSetItem9) shape_signature:[-1, -1, 16], type:INT8\n",
            "  T#2_47(while/TensorArrayV2Write/TensorListSetItem10) shape_signature:[1, -1, 16], type:INT8\n",
            "  T#2_48(while/TensorArrayV2Write/TensorListSetItem11) shape_signature:[64, -1, 16], type:INT8\n",
            "  T#2_49(tfl.dequantize3) shape_signature:[64, -1, 16], type:FLOAT32\n",
            "  T#2_50(while/add_3) shape:[], type:INT32\n",
            "\n",
            "Subgraph#3 while1_cond(T#3_0, T#3_1, T#3_2, T#3_3, T#3_4, T#3_5) -> [T#3_7]\n",
            "  Op#0 LESS(T#3_1, T#3_6[64]) -> [T#3_7]\n",
            "\n",
            "Tensors of Subgraph#3\n",
            "  T#3_0(arg0) shape:[], type:INT32\n",
            "  T#3_1(arg1) shape:[], type:INT32\n",
            "  T#3_2(arg2) shape_signature:[1, -1, 16], type:FLOAT32\n",
            "  T#3_3(arg3) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#3_4(arg4) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#3_5(arg5) shape_signature:[64, -1, 16], type:FLOAT32\n",
            "  T#3_6(double_cnn_bi_lstm_2/reshape/Reshape/shape/12) shape:[], type:INT32 RO 4 bytes, buffer: 22, data:[64]\n",
            "  T#3_7(while/Less1) shape:[], type:BOOL\n",
            "\n",
            "Subgraph#4 while1_body(T#4_0, T#4_1, T#4_2, T#4_3, T#4_4, T#4_5) -> [T#4_45, T#4_19, T#4_44, T#4_39, T#4_36, T#4_5]\n",
            "  Op#0 QUANTIZE(T#4_2) -> [T#4_15]\n",
            "  Op#1 QUANTIZE(T#4_3) -> [T#4_16]\n",
            "  Op#2 QUANTIZE(T#4_4) -> [T#4_17]\n",
            "  Op#3 QUANTIZE(T#4_5) -> [T#4_18]\n",
            "  Op#4 ADD(T#4_1, T#4_6[1]) -> [T#4_19]\n",
            "  Op#5 FULLY_CONNECTED(T#4_16, T#4_14, T#-1) -> [T#4_20]\n",
            "  Op#6 GATHER(T#4_18, T#4_1) -> [T#4_21]\n",
            "  Op#7 FULLY_CONNECTED(T#4_21, T#4_13, T#-1) -> [T#4_22]\n",
            "  Op#8 ADD(T#4_22, T#4_20) -> [T#4_23]\n",
            "  Op#9 ADD(T#4_23, T#4_12) -> [T#4_24]\n",
            "  Op#10 SPLIT(T#4_6[1], T#4_24) -> [T#4_25, T#4_26, T#4_27, T#4_28]\n",
            "  Op#11 LOGISTIC(T#4_25) -> [T#4_29]\n",
            "  Op#12 LOGISTIC(T#4_26) -> [T#4_30]\n",
            "  Op#13 MUL(T#4_30, T#4_17) -> [T#4_31]\n",
            "  Op#14 LOGISTIC(T#4_28) -> [T#4_32]\n",
            "  Op#15 TANH(T#4_27) -> [T#4_33]\n",
            "  Op#16 MUL(T#4_29, T#4_33) -> [T#4_34]\n",
            "  Op#17 ADD(T#4_31, T#4_34) -> [T#4_35]\n",
            "  Op#18 DEQUANTIZE(T#4_35) -> [T#4_36]\n",
            "  Op#19 TANH(T#4_35) -> [T#4_37]\n",
            "  Op#20 MUL(T#4_32, T#4_37) -> [T#4_38]\n",
            "  Op#21 DEQUANTIZE(T#4_38) -> [T#4_39]\n",
            "  Op#22 SLICE(T#4_15, T#4_11[0, 0, 0], T#4_10[0, -1, -1]) -> [T#4_40]\n",
            "  Op#23 SLICE(T#4_15, T#4_9[1, 0, 0], T#4_8[-1, -1, -1]) -> [T#4_41]\n",
            "  Op#24 EXPAND_DIMS(T#4_38, T#4_7[0]) -> [T#4_42]\n",
            "  Op#25 CONCATENATION(T#4_40, T#4_42, T#4_41) -> [T#4_43]\n",
            "  Op#26 DEQUANTIZE(T#4_43) -> [T#4_44]\n",
            "  Op#27 ADD(T#4_0, T#4_6[1]) -> [T#4_45]\n",
            "\n",
            "Tensors of Subgraph#4\n",
            "  T#4_0(arg0) shape:[], type:INT32\n",
            "  T#4_1(arg1) shape:[], type:INT32\n",
            "  T#4_2(arg2) shape_signature:[1, -1, 16], type:FLOAT32\n",
            "  T#4_3(arg3) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#4_4(arg4) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#4_5(arg5) shape_signature:[64, -1, 16], type:FLOAT32\n",
            "  T#4_6(double_cnn_bi_lstm_2/concatenate/concat/axis2) shape:[], type:INT32 RO 4 bytes, buffer: 8, data:[1]\n",
            "  T#4_7(time2) shape:[], type:INT32 RO 4 bytes, buffer: 6, data:[0]\n",
            "  T#4_8(while/TensorArrayV2Write/TensorListSetItem12) shape:[3], type:INT32 RO 12 bytes, buffer: 240, data:[-1, -1, -1]\n",
            "  T#4_9(while/TensorArrayV2Write/TensorListSetItem13) shape:[3], type:INT32 RO 12 bytes, buffer: 297, data:[1, 0, 0]\n",
            "  T#4_10(while/TensorArrayV2Write/TensorListSetItem14) shape:[3], type:INT32 RO 12 bytes, buffer: 298, data:[0, -1, -1]\n",
            "  T#4_11(while/TensorArrayV2Write/TensorListSetItem15) shape:[3], type:INT32 RO 12 bytes, buffer: 238, data:[0, 0, 0]\n",
            "  T#4_12(double_cnn_bi_lstm_2/lstm_5/Read_2/ReadVariableOp1) shape:[64], type:INT8 RO 64 bytes, buffer: 300, data:[., ., ., ., ., ...]\n",
            "  T#4_13(while/MatMul2) shape:[64, 16], type:INT8 RO 1024 bytes, buffer: 301, data:[., ., ., ., ., ...]\n",
            "  T#4_14(while/MatMul_14) shape:[64, 16], type:INT8 RO 1024 bytes, buffer: 302, data:[!, ., -, ,, ., ...]\n",
            "  T#4_15(tfl.quantize7) shape_signature:[1, -1, 16], type:INT8\n",
            "  T#4_16(tfl.quantize8) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_17(tfl.quantize9) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_18(tfl.quantize10) shape_signature:[64, -1, 16], type:INT8\n",
            "  T#4_19(while/add_21) shape:[], type:INT32\n",
            "  T#4_20(while/MatMul_15) shape_signature:[-1, 64], type:INT8\n",
            "  T#4_21(while/TensorArrayV2Read/TensorListGetItem;time) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_22(while/MatMul3) shape_signature:[-1, 64], type:INT8\n",
            "  T#4_23(while/add1) shape_signature:[-1, 64], type:INT8\n",
            "  T#4_24(while/BiasAdd1) shape_signature:[-1, 64], type:INT8\n",
            "  T#4_25(while/split4;while/split5;while/split6;while/split7) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_26(while/split4;while/split5;while/split6;while/split71) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_27(while/split4;while/split5;while/split6;while/split72) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_28(while/split4;while/split5;while/split6;while/split73) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_29(while/Sigmoid1) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_30(while/Sigmoid_11) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_31(while/mul1) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_32(while/Sigmoid_21) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_33(while/Tanh1) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_34(while/mul_11) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_35(while/add_11) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_36(tfl.dequantize4) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#4_37(while/Tanh_11) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_38(while/mul_211) shape_signature:[-1, 16], type:INT8\n",
            "  T#4_39(while/mul_212) shape_signature:[-1, 16], type:FLOAT32\n",
            "  T#4_40(while/TensorArrayV2Write/TensorListSetItem16) shape_signature:[0, -1, 16], type:INT8\n",
            "  T#4_41(while/TensorArrayV2Write/TensorListSetItem17) shape_signature:[0, -1, 16], type:INT8\n",
            "  T#4_42(while/TensorArrayV2Write/TensorListSetItem18) shape_signature:[1, -1, 16], type:INT8\n",
            "  T#4_43(while/TensorArrayV2Write/TensorListSetItem19) shape_signature:[1, -1, 16], type:INT8\n",
            "  T#4_44(tfl.dequantize5) shape_signature:[1, -1, 16], type:FLOAT32\n",
            "  T#4_45(while/add_31) shape:[], type:INT32\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Your TFLite model has '1' signature_def(s).\n",
            "\n",
            "Signature#0 key: 'serving_default'\n",
            "- Subgraph: Subgraph#0\n",
            "- Inputs: \n",
            "    'input_1' : T#0\n",
            "- Outputs: \n",
            "    'output_1' : T#219\n",
            "\n",
            "---------------------------------------------------------------\n",
            "              Model size:      81752 bytes\n",
            "    Non-data buffer size:      59895 bytes (73.26 %)\n",
            "  Total data buffer size:      21857 bytes (26.74 %)\n",
            "          - Subgraph#0  :      13474 bytes (16.48 %)\n",
            "          - Subgraph#1  :          4 bytes (00.00 %)\n",
            "          - Subgraph#2  :       6580 bytes (08.05 %)\n",
            "          - Subgraph#3  :          4 bytes (00.00 %)\n",
            "          - Subgraph#4  :       2168 bytes (02.65 %)\n",
            "    (Zero value buffers):       4217 bytes (05.16 %)\n",
            "\n",
            "* Buffers of TFLite model are mostly used for constant tensors.\n",
            "  And zero value buffers are buffers filled with zeros.\n",
            "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
            "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_eg-2Ys11-4"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "Vh7FlljnIEdM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kVLjD9J-I49"
      },
      "outputs": [],
      "source": [
        "def test_preprocess_data(data_dir):\n",
        "    # Load all files\n",
        "    # print('data_dir:',data_dir)\n",
        "    test_files = os.listdir(data_dir)\n",
        "    \n",
        "    test_files = sorted(test_files)\n",
        "    \n",
        "    x_val, y_val = load_npz_list_files(data_dir, test_files)\n",
        "    # print('data_train',data_train.shape) # \n",
        "    # print('x_val',x_val.shape) # (2884, 1, 7680)\n",
        "    # print('label_train',label_train.shape) # (2796,)\n",
        "    # print('y_val',y_val.shape) # (2884,)\n",
        "    \n",
        "    # Reshape the data to match the input of the model - conv2d\n",
        "    x_val = np.squeeze(x_val) # (1812, 7680)\n",
        "    x_val = x_val[:, :, np.newaxis, np.newaxis] # (1812, 7680, 1, 1)\n",
        "    \n",
        "    # Casting\n",
        "    x_val = x_val.astype(np.float32)\n",
        "    y_val = y_val.astype(np.int32)\n",
        "        \n",
        "    print(\" \")\n",
        "    return x_val, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3WnnfWR_qky",
        "outputId": "463cc0c5-e644-483a-96bc-5558ac6b7b66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "[ 0 ]= 4  [ 1 ]= 2  [ 2 ]= 2  [ 3 ]= 0  [ 4 ]= 1  [ 5 ]= 2  [ 6 ]= 0  [ 7 ]= 1  [ 8 ]= 4  [ 9 ]= 2  [ 10 ]= 4  [ 11 ]= 2  [ 12 ]= 2  [ 13 ]= 2  [ 14 ]= 2  [ 15 ]= 1  [ 16 ]= 2  [ 17 ]= 2  [ 18 ]= 2  [ 19 ]= 2  [ 20 ]= 2  [ 21 ]= 2  [ 22 ]= 1  [ 23 ]= 2  [ 24 ]= 3  [ 25 ]= 2  [ 26 ]= 3  [ 27 ]= 3  [ 28 ]= 2  [ 29 ]= 2  [ 30 ]= 2  [ 31 ]= 3  [ 32 ]= 3  [ 33 ]= 0  [ 34 ]= 2  [ 35 ]= 2  [ 36 ]= 2  [ 37 ]= 4  [ 38 ]= 2  [ 39 ]= 2  [ 40 ]= 2  [ 41 ]= 4  [ 42 ]= 2  [ 43 ]= 0  [ 44 ]= 4  [ 45 ]= 2  [ 46 ]= 2  [ 47 ]= 1  [ 48 ]= 2  [ 49 ]= 2  [ 50 ]= 1  [ 51 ]= 2  [ 52 ]= 2  [ 53 ]= 0  [ 54 ]= 2  [ 55 ]= 2  [ 56 ]= 0  [ 57 ]= 1  [ 58 ]= 2  [ 59 ]= 1  [ 60 ]= 4  [ 61 ]= 2  [ 62 ]= 4  [ 63 ]= 4  [ 64 ]= 1  [ 65 ]= 0  [ 66 ]= 0  [ 67 ]= 2  [ 68 ]= 2  [ 69 ]= 0  [ 70 ]= 0  [ 71 ]= 4  [ 72 ]= 4  [ 73 ]= 2  [ 74 ]= 4  [ 75 ]= 2  [ 76 ]= 1  [ 77 ]= 2  [ 78 ]= 3  [ 79 ]= 2  [ 80 ]= 2  [ 81 ]= 2  [ 82 ]= 2  [ 83 ]= 2  [ 84 ]= 4  [ 85 ]= 1  [ 86 ]= 1  [ 87 ]= 2  [ 88 ]= 2  [ 89 ]= 3  [ 90 ]= 2  [ 91 ]= 2  [ 92 ]= 4  [ 93 ]= 4  [ 94 ]= 4  [ 95 ]= 2  [ 96 ]= 2  [ 97 ]= 2  [ 98 ]= 1  [ 99 ]= 2  [ 100 ]= 4  [ 101 ]= 4  [ 102 ]= 4  [ 103 ]= 2  [ 104 ]= 4  [ 105 ]= 0  [ 106 ]= 3  [ 107 ]= 2  [ 108 ]= 2  [ 109 ]= 4  [ 110 ]= 3  [ 111 ]= 2  [ 112 ]= 2  [ 113 ]= 4  [ 114 ]= 1  [ 115 ]= 2  [ 116 ]= 2  [ 117 ]= 2  [ 118 ]= 1  [ 119 ]= 4  [ 120 ]= 0  [ 121 ]= 1  [ 122 ]= 2  [ 123 ]= 2  [ 124 ]= 2  [ 125 ]= 2  [ 126 ]= 2  [ 127 ]= 4  [ 128 ]= 2  [ 129 ]= 2  [ 130 ]= 2  [ 131 ]= 2  [ 132 ]= 2  [ 133 ]= 4  [ 134 ]= 2  [ 135 ]= 2  [ 136 ]= 4  [ 137 ]= 2  [ 138 ]= 0  [ 139 ]= 1  [ 140 ]= 2  [ 141 ]= 2  [ 142 ]= 2  [ 143 ]= 1  [ 144 ]= 2  [ 145 ]= 3  [ 146 ]= 1  [ 147 ]= 2  [ 148 ]= 0  [ 149 ]= 2  [ 150 ]= 2  [ 151 ]= 1  [ 152 ]= 0  [ 153 ]= 2  [ 154 ]= 2  [ 155 ]= 4  [ 156 ]= 2  [ 157 ]= 3  [ 158 ]= 2  [ 159 ]= 2  [ 160 ]= 2  [ 161 ]= 2  [ 162 ]= 2  [ 163 ]= 4  [ 164 ]= 0  [ 165 ]= 2  [ 166 ]= 2  [ 167 ]= 2  [ 168 ]= 4  [ 169 ]= 2  [ 170 ]= 4  [ 171 ]= 2  [ 172 ]= 2  [ 173 ]= 2  [ 174 ]= 2  [ 175 ]= 3  [ 176 ]= 4  [ 177 ]= 2  [ 178 ]= 2  [ 179 ]= 1  [ 180 ]= 4  [ 181 ]= 2  [ 182 ]= 0  [ 183 ]= 2  [ 184 ]= 2  [ 185 ]= 1  [ 186 ]= 2  [ 187 ]= 3  [ 188 ]= 2  [ 189 ]= 2  [ 190 ]= 2  [ 191 ]= 2  [ 192 ]= 0  [ 193 ]= 2  [ 194 ]= 2  [ 195 ]= 2  [ 196 ]= 2  [ 197 ]= 2  [ 198 ]= 2  [ 199 ]= 1  [ 200 ]= 1  [ 201 ]= 2  [ 202 ]= 0  [ 203 ]= 0  [ 204 ]= 2  [ 205 ]= 4  [ 206 ]= 2  [ 207 ]= 2  [ 208 ]= 2  [ 209 ]= 3  [ 210 ]= 2  [ 211 ]= 2  [ 212 ]= 2  [ 213 ]= 2  [ 214 ]= 2  [ 215 ]= 2  [ 216 ]= 2  [ 217 ]= 2  [ 218 ]= 2  [ 219 ]= 1  [ 220 ]= 1  [ 221 ]= 4  [ 222 ]= 0  [ 223 ]= 2  [ 224 ]= 2  [ 225 ]= 2  [ 226 ]= 2  [ 227 ]= 0  [ 228 ]= 2  [ 229 ]= 4  [ 230 ]= 2  [ 231 ]= 1  [ 232 ]= 4  [ 233 ]= 0  [ 234 ]= 2  [ 235 ]= 4  [ 236 ]= 2  [ 237 ]= 1  [ 238 ]= 4  [ 239 ]= 2  [ 240 ]= 2  [ 241 ]= 2  [ 242 ]= 4  [ 243 ]= 2  [ 244 ]= 2  [ 245 ]= 2  [ 246 ]= 2  [ 247 ]= 0  [ 248 ]= 2  [ 249 ]= 2  [ 250 ]= 2  [ 251 ]= 0  [ 252 ]= 2  [ 253 ]= 2  [ 254 ]= 0  [ 255 ]= 2  [ 256 ]= 2  [ 257 ]= 2  [ 258 ]= 2  [ 259 ]= 2  [ 260 ]= 0  [ 261 ]= 2  [ 262 ]= 2  [ 263 ]= 2  [ 264 ]= 2  [ 265 ]= 2  [ 266 ]= 4  [ 267 ]= 2  [ 268 ]= 3  [ 269 ]= 2  [ 270 ]= 2  [ 271 ]= 2  [ 272 ]= 2  [ 273 ]= 2  [ 274 ]= 2  [ 275 ]= 3  [ 276 ]= 2  [ 277 ]= 2  [ 278 ]= 2  [ 279 ]= 2  [ 280 ]= 2  [ 281 ]= 4  [ 282 ]= 2  [ 283 ]= 2  [ 284 ]= 2  [ 285 ]= 4  [ 286 ]= 2  [ 287 ]= 2  [ 288 ]= 3  [ 289 ]= 2  [ 290 ]= 2  [ 291 ]= 2  [ 292 ]= 2  [ 293 ]= 1  [ 294 ]= 2  [ 295 ]= 2  [ 296 ]= 1  [ 297 ]= 4  [ 298 ]= 2  [ 299 ]= 2  [ 300 ]= 3  [ 301 ]= 2  [ 302 ]= 4  [ 303 ]= 0  [ 304 ]= 2  [ 305 ]= 0  [ 306 ]= 2  [ 307 ]= 3  [ 308 ]= 2  [ 309 ]= 2  [ 310 ]= 2  [ 311 ]= 4  [ 312 ]= 4  [ 313 ]= 3  [ 314 ]= 2  [ 315 ]= 1  [ 316 ]= 4  [ 317 ]= 1  [ 318 ]= 2  [ 319 ]= 1  [ 320 ]= 2  [ 321 ]= 2  [ 322 ]= 4  [ 323 ]= 1  [ 324 ]= 4  [ 325 ]= 2  [ 326 ]= 1  [ 327 ]= 2  [ 328 ]= 2  [ 329 ]= 2  [ 330 ]= 2  [ 331 ]= 0  [ 332 ]= 2  [ 333 ]= 2  [ 334 ]= 1  [ 335 ]= 2  [ 336 ]= 2  [ 337 ]= 1  [ 338 ]= 3  [ 339 ]= 2  [ 340 ]= 2  [ 341 ]= 2  [ 342 ]= 1  [ 343 ]= 2  [ 344 ]= 2  [ 345 ]= 0  [ 346 ]= 2  [ 347 ]= 2  [ 348 ]= 1  [ 349 ]= 2  [ 350 ]= 1  [ 351 ]= 2  [ 352 ]= 2  [ 353 ]= 2  [ 354 ]= 2  [ 355 ]= 0  [ 356 ]= 2  [ 357 ]= 2  [ 358 ]= 3  [ 359 ]= 2  [ 360 ]= 4  [ 361 ]= 4  [ 362 ]= 2  [ 363 ]= 0  [ 364 ]= 4  [ 365 ]= 3  [ 366 ]= 0  [ 367 ]= 2  [ 368 ]= 2  [ 369 ]= 2  [ 370 ]= 0  [ 371 ]= 4  [ 372 ]= 2  [ 373 ]= 1  [ 374 ]= 2  [ 375 ]= 1  [ 376 ]= 2  [ 377 ]= 2  [ 378 ]= 4  [ 379 ]= 2  [ 380 ]= 2  [ 381 ]= 2  [ 382 ]= 2  [ 383 ]= 1  [ 384 ]= 4  [ 385 ]= 2  [ 386 ]= 4  [ 387 ]= 0  [ 388 ]= 2  [ 389 ]= 2  [ 390 ]= 0  [ 391 ]= 2  [ 392 ]= 1  [ 393 ]= 2  [ 394 ]= 2  [ 395 ]= 3  [ 396 ]= 4  [ 397 ]= 2  [ 398 ]= 3  [ 399 ]= 1  [ 400 ]= 2  [ 401 ]= 2  [ 402 ]= 1  [ 403 ]= 2  [ 404 ]= 4  [ 405 ]= 2  [ 406 ]= 2  [ 407 ]= 2  [ 408 ]= 4  [ 409 ]= 4  [ 410 ]= 3  [ 411 ]= 2  [ 412 ]= 2  [ 413 ]= 2  [ 414 ]= 1  [ 415 ]= 4  [ 416 ]= 2  [ 417 ]= 2  [ 418 ]= 2  [ 419 ]= 2  [ 420 ]= 2  [ 421 ]= 0  [ 422 ]= 4  [ 423 ]= 2  [ 424 ]= 2  [ 425 ]= 2  [ 426 ]= 1  [ 427 ]= 4  [ 428 ]= 2  [ 429 ]= 0  [ 430 ]= 2  [ 431 ]= 2  [ 432 ]= 2  [ 433 ]= 3  [ 434 ]= 4  [ 435 ]= 1  [ 436 ]= 3  [ 437 ]= 2  [ 438 ]= 4  [ 439 ]= 2  [ 440 ]= 2  [ 441 ]= 2  [ 442 ]= 0  [ 443 ]= 1  [ 444 ]= 4  [ 445 ]= 3  [ 446 ]= 0  [ 447 ]= 1  [ 448 ]= 2  [ 449 ]= 4  [ 450 ]= 4  [ 451 ]= 4  [ 452 ]= 2  [ 453 ]= 2  [ 454 ]= 2  [ 455 ]= 2  [ 456 ]= 2  [ 457 ]= 2  [ 458 ]= 2  [ 459 ]= 2  [ 460 ]= 2  [ 461 ]= 2  [ 462 ]= 2  [ 463 ]= 2  [ 464 ]= 2  [ 465 ]= 0  [ 466 ]= 2  [ 467 ]= 1  [ 468 ]= 2  [ 469 ]= 2  [ 470 ]= 2  [ 471 ]= 4  [ 472 ]= 4  [ 473 ]= 1  [ 474 ]= 2  [ 475 ]= 2  [ 476 ]= 1  [ 477 ]= 2  [ 478 ]= 2  [ 479 ]= 2  [ 480 ]= 4  [ 481 ]= 2  [ 482 ]= 2  [ 483 ]= 2  [ 484 ]= 4  [ 485 ]= 4  [ 486 ]= 2  [ 487 ]= 1  [ 488 ]= 4  [ 489 ]= 2  [ 490 ]= 2  [ 491 ]= 3  [ 492 ]= 2  [ 493 ]= 2  [ 494 ]= 2  [ 495 ]= 2  [ 496 ]= 2  [ 497 ]= 4  [ 498 ]= 2  [ 499 ]= 4  [ 500 ]= 2  [ 501 ]= 1  [ 502 ]= 1  [ 503 ]= 3  [ 504 ]= 0  [ 505 ]= 1  [ 506 ]= 2  [ 507 ]= 0  [ 508 ]= 2  [ 509 ]= 2  [ 510 ]= 2  [ 511 ]= 2  [ 512 ]= 2  [ 513 ]= 4  [ 514 ]= 2  [ 515 ]= 2  [ 516 ]= 0  [ 517 ]= 2  [ 518 ]= 2  [ 519 ]= 2  [ 520 ]= 2  [ 521 ]= 2  [ 522 ]= 4  [ 523 ]= 2  [ 524 ]= 1  [ 525 ]= 2  [ 526 ]= 2  [ 527 ]= 4  [ 528 ]= 2  [ 529 ]= 2  [ 530 ]= 0  [ 531 ]= 2  [ 532 ]= 2  [ 533 ]= 0  [ 534 ]= 2  [ 535 ]= 4  [ 536 ]= 2  [ 537 ]= 4  [ 538 ]= 2  [ 539 ]= 2  [ 540 ]= 4  [ 541 ]= 3  [ 542 ]= 2  [ 543 ]= 2  [ 544 ]= 4  [ 545 ]= 1  [ 546 ]= 4  [ 547 ]= 2  [ 548 ]= 2  [ 549 ]= 4  [ 550 ]= 0  [ 551 ]= 2  [ 552 ]= 3  [ 553 ]= 4  [ 554 ]= 2  [ 555 ]= 0  [ 556 ]= 2  [ 557 ]= 4  [ 558 ]= 2  [ 559 ]= 0  [ 560 ]= 0  [ 561 ]= 1  [ 562 ]= 2  [ 563 ]= 2  [ 564 ]= 2  [ 565 ]= 0  [ 566 ]= 2  [ 567 ]= 0  [ 568 ]= 4  [ 569 ]= 0  [ 570 ]= 0  [ 571 ]= 2  [ 572 ]= 2  [ 573 ]= 2  [ 574 ]= 2  [ 575 ]= 2  [ 576 ]= 2  [ 577 ]= 2  [ 578 ]= 0  [ 579 ]= 3  [ 580 ]= 2  [ 581 ]= 1  [ 582 ]= 2  [ 583 ]= 4  [ 584 ]= 2  [ 585 ]= 1  [ 586 ]= 2  [ 587 ]= 1  [ 588 ]= 2  [ 589 ]= 2  [ 590 ]= 0  [ 591 ]= 2  [ 592 ]= 2  [ 593 ]= 1  [ 594 ]= 0  [ 595 ]= 4  [ 596 ]= 3  [ 597 ]= 4  [ 598 ]= 2  [ 599 ]= 2  [ 600 ]= 1  [ 601 ]= 4  [ 602 ]= 2  [ 603 ]= 1  [ 604 ]= 2  [ 605 ]= 2  [ 606 ]= 1  [ 607 ]= 0  [ 608 ]= 2  [ 609 ]= 4  [ 610 ]= 2  [ 611 ]= 4  [ 612 ]= 1  [ 613 ]= 2  [ 614 ]= 2  [ 615 ]= 2  [ 616 ]= 2  [ 617 ]= 2  [ 618 ]= 2  [ 619 ]= 2  [ 620 ]= 2  [ 621 ]= 2  [ 622 ]= 4  [ 623 ]= 2  [ 624 ]= 2  [ 625 ]= 2  [ 626 ]= 2  [ 627 ]= 2  [ 628 ]= 2  [ 629 ]= 1  [ 630 ]= 0  [ 631 ]= 2  [ 632 ]= 2  [ 633 ]= 1  [ 634 ]= 2  [ 635 ]= 4  [ 636 ]= 2  [ 637 ]= 2  [ 638 ]= 2  [ 639 ]= 2  [ 640 ]= 0  [ 641 ]= 2  [ 642 ]= 1  [ 643 ]= 2  [ 644 ]= 1  [ 645 ]= 3  [ 646 ]= 3  [ 647 ]= 2  [ 648 ]= 2  [ 649 ]= 1  [ 650 ]= 2  [ 651 ]= 2  [ 652 ]= 1  [ 653 ]= 2  [ 654 ]= 4  [ 655 ]= 1  [ 656 ]= 4  [ 657 ]= 1  [ 658 ]= 4  [ 659 ]= 2  [ 660 ]= 2  [ 661 ]= 2  [ 662 ]= 0  [ 663 ]= 1  [ 664 ]= 1  [ 665 ]= 2  [ 666 ]= 2  [ 667 ]= 2  [ 668 ]= 2  [ 669 ]= 0  [ 670 ]= 2  [ 671 ]= 0  [ 672 ]= 0  [ 673 ]= 0  [ 674 ]= 1  [ 675 ]= 2  [ 676 ]= 2  [ 677 ]= 0  [ 678 ]= 2  [ 679 ]= 3  [ 680 ]= 1  [ 681 ]= 2  [ 682 ]= 2  [ 683 ]= 2  [ 684 ]= 1  [ 685 ]= 4  [ 686 ]= 1  [ 687 ]= 2  [ 688 ]= 2  [ 689 ]= 2  [ 690 ]= 1  [ 691 ]= 2  [ 692 ]= 4  [ 693 ]= 2  [ 694 ]= 1  [ 695 ]= 4  [ 696 ]= 2  [ 697 ]= 2  [ 698 ]= 2  [ 699 ]= 2  [ 700 ]= 1  [ 701 ]= 2  [ 702 ]= 2  [ 703 ]= 1  [ 704 ]= 2  [ 705 ]= 2  [ 706 ]= 1  [ 707 ]= 2  [ 708 ]= 2  [ 709 ]= 2  [ 710 ]= 2  [ 711 ]= 2  [ 712 ]= 0  [ 713 ]= 2  [ 714 ]= 4  [ 715 ]= 2  [ 716 ]= 1  [ 717 ]= 2  [ 718 ]= 2  [ 719 ]= 1  [ 720 ]= 2  [ 721 ]= 0  [ 722 ]= 2  [ 723 ]= 2  [ 724 ]= 2  [ 725 ]= 2  [ 726 ]= 2  [ 727 ]= 2  [ 728 ]= 2  [ 729 ]= 0  [ 730 ]= 0  [ 731 ]= 2  [ 732 ]= 4  [ 733 ]= 2  [ 734 ]= 4  [ 735 ]= 1  [ 736 ]= 2  [ 737 ]= 2  [ 738 ]= 2  [ 739 ]= 2  [ 740 ]= 4  [ 741 ]= 2  [ 742 ]= 2  [ 743 ]= 4  [ 744 ]= 4  [ 745 ]= 2  [ 746 ]= 2  [ 747 ]= 2  [ 748 ]= 4  [ 749 ]= 1  [ 750 ]= 2  [ 751 ]= 4  [ 752 ]= 0  [ 753 ]= 4  [ 754 ]= 1  [ 755 ]= 2  [ 756 ]= 4  [ 757 ]= 1  [ 758 ]= 0  [ 759 ]= 4  [ 760 ]= 4  [ 761 ]= 1  [ 762 ]= 2  [ 763 ]= 2  [ 764 ]= 2  [ 765 ]= 2  [ 766 ]= 0  [ 767 ]= 2  [ 768 ]= 0  [ 769 ]= 1  [ 770 ]= 1  [ 771 ]= 2  [ 772 ]= 2  [ 773 ]= 3  [ 774 ]= 4  [ 775 ]= 2  [ 776 ]= 1  [ 777 ]= 2  [ 778 ]= 0  [ 779 ]= 4  [ 780 ]= 1  [ 781 ]= 2  [ 782 ]= 2  [ 783 ]= 4  [ 784 ]= 4  [ 785 ]= 2  [ 786 ]= 2  [ 787 ]= 2  [ 788 ]= 2  [ 789 ]= 0  [ 790 ]= 3  [ 791 ]= 4  [ 792 ]= 3  [ 793 ]= 2  [ 794 ]= 2  [ 795 ]= 2  [ 796 ]= 2  [ 797 ]= 4  [ 798 ]= 2  [ 799 ]= 2  [ 800 ]= 2  [ 801 ]= 4  [ 802 ]= 0  [ 803 ]= 4  [ 804 ]= 2  [ 805 ]= 1  [ 806 ]= 4  [ 807 ]= 2  [ 808 ]= 2  [ 809 ]= 4  [ 810 ]= 2  [ 811 ]= 4  [ 812 ]= 1  [ 813 ]= 2  [ 814 ]= 2  [ 815 ]= 4  [ 816 ]= 2  [ 817 ]= 1  [ 818 ]= 4  [ 819 ]= 2  [ 820 ]= 2  [ 821 ]= 4  [ 822 ]= 4  [ 823 ]= 2  [ 824 ]= 2  [ 825 ]= 4  [ 826 ]= 2  [ 827 ]= 2  [ 828 ]= 2  [ 829 ]= 2  [ 830 ]= 2  [ 831 ]= 0  [ 832 ]= 1  [ 833 ]= 4  [ 834 ]= 0  [ 835 ]= 0  [ 836 ]= 4  [ 837 ]= 2  [ 838 ]= 2  [ 839 ]= 4  [ 840 ]= 2  [ 841 ]= 2  [ 842 ]= 4  [ 843 ]= 2  [ 844 ]= 2  [ 845 ]= 2  [ 846 ]= 1  [ 847 ]= 2  [ 848 ]= 3  [ 849 ]= 0  [ 850 ]= 4  [ 851 ]= 2  [ 852 ]= 2  [ 853 ]= 4  [ 854 ]= 4  [ 855 ]= 4  [ 856 ]= 4  [ 857 ]= 0  [ 858 ]= 1  [ 859 ]= 1  [ 860 ]= 4  [ 861 ]= 2  [ 862 ]= 2  [ 863 ]= 2  [ 864 ]= 2  [ 865 ]= 2  [ 866 ]= 2  [ 867 ]= 0  [ 868 ]= 0  [ 869 ]= 2  [ 870 ]= 4  [ 871 ]= 4  [ 872 ]= 1  [ 873 ]= 0  [ 874 ]= 2  [ 875 ]= 2  [ 876 ]= 1  [ 877 ]= 2  [ 878 ]= 2  [ 879 ]= 2  [ 880 ]= 2  [ 881 ]= 2  [ 882 ]= 1  [ 883 ]= 2  [ 884 ]= 2  [ 885 ]= 2  [ 886 ]= 4  [ 887 ]= 4  [ 888 ]= 4  [ 889 ]= 2  [ 890 ]= 2  [ 891 ]= 2  [ 892 ]= 2  [ 893 ]= 2  [ 894 ]= 2  [ 895 ]= 2  [ 896 ]= 4  [ 897 ]= 2  [ 898 ]= 1  [ 899 ]= 0  [ 900 ]= 1  [ 901 ]= 0  [ 902 ]= 1  [ 903 ]= 1  [ 904 ]= 1  [ 905 ]= 2  [ 906 ]= 1  [ 907 ]= 2  [ 908 ]= 1  [ 909 ]= 2  [ 910 ]= 2  [ 911 ]= 2  [ 912 ]= 2  [ 913 ]= 2  [ 914 ]= 2  [ 915 ]= 2  [ 916 ]= 2  [ 917 ]= 2  [ 918 ]= 2  [ 919 ]= 2  [ 920 ]= 2  [ 921 ]= 4  [ 922 ]= 0  [ 923 ]= 0  [ 924 ]= 2  [ 925 ]= 0  [ 926 ]= 4  [ 927 ]= 4  [ 928 ]= 2  [ 929 ]= 2  [ 930 ]= 2  [ 931 ]= 2  [ 932 ]= 2  [ 933 ]= 2  [ 934 ]= 2  [ 935 ]= 4  [ 936 ]= 2  [ 937 ]= 0  [ 938 ]= 0  [ 939 ]= 4  [ 940 ]= 2  [ 941 ]= 2  [ 942 ]= 2  [ 943 ]= 2  [ 944 ]= 2  [ 945 ]= 2  [ 946 ]= 2  [ 947 ]= 0  [ 948 ]= 4  [ 949 ]= 2  [ 950 ]= 2  [ 951 ]= 2  [ 952 ]= 0  [ 953 ]= 0  [ 954 ]= 1  [ 955 ]= 2  [ 956 ]= 2  [ 957 ]= 1  [ 958 ]= 2  [ 959 ]= 0  [ 960 ]= 2  [ 961 ]= 4  [ 962 ]= 2  [ 963 ]= 0  [ 964 ]= 2  [ 965 ]= 2  [ 966 ]= 2  [ 967 ]= 2  [ 968 ]= 2  [ 969 ]= 2  [ 970 ]= 1  [ 971 ]= 2  [ 972 ]= 2  [ 973 ]= 0  [ 974 ]= 1  [ 975 ]= 2  [ 976 ]= 2  [ 977 ]= 4  [ 978 ]= 4  [ 979 ]= 2  [ 980 ]= 1  [ 981 ]= 1  [ 982 ]= 2  [ 983 ]= 2  [ 984 ]= 4  [ 985 ]= 2  [ 986 ]= 4  [ 987 ]= 2  [ 988 ]= 4  [ 989 ]= 2  [ 990 ]= 1  [ 991 ]= 0  [ 992 ]= 2  [ 993 ]= 3  [ 994 ]= 4  [ 995 ]= 3  [ 996 ]= 2  [ 997 ]= 3  [ 998 ]= 2  [ 999 ]= 2  [ 1000 ]= 2  [ 1001 ]= 2  [ 1002 ]= 4  [ 1003 ]= 2  [ 1004 ]= 2  [ 1005 ]= 0  [ 1006 ]= 1  [ 1007 ]= 2  [ 1008 ]= 2  [ 1009 ]= 2  [ 1010 ]= 0  [ 1011 ]= 3  [ 1012 ]= 2  [ 1013 ]= 2  [ 1014 ]= 2  [ 1015 ]= 2  [ 1016 ]= 2  [ 1017 ]= 2  [ 1018 ]= 4  [ 1019 ]= 1  [ 1020 ]= 2  [ 1021 ]= 4  [ 1022 ]= 2  [ 1023 ]= 4  [ 1024 ]= 4  [ 1025 ]= 2  [ 1026 ]= 0  [ 1027 ]= 4  [ 1028 ]= 2  [ 1029 ]= 1  [ 1030 ]= 2  [ 1031 ]= 0  [ 1032 ]= 2  [ 1033 ]= 1  [ 1034 ]= 2  [ 1035 ]= 4  [ 1036 ]= 2  [ 1037 ]= 0  [ 1038 ]= 2  [ 1039 ]= 0  [ 1040 ]= 4  [ 1041 ]= 2  [ 1042 ]= 0  [ 1043 ]= 4  [ 1044 ]= 2  [ 1045 ]= 0  [ 1046 ]= 2  [ 1047 ]= 3  [ 1048 ]= 2  [ 1049 ]= 2  [ 1050 ]= 4  [ 1051 ]= 2  [ 1052 ]= 2  [ 1053 ]= 2  [ 1054 ]= 2  [ 1055 ]= 0  [ 1056 ]= 2  [ 1057 ]= 4  [ 1058 ]= 2  [ 1059 ]= 4  [ 1060 ]= 2  [ 1061 ]= 0  [ 1062 ]= 0  [ 1063 ]= 0  [ 1064 ]= 4  [ 1065 ]= 2  [ 1066 ]= 2  [ 1067 ]= 2  [ 1068 ]= 2  [ 1069 ]= 1  [ 1070 ]= 2  [ 1071 ]= 0  [ 1072 ]= 4  [ 1073 ]= 2  [ 1074 ]= 2  [ 1075 ]= 0  [ 1076 ]= 1  [ 1077 ]= 0  [ 1078 ]= 2  [ 1079 ]= 0  [ 1080 ]= 2  [ 1081 ]= 2  [ 1082 ]= 2  [ 1083 ]= 2  [ 1084 ]= 2  [ 1085 ]= 2  [ 1086 ]= 0  [ 1087 ]= 2  [ 1088 ]= 2  [ 1089 ]= 2  [ 1090 ]= 2  [ 1091 ]= 2  [ 1092 ]= 2  [ 1093 ]= 2  [ 1094 ]= 4  [ 1095 ]= 0  [ 1096 ]= 4  [ 1097 ]= 0  [ 1098 ]= 2  [ 1099 ]= 2  [ 1100 ]= 2  [ 1101 ]= 1  [ 1102 ]= 3  [ 1103 ]= 2  [ 1104 ]= 0  [ 1105 ]= 2  [ 1106 ]= 1  [ 1107 ]= 2  [ 1108 ]= 2  [ 1109 ]= 2  [ 1110 ]= 2  [ 1111 ]= 0  [ 1112 ]= 4  [ 1113 ]= 2  [ 1114 ]= 2  [ 1115 ]= 2  [ 1116 ]= 2  [ 1117 ]= 2  [ 1118 ]= 3  [ 1119 ]= 2  [ 1120 ]= 2  [ 1121 ]= 0  [ 1122 ]= 1  [ 1123 ]= 2  [ 1124 ]= 1  [ 1125 ]= 0  [ 1126 ]= 2  [ 1127 ]= 3  [ 1128 ]= 0  [ 1129 ]= 2  [ 1130 ]= 2  [ 1131 ]= 2  [ 1132 ]= 0  [ 1133 ]= 0  [ 1134 ]= 2  [ 1135 ]= 2  [ 1136 ]= 2  [ 1137 ]= 2  [ 1138 ]= 2  [ 1139 ]= 2  [ 1140 ]= 4  [ 1141 ]= 2  [ 1142 ]= 3  [ 1143 ]= 2  [ 1144 ]= 4  [ 1145 ]= 4  [ 1146 ]= 2  [ 1147 ]= 2  [ 1148 ]= 2  [ 1149 ]= 0  [ 1150 ]= 1  [ 1151 ]= 4  [ 1152 ]= 0  [ 1153 ]= 2  [ 1154 ]= 2  [ 1155 ]= 2  [ 1156 ]= 2  [ 1157 ]= 2  [ 1158 ]= 2  [ 1159 ]= 4  [ 1160 ]= 3  [ 1161 ]= 4  [ 1162 ]= 1  [ 1163 ]= 2  [ 1164 ]= 3  [ 1165 ]= 2  [ 1166 ]= 0  [ 1167 ]= 2  [ 1168 ]= 0  [ 1169 ]= 2  [ 1170 ]= 2  [ 1171 ]= 2  [ 1172 ]= 2  [ 1173 ]= 0  [ 1174 ]= 2  [ 1175 ]= 2  [ 1176 ]= 0  [ 1177 ]= 2  [ 1178 ]= 3  [ 1179 ]= 1  [ 1180 ]= 2  [ 1181 ]= 2  [ 1182 ]= 2  [ 1183 ]= 2  [ 1184 ]= 2  [ 1185 ]= 2  [ 1186 ]= 1  [ 1187 ]= 3  [ 1188 ]= 2  [ 1189 ]= 4  [ 1190 ]= 2  [ 1191 ]= 2  [ 1192 ]= 2  [ 1193 ]= 2  [ 1194 ]= 4  [ 1195 ]= 2  [ 1196 ]= 4  [ 1197 ]= 2  [ 1198 ]= 2  [ 1199 ]= 2  [ 1200 ]= 2  [ 1201 ]= 0  [ 1202 ]= 3  [ 1203 ]= 0  [ 1204 ]= 1  [ 1205 ]= 1  [ 1206 ]= 2  [ 1207 ]= 4  [ 1208 ]= 2  [ 1209 ]= 2  [ 1210 ]= 2  [ 1211 ]= 2  [ 1212 ]= 2  [ 1213 ]= 2  [ 1214 ]= 2  [ 1215 ]= 2  [ 1216 ]= 4  [ 1217 ]= 0  [ 1218 ]= 2  [ 1219 ]= 2  [ 1220 ]= 2  [ 1221 ]= 4  [ 1222 ]= 2  [ 1223 ]= 3  [ 1224 ]= 0  [ 1225 ]= 2  [ 1226 ]= 2  [ 1227 ]= 4  [ 1228 ]= 4  [ 1229 ]= 4  [ 1230 ]= 4  [ 1231 ]= 2  [ 1232 ]= 2  [ 1233 ]= 1  [ 1234 ]= 4  [ 1235 ]= 4  [ 1236 ]= 2  [ 1237 ]= 1  [ 1238 ]= 2  [ 1239 ]= 4  [ 1240 ]= 2  [ 1241 ]= 2  [ 1242 ]= 0  [ 1243 ]= 3  [ 1244 ]= 2  [ 1245 ]= 2  [ 1246 ]= 2  [ 1247 ]= 2  [ 1248 ]= 3  [ 1249 ]= 2  [ 1250 ]= 2  [ 1251 ]= 2  [ 1252 ]= 4  [ 1253 ]= 2  [ 1254 ]= 2  [ 1255 ]= 1  [ 1256 ]= 0  [ 1257 ]= 2  [ 1258 ]= 2  [ 1259 ]= 2  [ 1260 ]= 1  [ 1261 ]= 2  [ 1262 ]= 4  [ 1263 ]= 2  [ 1264 ]= 2  [ 1265 ]= 0  [ 1266 ]= 3  [ 1267 ]= 0  [ 1268 ]= 4  [ 1269 ]= 4  [ 1270 ]= 2  [ 1271 ]= 0  [ 1272 ]= 1  [ 1273 ]= 4  [ 1274 ]= 2  [ 1275 ]= 2  [ 1276 ]= 4  [ 1277 ]= 4  [ 1278 ]= 4  [ 1279 ]= 4  [ 1280 ]= 2  [ 1281 ]= 0  [ 1282 ]= 2  [ 1283 ]= 4  [ 1284 ]= 4  [ 1285 ]= 4  [ 1286 ]= 2  [ 1287 ]= 2  [ 1288 ]= 4  [ 1289 ]= 2  [ 1290 ]= 2  [ 1291 ]= 2  [ 1292 ]= 1  [ 1293 ]= 0  [ 1294 ]= 2  [ 1295 ]= 2  [ 1296 ]= 2  [ 1297 ]= 1  [ 1298 ]= 2  [ 1299 ]= 2  [ 1300 ]= 1  [ 1301 ]= 4  [ 1302 ]= 2  [ 1303 ]= 0  [ 1304 ]= 1  [ 1305 ]= 3  [ 1306 ]= 2  [ 1307 ]= 2  [ 1308 ]= 2  [ 1309 ]= 4  [ 1310 ]= 2  [ 1311 ]= 2  [ 1312 ]= 2  [ 1313 ]= 2  [ 1314 ]= 4  [ 1315 ]= 2  [ 1316 ]= 2  [ 1317 ]= 1  [ 1318 ]= 2  [ 1319 ]= 2  [ 1320 ]= 1  [ 1321 ]= 2  [ 1322 ]= 1  [ 1323 ]= 0  [ 1324 ]= 2  [ 1325 ]= 2  [ 1326 ]= 2  [ 1327 ]= 2  [ 1328 ]= 3  [ 1329 ]= 3  [ 1330 ]= 4  [ 1331 ]= 1  [ 1332 ]= 4  [ 1333 ]= 4  [ 1334 ]= 4  [ 1335 ]= 0  [ 1336 ]= 1  [ 1337 ]= 2  [ 1338 ]= 2  [ 1339 ]= 2  [ 1340 ]= 2  [ 1341 ]= 2  [ 1342 ]= 4  [ 1343 ]= 1  [ 1344 ]= 4  [ 1345 ]= 2  [ 1346 ]= 2  [ 1347 ]= 0  [ 1348 ]= 3  [ 1349 ]= 2  [ 1350 ]= 2  [ 1351 ]= 2  [ 1352 ]= 4  [ 1353 ]= 2  [ 1354 ]= 1  [ 1355 ]= 2  [ 1356 ]= 2  [ 1357 ]= 2  [ 1358 ]= 2  [ 1359 ]= 0  [ 1360 ]= 0  [ 1361 ]= 1  [ 1362 ]= 2  [ 1363 ]= 1  [ 1364 ]= 2  [ 1365 ]= 2  [ 1366 ]= 0  [ 1367 ]= 4  [ 1368 ]= 4  [ 1369 ]= 2  [ 1370 ]= 2  [ 1371 ]= 2  [ 1372 ]= 2  [ 1373 ]= 2  [ 1374 ]= 2  [ 1375 ]= 1  [ 1376 ]= 4  [ 1377 ]= 4  [ 1378 ]= 2  [ 1379 ]= 2  [ 1380 ]= 2  [ 1381 ]= 2  [ 1382 ]= 4  [ 1383 ]= 1  [ 1384 ]= 2  [ 1385 ]= 2  [ 1386 ]= 2  [ 1387 ]= 2  [ 1388 ]= 3  [ 1389 ]= 2  [ 1390 ]= 4  [ 1391 ]= 2  [ 1392 ]= 2  [ 1393 ]= 3  [ 1394 ]= 0  [ 1395 ]= 2  [ 1396 ]= 2  [ 1397 ]= 0  [ 1398 ]= 4  [ 1399 ]= 2  [ 1400 ]= 2  [ 1401 ]= 2  [ 1402 ]= 4  [ 1403 ]= 2  [ 1404 ]= 2  [ 1405 ]= 2  [ 1406 ]= 2  [ 1407 ]= 2  [ 1408 ]= 1  [ 1409 ]= 2  [ 1410 ]= 2  [ 1411 ]= 0  [ 1412 ]= 4  [ 1413 ]= 4  [ 1414 ]= 4  [ 1415 ]= 2  [ 1416 ]= 4  [ 1417 ]= 2  [ 1418 ]= 2  [ 1419 ]= 1  [ 1420 ]= 4  [ 1421 ]= 2  [ 1422 ]= 0  [ 1423 ]= 0  [ 1424 ]= 2  [ 1425 ]= 2  [ 1426 ]= 2  [ 1427 ]= 2  [ 1428 ]= 0  [ 1429 ]= 0  [ 1430 ]= 2  [ 1431 ]= 4  [ 1432 ]= 2  [ 1433 ]= 2  [ 1434 ]= 4  [ 1435 ]= 2  [ 1436 ]= 0  [ 1437 ]= 2  [ 1438 ]= 2  [ 1439 ]= 1  [ 1440 ]= 1  [ 1441 ]= 2  [ 1442 ]= 0  [ 1443 ]= 0  [ 1444 ]= 2  [ 1445 ]= 2  [ 1446 ]= 1  [ 1447 ]= 4  [ 1448 ]= 2  [ 1449 ]= 2  [ 1450 ]= 2  [ 1451 ]= 1  [ 1452 ]= 2  [ 1453 ]= 2  [ 1454 ]= 0  [ 1455 ]= 4  [ 1456 ]= 2  [ 1457 ]= 4  [ 1458 ]= 2  [ 1459 ]= 3  [ 1460 ]= 2  [ 1461 ]= 2  [ 1462 ]= 2  [ 1463 ]= 0  [ 1464 ]= 2  [ 1465 ]= 3  [ 1466 ]= 4  [ 1467 ]= 4  [ 1468 ]= 4  [ 1469 ]= 2  [ 1470 ]= 0  [ 1471 ]= 2  [ 1472 ]= 4  [ 1473 ]= 2  [ 1474 ]= 4  [ 1475 ]= 2  [ 1476 ]= 2  [ 1477 ]= 2  [ 1478 ]= 1  [ 1479 ]= 1  [ 1480 ]= 0  [ 1481 ]= 0  [ 1482 ]= 3  [ 1483 ]= 2  [ 1484 ]= 2  [ 1485 ]= 2  [ 1486 ]= 2  [ 1487 ]= 1  [ 1488 ]= 0  [ 1489 ]= 2  [ 1490 ]= 2  [ 1491 ]= 2  [ 1492 ]= 2  [ 1493 ]= 3  [ 1494 ]= 2  [ 1495 ]= 3  [ 1496 ]= 0  [ 1497 ]= 1  [ 1498 ]= 2  [ 1499 ]= 2  [ 1500 ]= 2  [ 1501 ]= 2  [ 1502 ]= 4  [ 1503 ]= 2  [ 1504 ]= 2  [ 1505 ]= 1  [ 1506 ]= 4  [ 1507 ]= 1  [ 1508 ]= 2  [ 1509 ]= 2  [ 1510 ]= 2  [ 1511 ]= 2  [ 1512 ]= 0  [ 1513 ]= 2  [ 1514 ]= 2  [ 1515 ]= 4  [ 1516 ]= 2  [ 1517 ]= 2  [ 1518 ]= 4  [ 1519 ]= 0  [ 1520 ]= 2  [ 1521 ]= 2  [ 1522 ]= 2  [ 1523 ]= 4  [ 1524 ]= 2  [ 1525 ]= 2  [ 1526 ]= 4  [ 1527 ]= 0  [ 1528 ]= 4  [ 1529 ]= 2  [ 1530 ]= 2  [ 1531 ]= 4  [ 1532 ]= 2  [ 1533 ]= 1  [ 1534 ]= 4  [ 1535 ]= 2  [ 1536 ]= 2  [ 1537 ]= 2  [ 1538 ]= 2  [ 1539 ]= 2  [ 1540 ]= 4  [ 1541 ]= 2  [ 1542 ]= 1  [ 1543 ]= 4  [ 1544 ]= 2  [ 1545 ]= 0  [ 1546 ]= 4  [ 1547 ]= 2  [ 1548 ]= 1  [ 1549 ]= 2  [ 1550 ]= 0  [ 1551 ]= 2  [ 1552 ]= 2  [ 1553 ]= 0  [ 1554 ]= 0  [ 1555 ]= 4  [ 1556 ]= 4  [ 1557 ]= 2  [ 1558 ]= 0  [ 1559 ]= 4  [ 1560 ]= 2  [ 1561 ]= 2  [ 1562 ]= 2  [ 1563 ]= 2  [ 1564 ]= 2  [ 1565 ]= 1  [ 1566 ]= 2  [ 1567 ]= 1  [ 1568 ]= 2  [ 1569 ]= 1  [ 1570 ]= 4  [ 1571 ]= 2  [ 1572 ]= 2  [ 1573 ]= 4  [ 1574 ]= 4  [ 1575 ]= 4  [ 1576 ]= 4  [ 1577 ]= 0  [ 1578 ]= 2  "
          ]
        }
      ],
      "source": [
        "x_test, y_test = test_preprocess_data(\"testdata20230405\")\n",
        "x_test = x_test.swapaxes(1, 2)\n",
        "\n",
        "idx = np.random.permutation(len(x_test))\n",
        "x_test,y_test = x_test[idx], y_test[idx]\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "  print(\"[\",i,\"]=\",y_test[i],end=\"  \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save to File"
      ],
      "metadata": {
        "id": "xhb8IwdnIJpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "Zps8laoiRkdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBfEXY2FF3yu"
      },
      "outputs": [],
      "source": [
        "# save directly to file instead\n",
        "def data_gen_mcu():\n",
        "    sample_data = \"mcu_eeg_data1_4.txt\"\n",
        "    original_stdout = sys.stdout\n",
        "\n",
        "    with open(sample_data, \"w\") as f:\n",
        "      sys.stdout = f\n",
        "      print(y_test[1576])\n",
        "      want_data = x_test[1576:1576+1, :]\n",
        "      for j in range(7680):\n",
        "          print('{:.15f}'.format(want_data[0][0][j][0]))\n",
        "\n",
        "    sys.stdout = original_stdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbKLN7rXF5cE"
      },
      "outputs": [],
      "source": [
        "data_gen_mcu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv1fhXQepYUm"
      },
      "outputs": [],
      "source": [
        "# save directly to file instead\n",
        "import sys\n",
        "\n",
        "def data_gen():\n",
        "  sample_data = \"mcu_data_0.txt\"\n",
        "  original_stdout = sys.stdout\n",
        "\n",
        "  with open(sample_data, \"w\") as f:\n",
        "    sys.stdout = f\n",
        "\n",
        "    \"\"\"Label\n",
        "    print(\"float sleep_label[2] = {\", end=\"\")\n",
        "    for i in range(2):\n",
        "      print(y[i], end=\"\")\n",
        "      if i != 1:\n",
        "        print(\",\", end=\"\")\n",
        "    print(\"};\")\"\"\"\n",
        "\n",
        "    #Data\n",
        "    print(\"float sleep_array[7680] = {\", end=\"\")\n",
        "\n",
        "    #for i in range(2):\n",
        "      #print(\"{\", end=\"\")\n",
        "    want_data = x_val[0:1, :]\n",
        "    for j in range(7680):\n",
        "      if j % 10 == 0:\n",
        "        print()\n",
        "      if j == 7679:\n",
        "        print(want_data[0][0][j][0], end=\"\")\n",
        "      else:\n",
        "        print(want_data[0][0][j][0], end=\",\")\n",
        "    #print(\"}\", end=\"\")\n",
        "      #if i != 1:\n",
        "        #print(\",\", end=\"\")\n",
        "\n",
        "    print(\"};\")\n",
        "\n",
        "  sys.stdout = original_stdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3SXmDJ9MXif"
      },
      "outputs": [],
      "source": [
        "data_gen()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & Val"
      ],
      "metadata": {
        "id": "3PzLUU78H0Tc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE2Gx7X4InE-",
        "outputId": "362e253f-4262-4174-b99a-305bb6a4f9ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_dir: /content/gdrive/MyDrive/University/U of T/4th Year/ECE496 Team/Software/ECE496Software/data/eeg_fz_ler\n",
            "allfiles ['01-03-0001.npz', '01-03-0002.npz', '01-03-0003.npz', '01-03-0004.npz', '01-03-0012.npz', '01-03-0008.npz']\n",
            " \n"
          ]
        }
      ],
      "source": [
        "#trainfile_idx = [0,3,4,5]\n",
        "#testfile_idx = [1,2]\n",
        "trainfile_idx = [0,1,2,3]\n",
        "testfile_idx = [4,5]\n",
        "\n",
        "data_dir = \"/content/gdrive/MyDrive/University/U of T/4th Year/ECE496 Team/Software/ECE496Software/data/eeg_fz_ler\"\n",
        "\n",
        "x_train, y_train, x_val, y_val = preprocess_data(data_dir, trainfile_idx, testfile_idx)\n",
        "test_batches_ori = (x_val, y_val)\n",
        "x_train = x_train.swapaxes(1, 2)\n",
        "x_val = x_val.swapaxes(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnyBozo-MXUE",
        "outputId": "4f15b67b-5855-4315-a49d-075bcab0b6ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1 0 0 3 2 4 3 4 1 2 0 0 2 1 3 0 2 0 0 3 4 0 2 0 4 4 4 0 2 1 0 4 1 1 2 4 1 2 0 1 3 1 3 4 4 0 2 2 3 4 2 1 1 2 2 2 0 1 2 0 2 2 1 2 0 1 2 2 3 1 3 2 1 4 0 3 3 3 3 4 1 0 1 0 2 1 4 0 0 1 1 1 4 4 4 1 4 3 4 0 4 3 4 3 0 4 4 3 2 4 1 4 1 0 1 4 0 1 1 0 0 3 3 1 2 3 2 0 2 0 2 1 1 2 4 2 4 2 2 4 4 2 0 0 1 1 2 4 2 2 0 1 1 0 2 0 0 1 1 0 0 4 3 1 2 2 4 1 4 4 4 0 2 2 4 4 4 2 2 0 4 0 4 3 0 0 4 1 4 1 0 2 3 4 3 0 0 3 2 0 4 3 2 4 1 0 3 2 0 3 0 2 0 3 4 4 0 0 2 4 0 4 3 3 1 3 1 1 0 3 3 3 3 2 2 1 4 0 3 0 0 1 4 1 0 3 4 4 4 2 0 3 2 2 1 3 2 0 3 2 4 1 3 0 1 4 3 3 4 4 4 1 4 1 2 4 1 3 2 4 4 0 4 1 3 3 4 0 1 2 3 3 4 1 3 2 3 3 4 2 0 3 0 3 0 1 0 4 3 3 0 1 2 3 4 4 2 1 0 1 4 3 1 0 4 4 3 2 1 1 1 3 3 3 0 4 3 2 4 2 2 4 0 0 3 0 4 3 0 1 4 1 1 4 3 4 0 2 0 3 0 0 2 0 4 3 3 2 4 1 3 3 2 0 1 1 4 2 4 3 3 3 4 0 0 0 0 1 0 3 2 0 2 0 2 1 0 3 1 0 3 4 4 0 1 4 3 4 2 1 4 2 3 0 3 4 2 2 0 2 0 2 1 4 0 1 3 4 3 4 2 3 4 1 1 4 0 1 0 3 4 4 4 4 0 3 2 0 0 0 4 2 0 2 0 0 4 0 4 3 0 3 1 1 2 4 4 0 0 1 3 3 4 3 3 1 4 2 0 4 1 0 0 4 3 2 4 0 0 4 4 0 4 3 3 4 2 4 0 0 1 1 2 0 1 1 3 4 1 0 1 4 1 1 0 1 3 3 4 3 0 3 0 0 0 0 3 1 3 4 4 3 1 3 4 1 2 1 1 2 3 1 1 4 1 3 2 2 1 2 1 0 3 4 3 2 1 4 1 0 4 3 3 4 2 2 3 0 3 1 2 3 2 0 1 1 2 4 0 0 4 0 3 2 0 4 0 1 2 3 3 4 4 0 1 2 0 1 3 3 0 3 2 3 3 4 1 2 2 1 0 2 3 4 4 3 1 4 3 0 0 2 2 4 3 0 0 4 0 4 1 2 4 4 1 0 1 4 0 2 4 0 1 4 4 2 1 0 4 1 0 3 3 4 3 2 1 3 2 0 0 0 1 2 0 4 1 1 3 1 1 2 4 2 3 1 2 4 3 2 0 0 3 1 2 3 0 1 4 4 4 2 1 4 3 2 0 1 4 3 0 3 2 0 4 1 4 4 2 2 0 2 1 1 3 0 4 3 4 0 4 2 1 3 3 2 0 3 2 0 0 1 1 4 2 3 2 1 1 0 2 2 0 1 2 1 1 1 0 1 3 1 3 0 3 1 3 0 4 0 4 1 4 4 0 3 1 0 4 4 2 4 2 0 4 3 1 0 2 4 2 2 3 4 3 4 0 0 2 0 3 3 2 1 0 4 2 3 2 1 0 4 1 1 4 3 4 4 1 1 4 0 0 1 4 0 2 1 1 3 3 2 4 2 1 1 1 2 2 4 0 1 2 1 2 4 2 1 2 2 2 1 2 1 4 0 3 4 3 0 3 1 1 3 1 2 0 2 3 0 4 4 3 1 3 4 1 3 3 3 2 4 0 2 4 2 1 2 1 2 3 2 0 2 4 0 1 0 1 0 4 4 0 2 1 3 0 3 1 4 2 2 3 3 2 0 2 0 3 2 0 2 3 2 2 3 4 0 3 0 0 4 0 1 4 0 0 1 1 4 4 3 1 2 3 1 0 0 3 1 0 2 1 4 3 4 1 4 4 0 3 4 1 0 3 1 2 3 2 3 4 3 4 3 2 4 2 4 2 2 1 4 0 0 1 4 2 3 4 0 3 0 0 4 1 1 3 1 1 1 1 0 2 4 4 3 3 0 4 3 3 3 0 1 1 1 4 2 4 2 3 2 3 3 2 0 0 3 2 4 2 4 0 2 0 1 1 3 1 2 4 2 1 3 4 1 3 0 2 0 2 0 3 4 0 0 1 2 3 3 1 1 2 2 2 3 1 2 2 4 0 1 1 1 2 1 4 1 0 2 2 0 3 2 2 0 3 0 2 3 1 3 2 2 0 2 0 2 2 2 2 0 3 0 4 4 1 2 1 2 2 0 0 4 4 3 1 2 3 4 4 4 4 3 0 2 2 0 3 3 3 0 2 4 2 3 3 2 4 3 1 3 0 3 3 3 2 0 3 4 4 2 0 0 1 4 4 4 2 2 4 1 4 4 4 3 2 1 0 0 4 2 3 2 1 1 3 3 2 2 2 3 3 4 1 3 4 0 4 3 1 2 2 1 4 2 0 4 4 1 1 0 2 3 4 4 4 0 3 2 2 1 2 3 3 0 0 1 4 0 0 1 2 4 3 2 2 1 2 0 0 2 4 4 2 4 2 3 1 3 3 4 0 0 3 4 2 3 4 1 1 2 4 1 0 3 0 2 4 2 3 1 3 0 4 2 1 4 4 3 4 3 1 0 4 4 0 1 0 2 4 3 3 0 3 0 3 1 2 2 3 4 2 4 2 0 4 1 0 3 1 1 2 2 4 2 3 4 3 4 4 3 1 3 2 3 1 3 0 2 0 0 1 4 4 3 4 2 3 1 3 0 2 4 4 3 1 3 3 3 1 4 2 2 4 2 1 2 2 1 1 4 0 2 2 1 0 0 1 2 0 2 0 4 3 1 2 3 4 4 3 1 0 2 3 4 4 1 4 0 0 1 3 3 1 0 2 2 0 2 2 3 3 1 0 2 1 0 3 1 0 0 3 2 4 1 2 0 2 0 3 0 1 2 2 0 1 2 0 3 2 2 0 2 0 3 4 4 0 4 2 0 2 3 2 2 3 3 4 3 0 2 3 0 1 1 4 0 4 4 3 0 1 2 3 2 0 3 1 1 0 3 4 4 1 1 0 3 0 2 1 2 3 2 1 1 2 1 2 4 3 2 0 3 4 2 4 0 1 2 3 1 3 3 2 3 4 0 4 3 1 2 3 1 4 3 4 2 3 1 0 3 4 2 4 4 4 1 0 4 3 3 3 2 1 0 3 3 3 1 1 2 2 4 4 1 3 2 0 2 1 1 2 2 0 0 2 0 3 3 3 4 0 4 2 3 1 3 3 4 2 0 1 4 1 0 0 0 2 2 0 3 3 1 1 0 3 4 1 1 2 3 4 0 0 1 1 3 4 4 1 3 1 0 3 3 0 2 2 2 2 4 0 1 1 3 2 2 0 2 3 0 4 0 3 1 0 0 1 4 1 4 1 1 3 3 3 3 4 2 1 4 0 2 4 3 0 0 3 2 4 4 2 1 3 1 2 0 4 0 0 2 1 0 0 1 2 3 0 4 3 0 4 4 1 3 2 1 2 1 3 0 1 1 3 4 4 4 4 3 0 3 1 3 3 0 0 1 0 1 0 4 4 1 3 3 2 2 0 2 1 2 2 4 1 1 3 4 4 1 0 3 0 4 0 2 0 1 4 4 1 4 4 2 1 2 0 3 4 0 4 1 1 0 3 0 0 3 1 1 1 1 2 4 3 2 3 3 0 4 2 1 1 0 4 1 3 4 4 2 4 3 2 3 3 4 3 0 1 1 1 4 3 4 2 1 1 1 0 1 4 2 3 2 3 4 1 2 3 1 1 3 0 1 2 0 1 4 3 4 2 0 3 0 1 1 2 2 1 3 0 2 3 1 3 1 2 4 1 0 2 2 2 4 2 2 4 2 3 0 1 3 2 2 3 3 1 2 4 2 2 4 3 0 2 4 4 2 1 4 1 4 1 3 1 0 4 0 0 2 0 1 4 0 2 1 1 0 2 1 2 4 0 0 4 0 3 0 1 1 2 2 0 4 0 0 0 0 2 2 3 1 2 3 0 1 4 3 1 3 4 4 0 4 3 2 1 3 2 1 0 2 0 4 1 2 2 2 2 1 1 0 2 1 0 2 4 0 0 3 3 0 4 2 4 0 4 2 0 3 0 2 1 3 2 4 0 0 3 1 3 4 3 4 2 0 2 1 1 1 1 1 3 4 4 1 0 1 2 0 3 2 2 0 0 1 1 4 1 2 0 3 4 0 1 1 0 0 1 3 1 3 3 2 1 2 2 1 3 2 2 3 1 0 0 0 4 3 3 1 0 0 4 2 3 4 2 2 3 2 3 1 1 0 0 3 1 4 4 3 2 0 3 3 1 2 3 1 1 0 4 2 4 2 2 1 4 4 4 1 4 4 3 1 4 0 4 3 4 1 2 0 0 4 0 3 2 0 0 0 2 3 1 4 1 4 3 4 2 0 1 0 4 1 3 2 2 3 4 1 1 3 0 4 3 2 3 1 3 3 2 2 1 1 4 2 1 2 0 3 0 3 3 1 4 2 0 4 4 3 2 3 1 4 0 1 3 3 4 3 0 2 0 2 3 4 4 3 1 4 1 0 0 2 3 3 2 3 2 4 0 2 3 4 4 1 1 2 1 4 2 1 0 2 3 2 3 2 1 1 4 4 3 4 3 2 1 1 0 3 4 4 2 2 2 1 4 2 1 3 3 2 4 4 4 3 2 3 4 4 4 2 3 4 0 2 3 4 1 1 0 2 3 2 0 4 1 4 0 1 3 0 0 0 3 3 3 3 1 3 1 1 3 2 2 2 2 4 0 1 1 3 1 4 1 3 4 0 3 0 4 4 2 0 1 3 4 3 4 1 0 3 1 1 2 4 3 3 1 3 1 0 3 1 2 4 3 0 4 2 0 4 3 1 2 2 4 3 4 1 1 2 0 1 1 1 4 2 1 4 0 1 2 3 4 1 2 2 4 1 4 4 0 4 2 2 1 3 3 4 0 3 2 0 4 3 0 2 3 3 3 4 3 2 3 4 0 1 3 2 4 0 1 2 1 4 1 1 2 0 0 2 0 1 1 4 2 1 2 0 0 3 0 0 1 4 0 3 4 4 1 2 1 2 3 0 1 1 4 3 4 3 3 4 0 4 3 2 1 3 4 2 2 2 1 2 1 1 3 0 1 1 4 2 1 4 2 0 1 4 1 0 3 2 0 1 1 1 0 3 0 1 4 2 2 3 2 2 1 0 4 4 0 4 2 3 2 2 4 2 3 4 1 1 4 3 2 1 4 0 3 2 3 4 4 1 3 4 1 2 1 4 2 3 3 1 1 0 2 3 2 4 0 2 0 4 2 4 1 3 1 3 1 3 4 3 2 2 2 2 3 3 0 2 2 2 3 3 4 2 4 1 2 1 0 4 1 3 2 2 0 4 3 0 1 4 0 0 0 3 4 0 0 3 4 3 0 3 4 1 3 4 3 4 3 3 0 1 3 1 0 0 4 4 2 2 1 1 0 0 4 0 3 4 2 4 0 4 3 0 0 2 2 2 2 3 1 1 2 0 3 2 4 2 2 2 2 4 3 2 0 3 1 2 1 0 3 1 4 2 4 4 1 3 4 0 3 0 0 2 1 1 1 1 4 1 4 1 0 2 3 2 2 0 2 1 0 3 3 1 2 3 4 0 3 3 0 0 2 0 2 3 3 3 1 4 1 2 0 1 0 4 4 4 1 0 0 1 2 2 0 0 2 3 3 3 3 2 1 1 0 2 4 0 4 0 0 2 4 0 4 3 4 4 2 0 3 2 4 2 2 3 0 0 2 2 0 0 4 4 1 3 4 4 3 0 1 1 0 3 1 1 0 4 3 1 4 1 2 0 1 1 0 2 1 1 2 0 3 0 4 1 1 4 1 0 1 4 1 1 3 2 4 4 2 2 4 2 2 2 0 4 0 2 1 0 3 4 2 4 3 0 0 1 3 1 4 2 4 0 2 4 2 1 4 4 4 3 2 0 2 2 2 1 3 4 4 2 0 4 3 4 4 1 3 1 0 1 4 4 4 1 0 2 3 2 4 3 3 3 2 4 2 0 3 0 3 2 2 2 2 4 1 4 4 0 0 3 2 4 3 4 4 1 1 2 2 1 3 1 0 1 3 2 2 3 0 0 2 1 3 4 0 1 2 3 4 4 3 0 3 3 3 1 0 2 0 2 2 3 0 2 4 0 1 2 0 3 4 1 2 3 3 2 1 2 3 4 1 0 2 0 0 2 2 3 2 3 4 0 3 4 0 4 4 0 3 2 2 3 1 2 2 2 0 0 0 4 2 4 1 2 4 1 1 2 1 4 4 0 4 2 1 4 0 0 1 1 1 2 1 0 1 2 3 3 2 2 0 3 2 4 1 4 1 3 0 1 2 0 1 1 0 1 0 0 2 3 3 4 2 2 1 2 0 2 4 0 2 1 4 2 2 3 2 4 0 3 1 3 4 4 0 3 4 4 4 4 3 4 3 4 1 3 1 1 2 1 2 0 3 4 3 4 2 1 0 4 1 0 4 4 4 4 4 4 3 2 3 2 2 2 3 3 0 4 3 1 4 2 2 3 2 3 4 3 2 2 0 3 0 4 1 1 2 2 0 2 0 1 1 0 0 2 2 0 1 2 0 3 2 4 4 2 2 3 2 0 3 4 2 1 3 0 1 3 0 2 3 2 4 3 4 1 2 1 4 2 4 4 3 1 2 3 1 1 0 3 1 1 4 2 2 4 0 0 2 1 4 3 3 0 3 1 4 3 2 0 3 2 4 1 1 2 3 0 4 2 2 3 2 0 0 1 2 2 3 2 0 4 0 1 1 1 1 3 3 1 0 0 4 0 0 2 1 3 0 3 4 4 0 1 2 3 4 3 1 4 2 2 2 0 1 2 4 1 0 0 1 0 2 3 3 0 0 3 3 2 3 0 1 0 1 3 3 1 4 3 3 3 1 2 0 0 4 1 0 0 4 0 3 1 2 4 1 3 0 4 1 0 1 4 1 3 2 0 2 0 3 1 2 1 3 3 3 1 2 1 1 3 4 0 0 1 0 1 0 2 0 2 1 2 4 2 4 0 4 4 0 1 4 4 4 2 4 1 2 4 3 4 1 2 4 3 3 1 2 1 0 0 3 0 3 4 4 0 2 4 0 4 3 4 2 1 4 1 2 3 0 3 0 1 3 1 4 1 3 4 3 0 4 2 1 0 4 0 0 4 2 1 0 0 0 3 0 1 4 3 3 0 2 4 0 0 3 4 2 4 1 1 3 2 0 3 4 0 1 2 0 1 2 0 4 1 0 2 2 0 1 4 1 0 4 2 3 1 1 0 4 0 2 2 4 2 2 0 3 0 2 0 2 0 0 0 0 2 4 1 4 1 0 4 1 4 0 4 0 1 4 0 1 0 0 1 4 4 1 1 1 3 0 3 3 3 4 4 4 4 0 0 0 4 4 0 2 1 1 1 1 2 0 4 1 4 3 3 3 0 4 3 3 4 4 1 3 3 2 1 0 0 3 2 0 2 2 0 3 3 0 1 0 3 0 1 4 4 2 2 4 4 4 3 1 4 3 1 2 0 4 0 2 0 0 3 2 2 0 1 3 4 1 3 2 1 4 2 4 0 2 0 0 0 3 4 1 0 4 3 2 0 0 4 1 2 2 1 3 3 0 4 0 3 3 0 2 0 2 0 4 2 4 4 1 1 1 3 4 1 0 2 1 4 0 4 2 2 0 2 0 2 0 0 3 1 3 2 1 0 0 4 1 3 4 1 1 1 0 3 2 0 4 0 0 3 0 0 0 3 3 1 4 4 1 0 0 0 4 2 1 4 3 2 2 3 3 0 3 0 4 2 0 1 4 3 1 1 3 2 2 0 3 4 1 4 0 1 3 2 0 0 4 4 3 0 1 4 1 3 1 4 1 3 1 2 3 4 0 1 4 0 3 4 2 3 0 3 4 3 0 4 0 4 4 2 3 4 3 1 4 3 0 1 0 0 0 3 4 3 1 1 0 1 3 3 0 0 2 1 3 3 2 4 3 2 2 2 3 0 2 1 1 0 1 4 3 4 0 3 1 0 0 3 3 3 3 2 0 3 1 4 0 1 3 1 3 4 4 0 0 3 4 0 3 1 0 3 2 3 0 3 1 1 2 1 2 2 0 2 2 1 1 1 4 2 0 2 1 2 0 3 0 4 1 4 2 4 2 4 0 3 2 4 3 4 0 4 3 0 2 3 4 3 4 4 4 1 1 0 1 4 2 4 2 1 3 0 1 1 1 0 3 0 3 4 0 0 0 4 4 4 0 1 2 3 2 4 4 0 0 4 0 0 0 3 1 2 1 2 1 3 2 2 0 1 0 4 3 2 3 0 2 1 0 2 3 4 3 3 2 3 4 1 2 0 0 0 0 0 4 1 2 4 0 2 0 0 0 0 2 4 0 0 2 1 0 2 0 3 2 2 4 0 1 1 1 1 4 0 0 4 1 1 2 3 2 2 0 1 1 1 4 0 3 2 4 2 2 0 0 3 1 3 0 2 1 1 2 0 1 1 1 0 2 1 1 0 4 0 0 3 4 1 1 0 2 4 2 0 1 3 1 4 1 0 2 2 4 0 2 3 2 3 3 4 0 0 1 0 4 4 3 0 2 4 0 0 4 4 1 1 3 4 1 2 2 4 3 3 2 3 0 1 1 1 3 1 3 2 2 4 3 3 1 4 4 0 1 1 2 2 2 3 4 4 4 0 3 3 3 3 2 2 3 1 2 3 1 3 1 3 2 3 0 2 1 3 4 0 0 4 3 0 1 0 4 1 4 0 4 0 2 0 1 0 0 2 2 1 2 4 0 4 2 2 1 3 3 2 1 3 1 0 0 0 1 0 3 1 0 0 3 3 0 3 3 0 0 4 2 4 1 0 2 1 0 0 0 1 4 0 1 1 1 4 0 3 3 3 4 4 2 0 3 2 0 1 1 4 4 3 2 1 2 3 3 4 0 0 4 2 2 2 2 2 1 0 3 0 0 0 1 2 1 2 3 2 2 2 2 4 3 2 3 4 0 3 4 4 1 1 1 4 4 0 2 2 2 4 2 1 0 4 1 1 3 2 3 3 2 2 2 4 1 4 1 2 4 2 2 3 4 1 4 4 1 0 3 2 0 1 2 1 1 3 3 1 3 1 1 2 1 0 4 3 1 1 3 0 1 4 2 4 3 2 0 4 3 4 0 4 3 2 1 0 3 0 3 1 1 0 0 0 0 2 0 0 1 1 1 3 2 1 2 1 3 0 1 4 1 1 1 1 3 4 0 2 2 4 3 3 2 2 2 2 3 3 2 3 3 4 2 1 2 0 2 4 3 4 2 3 1 1 0 2 2 4 0 2 1 3 2 1 4 4 0 4 1 1 3 2 1 3 4 4 2 2 3 3 3 3 0 0 3 2 3 2 1 2 4 2 2 3 4 2 3 0 4 3 0 2 1 4 1 0 4 0 3 3 0 0 3 3 3 4 4 2 2 3 3 3 0 3 2 3 3 1 1 2 3 4 2 0 3 3 4 3 3 3 2 1 3 2 2 3 1 2 2 0 3 1 4 0 2 1 4 4 3 1 4 2 4 3 0 3 3 2 1 1 0 1 1 2 1 3 4 4 4 4 4 4 3 1 4 4 3 0 4 2 2 1 1 4 1 3 4 2 0 1 1 2 0 2 1 4 2 3 2 0 1 0 0 3 4 4 3 1 2 3 0 3 2 2 3 1 0 2 0 4 3 4 2 2 3 0 1 4 0 0 4 1 4 2 1 1 3 0 3 0 3 0 4 1 2 4 0 0 4 1 4 2 2 4 2 0 1 0 1 1 1 1 1 3 4 0 2 3 4 0 0 2 2 3 1 2 1 4 3 4 1 3 1 0 3 3 1 2 2 4 2 4 4 0 0 1 4 1 3 2 0 4 4 2 3 0 3 2 1 4 2 2 2 4 4 4 3 4 4 2 0 2 2 0 2 1 4 4 0 4 1 1 4 3 0 0 4 4 0 2 3 4 3 3 3 2 1 4 3 1 2 2 4 2 3 1 0 0 0 1 1 2 4 3 0 1 4 0 3 3 3 3 3 4 2 2 0 3 2 2 3 4 0 1 2 2 1 1 4 0 3 0 0 0 1 1 3 2 3 2 4 0 0 3 1 2 2 2 0 1 1 1 2 4 2 4 0 4 4 2 0 1 2 4 3 2 3 3 0 1 0 0 4 3 4 1 4 3 2 0 4 1 2 1 0 1 2 2 0 0 0 2 2 2 4 4 1 3 3 3 1 0 3 2 2 2 0 3 3 2 0 3 2 4 2 2 3 1 2 1 2 2 2 0 0 0 3 1 0 1 3 1 1 3 3 3 2 0 3 0 3 0 4 1 1 3 3 2 3 3 1 1 3 0 4 3 3 2 4 4 1 3 1 0 3 1 2 2 0 4 2 4 4 1 2 4 3 3 0 1 0 1 3 4 1 1 3 3 4 0 2 4 1 3 2 1 0 2 4 4 1 4 0 0 4 1 2 4 0 2 4 0 1 3 0 0 0 1 0 4 0 1 1 2 2 0 2 1 2 0 1 3 3 1 1 3 4 4 1 2 2 4 2 1 4 4 4 1 2 3 3 2 0 3 3 4 0 0 1 4 4 3 4 4 3 1 2 0 1 0 0 1 1 1 0 4 2 3 4 4 4 1 2 1 3 3 4 3 4 0 2 1 2 1 1 2 0 4 4 3 4 0 4 0 0 2 4 4 1 3 1 0 1 0 1 3 4 4 3 2 2 0 0 4 2 1 4 0 4 2 1 1 0 2 3 2 3 3 3 3 2 3 3 1 3 1 4 4 3 2 3 4 0 0 4 2 1 0 0 3 4 2 1 0 2 3 1 3 4 4 1 0 1 3 0 1 0 4 2 1 1 2 1 1 0 1 3 3 3 1 4 1 2 1 1 3 0 1 2 0 0 1 1 0 2 4 0 0 0 2 1 1 1 0 4 4 3 1 3 2 3 3 0 3 4 0 1 1 3 0 4 1 0 3 4 3 4 3 2 1 3 1 2 1 1 1 1 3 1 0 3 0 1 3 0 4 2 3 0 2 2 1 3 1 0 1 2 3 1 4 3 1 1 4 2 1 3 3 4 4 4 4 2 1 1 2 2 2 2 3 3 4 4 0 0 0 2 2 3 2 1 4 4 2 4 0 0 4 4 4 0 2 1 3 1 4 3 4 4 4 1 3 4 3 1 2 1 3 4 1 3 2 3 1 3 0 3 0 4 0 1 3 4 4 1 3 0 4 4 2 1 1 1 0 4 2 2 0 0 2 4 3 0 4 0 4 2 1 0 3 2 1 0 1 2 4 3 4 0 0 1 2 2 0 2 3 0 0 4 3 1 4 0 1 3 1 3 4 1 1 0 1 0 1 3 4 2 4 1 3 1 2 2 4 4 0 2 0 4 4 0 2 4 0 0 2 3 2 2 1 1 1 0 1 1 2 4 2 0 0 4 1 2 0 1 1 1 0 3 4 2 4 3 2 3 3 1 0 2 3 2 2 3 0 1 2 2 3 2 3 0 4 3 0 4 1 3 4 3 2 0 4 2 3 2 3 1 1 0 2 3 0 3 2 3 4 1 0 2 4 3 3 4 4 0 2 3 4 2 4 3 0 1 3 4 3 1 2 0 0 1 1 3 0 2 3 4 2 1 2 3 0 1 0 2 0 2 1 4 3 4 2 1 4 2 1 1 3 0 2 3 3 3 3 4 0 4 0 1 3 2 3 3 3 0 4 0 0 0 3 1 3 1 3 3 1 3 4 1 1 0 1 3 0 2 2 2 0 0 2 2 3 4 0 4 0 3 4 2 3 3 4 2 0 4 4 3 2 2 1 3 3 3 1 3 1 4 1 2 4 4 1 3 4 3 1 0 1 3 0 4 3 3 3 1 0 3 4 1 0 0 4 2 1 4 1 2 4 0 2 1 4 0 4 1 3 1 1 1 4 1 3 0 0 1 0 4 2 0 1 3 1 3 4 4 4 4 3 4 4 0 3 4 3 2 0 3 0 0 1 1 4 1 1 2 0 1 2 1 1 2 1 4 0 4 3 3 4 3 0 2 3 2 3 4 3 3 2 1 3 2 0 4 2 0 4 1 0 0 1 1 4 2 2 0 1 4 2 3 3 3 0 3 1 2 4 1 4 4 0 0 2 1 1 3 0 1 2 1 1 1 4 3 0 1 4 3 1 3 2 3 0 2 2 4 3 4 1 4 3 4 3 3 2 0 3 2 0 1 3 1 4 2 0 4 4 4 0 3 1 0 4 1 3 4 2 2 1 2 2 3 2 3 3 3 2 1 0 1 4 2 3 3 0 2 3 3 4 0 2 0 4 0 1 0 0 3 2 0 1 0 3 0 3 1 3 1 0 1 0 2 1 0 1 2 3 1 0 1 2 0 3 1 4 3 3 1 4 1 2 2 4 4 2 0 1 4 3 0 4 3 2 4 3 1 2 0 0 0 2 0 2 3 2 1 3 4 4 2 4 3 4 1 2 1 2 0 0 1 1 2 2 2 2 3 4 4 1 4 1 1 4 1 1 1 2 0 1 1 3 4 0 3 0 4 1 0 4 3 3 2 4 0 0 2 1 4 2 3 1 4 4 4 1 3 1 3 2 0 1 2 4 4 2 1 4 4 1 0 3 1 0 3 4 1 0 3 0 4 2 4 1 4 0 1 0 2 4 0 3 3 2 0 1 3 3 2 4 2 0 0 1 4 0 3 2 1 2 1 0 1 1 4 2 2 4 1 4 0 2 2 3 1 1 3 0 4 4 0 3 2 0 3 4 0 0 3 0 1 4 2 3 0 3 1 2 3 3 2 1 1 4 4 3 1 3 1 0 1 2 0 4 0 4 4 0 0 0 0 1 4 2 0 0 1 3 3 0 0 1 0 4 0 2 0 1 3 0 3 4 0 1 3 4 2 3 1 3 2 0 4 2 3 1 1 0 1 0 1 3 4 0 2 0 0 2 4 2 3 2 3 0 1 3 3 3 1 2 3 3 0 3 3 3 4 3 3 1 3 2 4 1 2 0 3 3 0 4 4 4 4 4 3 3 1 4 3 3 4 1 3 1 0 4 1 0 2 2 1 4 2 3 3 0 2 3 0 2 0 3 1 4 3 0 0 2 2 0 1 0 0 1 1 4 2 2 2 1 1 4 0 1 0 4 1 3 3 1 0 1 2 1 0 3 3 4 1 3 1 3 0 4 2 2 4 1 2 1 4 2 4 4 3 1 1 1 3 0 3 4 3 3 3 4 2 3 3 1 1 2 0 0 2 1 3 1 4 4 3 3 4 4 3 4 2 4 2 0 0 2 0 3 2 2 3 1 3 0 3 4 3 1 1 2 3 3 2 4 3 0 4 4 4 3 2 3 4 2 0 2 2 4 3 2 1 2 2 4 2 3 0 0 3 3 3 1 0 3 3 0 3 4 2 4 4 1 3 3 1 2 4 3 2 2 3 2 3 1 4 3 1 3 1 0 4 3 4 0 1 1 3 0 1 3 1 4 4 3 4 1 2 4 0 1 4 1 2 4 4 3 0 1 4 2 0 4 1 1 3 1 4 1 2 2 2 2 3 3 2 2 4 4 1 2 3 0 4 2 1 1 1 2 3 4 1 3 3 4 1 0 1 0 1 4 1 4 2 4 4 4 2 4 2 3 1 3 4 3 1 4 0 4 4 4 0 3 3 3 4 0 0 3 4 2 4 2 3 0 3 3 1 0 4 2 3 3 2 2 3 4 1 3 0 4 4 0 0 0 3 0 4 1 4 1 4 2 4 4 2 0 4 0 0 1 3 2 0 4 3 1 0 2 4 2 1 0 3 2 3 4 2 2 1 1 2 1 1 1 3 4 0 4 2 2 2 0 0 3 1 1 0 4 4 4 1 0 0 3 4 2 1 2 4 3 4 2 2 3 4 0 1 1 4 1 4 1 3 2 2 3 2 0 0 0 2 3 2 3 2 1 4 0 3 3 1 3 1 2 1 2 2 2 0 3 1 2 1 1 0 1 2 3 0 1 0 0 0 3 3 1 4 3 2 3 0 3 0 0 0 4 3 0 4 1 3 1 3 2 4 3 1 4 1 0 4 1 4 4 0 4 3 1 3 3 0 1 3 1 2 2 1 0 3 4 0 4 4 1 3 1 2 3 0 4 0 3 4 2 2 2 2 3 3 0 2 1 1 4 1 3 1 2 0 4 0 3 0 1 4 2 3 1 0 3 1 3 0 4 4 2 0 4 0 2 4 3 2 4 2 4 3 3 0 1 0 0 3 4 0 2 2 1 4 3 3 2 1 2 4 2 0 1 2 4 2 1 4 1 4 3 4 2 1 4 2 2 0 0 0 1 4 0 1 2 4 4 1 2 3 4 1 0 1 1 0 1 1 2 0 3 0 2 1 2 1 0 0 4 4 0 4 4 2 1 4 1 1 4 0 0 1 2 0 3 2 2 2 2 0 2 1 2 4 0 3 0 4 1 4 3 3 4 3 4 2 3 2 2 3 0 3 1 0 0 2 3 2 4 4 3 3 4 1 0 0 4 0 1 2 4 2 0 3 2 1 2 3 3 4 2 4 4 3 2 0 3 1 0 2 4 3 1 4 1 1 4 2 4 2 0 4 4 1 1 2 3 2 4 0 1 2 3 0 3 1 3 1 0 0 4 1 1 2 4 4 4 2 0 1 3 4 2 4 3 3 3 2 2 1 0 0 4 2 3 0 4 3 0 2 2 2 1 2 1 2 0 4 2 0 1 2 1 2 4 0 4 3 3 0 0 4 0 1 0 3 0 4 2 2 2 2 2 1 3 4 3 0 1 3 3 0 1 2 4 1 4 1 2 0 0 4 1 3 0 4 4 0 2 1 1 3 2 2 0 1 0 4 0 1 2 4 3 4 1 0 3 3 1 0 1 0 1 3 0 3 1 3 4 2 4 2 3 4 0 1 0 2 1 2 2 1 2 4 0 2 1 1 2 1 4 4 2 2 2 4 4 0 2 4 0 1 0 1 1 3 2 2 2 0 3 3 4 2 3 3 0 3 0 4 1 1 4 3 4 1 4 1 4 4 4 4 1 2 2 3 1 2 2 4 4 4 1 0 3 3 0 1 2 2 0 2 1 4 3 2 4 0 2 3 4 4 4 3 0 0 1 4 1 0 2 2 1 1 4 3 3 2 1 4 0 4 0 0 1 2 4 2 4 0 4 0 1 4 0 3 3 1 2 1 0 1 3 1 1 3 3 2 4 1 1 2 3 2 1 0 0 2 4 4 3 4 4 0 0 4 0 2 4 4 4 4 2 4 4 4 0 2 0 2 0 1 4 0 4 1 4 4 0 3 0 4 2 4 2 1 3 3 0 1 0 1 3 2 2 3 3 1 0 0 1 4 3 1 1 3 4 3 3 0 4 0 0 1 1 1 1 4 1 2 4 4 2 2 0 0 4 4 2 2 0 3 1 3 4 1 4 4 0 0 3 3 0 3 4 4 3 1 3 1 2 3 3 3 2 2 4 2 0 1 4 4 1 4 1 2 4 4 0 1 2 0 1 4 3 2 2 2 3 4 3 2 3 2 4 4 4 1 4 2 3 4 2 1 0 3 1 3 2 4 2 3 2 3 2 4 1 1 0 2 2 3 3 3 2 1 0 2 3 4 0 3 4 0 2 0 1 3 0 4 1 4 0 3 0 4 1 4 2 0 1 2 1 4 4 2 1 2 3 0 1 3 2 2 1 4 1 3 1 2 0 3 0 2 4 3 3 1 1 0 4 4 3 0 4 4 1 2 0 1 3 3 3 0 2 4 0 2 1 2 0 3 4 2 4 1 3 4 3 4 4 0 3 4 1 1 0 3 0 2 3 3 4 4 4 4 3 0 2 0 0 3 3 3 3 3 0 1 2 4 1 1 1 1 1 1 1 0 4 4 2 4 1 0 3 1 1 1 2 1 1 2 3 1 2 3 0 0 0 0 0 0 1 3 0 4 0 3 4 4 1 3 4 4 3 4 0 0 1 2 2 1 1 4 1 0 0 1 4 2 1 3 1 0 2 3 0 0 4 2 0 4 0 1 4 0 2 1 2 4 1 0 0 4 2 1 2 4 3 4 4 2 1 1 3 3 4 2 0 1 0 0 2 4 0 3 1 3 2 0 4 2 1 1 1 3 1 1 4 0 0 2 4 2 3 0 4 1 1 3 4 2 3 3 4 3 0 1 3 1 2 2 2 3 4 0 4 4 0 3 0 0 0 1 3 2 1 4 3 2 1 1 0 2 1 4 0 1 0 1 4 4 0 1 3 4 1 1 0 3 2 3 2 1 3 4 0 2 0 0 1 3 4 0 2 1 3 3 2 0 2 2 4 3 0 1 0 0 1 4 0 3 1 3 3 3 4 4 1 3 0 1 2 0 0 3 0 1 0 2 1 3 3 4 2 0 1 0 4 0 2 0 4 1 3 3 2 3 4 2 0 3 3 2 0 3 1 4 2 3 3 1 2 2 3 1 3 2 2 0 1 0 3 1 3 4 3 2 1 3 0 3 4 4 4 0 2 0 0 4 4 1 0 3 0 3 3 2 2 1 1 4 2 4 2 1 4 2 0 2 2 3 2 4 3 1 2 3 0 4 2 4 1 3 3 2 3 1 2 3 0 2 4 1 4 0 4 2 4 3 0 4 3 2 2 1 0 4 3 2 1 3 3 4 3 4 0 0 0 4 1 4 2 1 1 2 2 4 3 2 2 4 4 0 0 1 2 4 4 2 1 1 3 3 3 0 2 1 1 0 3 0 0 1 2 4 2 0 2 0 3 3 0 3 0 2 3 4 0 0 1 0 3 2 2 1 4 2 2 1 3 0 3 1 4 1 3 4 3 4 3 0 2 2 3 4 4 1 1 4 2 0 0 0 2 3 2 3 4 2 4 0 1 0 2 0 2 2 1 0 0 1 0 3 1 3 4 2 3 4 4 2 4 2 3 3 0 1 2 0 2 0 4 1 3 2 4 2 2 1 1 2 4 4 0 3 2 1 1 0 1 1 1 0 3 3 2 0 1 4 4 2 3 2 3 3 1 4 1 2 2 1 2 1 4 0 2 1 2 3 2 1 0 4 1 0 2 4 4 4 2 4 3 0 4 0 3 2 4 3 0 4 1 0 3 1 1 1 1 1 1 1 1 3 2 2 2 2 2 2 2 4 1 4 3 4 2 4 3 1 1 3 1 1 1 3 2 4 4 4 0 4 0 4 3 3 4 0 3 0 4 1 2 2 2 1 2 3 3 1 2 0 4 2 2 0 1 4 0 1 1 3 3 3 2 4 2 0 0 2 3 4 4 2 3 2 3 0 0 4 2 0 0 3 3 1 1 2 3 2 0 4 3 2 3 4 0 3 1 4 0 2 0 3 0 0 2 3 2 2 0 4 2 4 4 4 2 2 3 2 4 1 0 3 2 4 3 2 2 1 2 4 3 4 1 4 1 0 1 4 1 1 2 3 2 4 1 0 2 0 1 2 1 0 2 0 1 0 4 2 2 1 4 3 1 4 1 3 4 4 4 3 4 1 1 1 2 2 3 4 3 3 4 1 4 0 2 2 3 3 3 0 4 0 4 1 0 2 1 0 2 0 0 0 1 2 2 4 3 2 2 0 2 4 2 4 0 3 1 0 3 3 4 0 4 0 3 1 0 3 4 1 3 4 3 0 3 3 2 4 4 4 3 0 2 4 0 4 1 4 4 4 2 2 2 1 1 2 1 0 2 1 0 2 2 1 3 2 2 3 3 0 4 4 2 1 2 3 3 4 1 3 0 0 2 2 2 0 4 0 4 1 2 3 2 3 3 0 4 2 4 3 3 0 2 2 1 0 0 4 2 2 2 3 1 1 4 4 3 4 0 2 4 0 1 3 1 3 2 0 1 1 2 0 4 2 1 3 4 0 2 4 3 4 2 0 3 3 2 4 1 0 0 4 4 3 1 0 2 1 4 3 4 2 2 2 1 1 3 3 1 1 1 3 4 1 2 4 2 0 0 0 2 0 2 1 1 4 0 4 0 4 3 3 1 4 0 2 1 0 2 0 0 0 3 4 3 3 1 2 1 0 1 4 3 3 2 1 4 2 0 2 0 4 4 3 2 1 1 2 3 0 0 4 3 3 1 4 0 0 2 0 0 1 3 4 3 1 4 4 0 0 4 2 4 1 1 4 0 2 1 4 2 0 1 2 2 0 4 0 2 3 2 4 0 3 4 4 0 3 0 3 4 0 2 4 3 0 4 4 2 0 2 0 0 0 3 2 3 3 3 1 3 1 4 3 3 3 2 2 4 0 3 3 0 2 3 4 3 2 0 0 0 3 3 3 3 4 0 1 4 0 1 2 0 1 4 2 3 1 1 4 1 3 4 1 4 3 4 2 3 4 2 1 2 1 1 0 4 3 0 1 4 3 0 1 1 2 3 2 2 2 2 4 0 0 0 2 2 3 3 2 4 0 0 1 2 1 1 4 2 2 3 3 1 4 3 0 2 1 4 1 0 1 3 1 3 4 4 0 2 4 1 0 1 4 3 1 1 1 4 4 4 1 1 2 2 4 2 4 0 2 0 2 4 4 3 1 3 4 2 2 3 2 4 4 3 0 3 1 0 1 2 2 0 4 0 2 4 4 4 0 2 2 2 1 3 2 2 1 1 1 2 3 0 1 2 2 0 4 3 3 0 1 3 0 2 4 4 3 2 0 4 0 4 4 0 3 4 1 3 2 4 1 4 2 2 2 0 3 0 2 3 2 3 4 0 0 2 4 4 1 2 4 0 0 0 4 4 1 4 3 1 2 4 2 3 1 4 0 0 4 1 4 2 2 4 3 0 2 2 3 3 3 0 1 2 3 1 4 4 3 1 4 4 1 1 4 4 4 3 2 0 0 4 2 3 2 2 4 0 1 4 3 4 0 1 1 4 3 1 1 3 2 2 3 0 2 0 1 1 4 3 1 1 0 0 1 0 3 2 4 0 4 4 0 3 4 4 3 3 3 4 4 0 0 0 0 1 0 0 2 2 4 0 3 2 1 3 0 2 1 1 1 2 3 2 2 3 2 1 4 1 3 0 1 0 0 0 1 3 3 1 0 0 1 2 1 1 2 3 2 2 0 0 1 2 2 2 0 2 1 0 0 2 1 3 0 0 0 0 3 0 2 0 2 0 1 3 0 1 0 2 2 3 1 3 1 4 0 1 1 4 2 1 2 3 1 3 2 4 0 0 0 3 3 1 2 4 1 4 2 2 2 1 2 3 0 1 0 0 4 4 3 2 3 3 3 3 1 3 1 1 0 3 1 3 2 2 1 1 1 4 1 3 2 0 0 1 4 3 2 4 1 3 1 2 0 0 2 4 0 1 0 4 0 2 0 3 2 4 4 4 0 1 4 3 1 2 2 2 0 0 2 0 1 0 0 2 4 1 4 2 2 3 3 0 3 3 4 3 1 1 1 2 1 1 2 4 4 1 4 1 1 1 0 1 3 4 0 3 0 1 0 4 3 2 4 1 4 4 1 1 3 2 3 3 0 2 3 0 2 1 2 2 2 1 0 1 2 0 3 3 3 3 2 1 0 2 0 0 0 2 2 1 4 3 1 1 2 3 3 1 0 4 0 4 1 3 0 0 2 3 4 3 2 2 2 0 0 3 4 0 3 1 2 1 2 4 2 3 0 4 3 4 0 4 4 4 0 4 0 4 2 0 3 0 2 0 4 1 3 1 2 1 2 3 1 1 0 3 2 4 4 2 3 4 0 4 4 0 1 1 1 2 1 2 4 1 2 3 2 3 3 4 1 0 4 3 0 1 4 1 2 0 4 4 4 0 4 4 0 0 1 3 3 2 0 4 0 4 0 2 3 2 0 1 4 1 0 1 0 4 4 3 3 1 3 4 1 4 0 3 1 1 3 2 2 4 1 3 0 3 4 2 1 4 0 2 1 2 1 4 0 3 2 4 4 3 2 3 1 4 1 2 0 2 2 1 3 1 3 3 2 0 0 0 3 2 2 1 1 3 4 1 4 2 0 0 4 2 0 3 3 0 1 1 3 1 3 3 1 2 2 4 2 1 0 2 2 4 0 3 0 4 0 1 2 0 2 0 3 3 1 3 1 0 0 3 2 0 3 1 3 1 4 3 4 4 0 4 4 4 1 4 2 4 3 3 3 1 0 0 3 0 2 4 3 3 0 3 0 4 2 2 2 0 0 2 3 4 0 2 0 4 4 0 2 3 2 1 3 4 2 3 4 3 4 4 0 2 2 1 4 1 1 3 0 3 0 4 4 4 2 3 4 0 2 0 4 2 3 2 4 2 2 3 0 0 0 1 2 1 2 4 0 2 4 0 1 1 1 3 1 0 0 1 3 4 4 4 3 1 2 0 3 1 4 0 2 0 1 3 0 2 4 4 4 1 4 2 3 4 2 3 2 0 0 3 1 4 3 1 0 0 1 2 1 3 4 0 1 3 1 0 3 2 0 2 3 0 4 3 0 0 4 0 1 1 4 2 0 4 0 3 1 2 0 3 4 2 2 3 0 1 3 4 2 0 2 1 2 4 2 4 0 4 1 1 3 1 4 4 1 3 1 1 2 1 1 3 0 4 3 4 2 1 3 0 3 3 1 4 4 3 4 4 4 1 4 1 1 1 2 1 1 0 0 4 4 1 4 4 3 2 2 0 0 0 2 2 1 4 1 2 2 4 0 3 2 1 3 3 0 4 0 1 1 3 3 1 2 4 3 0 2 1 1 1 2 1 3 3 0 2 1 0 0 2 3 2 2 3 3 2 1 0 2 0 2 1 4 1 0 3 3 3 1 1 3 4 2 3 4 1 4 1 3 4 0 1 0 1 1 2 1 1 1 0 4 3 3 3 1 3 4 1 1 0 1 4 1 3 0 4 3 2 3 1 2 3 0 3 3 1 1 4 1 0 2 4 1 1 1 2 3 1 0 2 0 3 1 1 1 4 0 4 4 3 0 4 3 1 1 4 3 4 2 2 3 0 4 3 0 4 0 1 3 4 3 4 3 2 4 0 3 0 0 0 1 0 2 0 0 1 0 2 4 1 1 0 4 2 2 1 0 1 3 2 3 0 0 0 3 4 4 0 4 2 4 1 2 0 3 2 3 2 4 2 2 0 0 0 4 3 4 0 0 3 3 1 1 3 1 1 1 3 0 1 3 1 3 1 0 0 4 3 3 4 2 4 3 4 0 2 0 3 0 1 4 4 2 3 4 0 4 0 0 0 3 4 3 0 3 2 4 1 2 4 4 0 3 3 0 2 0 1 1 3 2 4 0 3 0 3 0 2 3 0 2 1 4 3 1 3 0 4 3 4 3 0 2 4 1 3 4 4 2 0 1 2 1 1 2 1 3 0 3 3 0 2 3 0 2 4 4 2 1 3 0 1 2 1 2 4 4 2 2 0 1 2 4 4 3 2 0 0 3 3 2 1 1 0 0 1 0 0 1 2 1 4 2 2 3 4 0 3 2 0 4 0 4 0 2 4 1 3 1 3 4 0 1 2 1 0 1 3 0 2 2 3 1 0 4 3 4 1 3 3 0 3 1 0 2 4 0 4 4 4 2 3 2 1 3 1 3 4 0 0 4 4 3 1 1 3 2 2 0 4 2 2 4 2 1 4 3 1 3 2 3 2 3 4 0 1 1 2 2 3 0 4 3 4 1 3 3 0 2 0 1 0 1 3 1 0 4 4 3 3 1 4 3 2 2 3 2 1 1 4 4 4 2 3 0 0 3 2 4 2 1 0 2 2 0 2 1 3 4 1 4 0 1 4 1 0 3 0 3 0 4 4 1 0 0 4 3 3 1 3 0 0 2 3 3 3 2 2 1 3 1 3 0 3 4 1 3 1 3 4 3 0 2 0 0 2 0 0 3 2 2 2 4 3 0 4 3 2 2 2 0 2 4 3 0 0 4 2 4 1 0 2 4 1 2 3 2 0 1 4 3 4 1 3 4 4 2 3 0 2 3 1 0 2 2 2 2 1 3 3 3 4 0 4 4 0 0 3 3 2 3 1 3 4 4 3 1 4 3 1 2 0 2 1 1 1 3 4 2 2 4 2 0 3 0 1 3 2 4 0 1 0 0 0 3 1 0 2 2 4 0 3 4 1 0 4 1 3 2 1 0 4 3 4 0 4 2 1 2 2 4 3 3 3 2 1 4 4 4 4 2 4 2 4 2 2 2 4 4 1 1 4 0 1 3 4 3 2 0 1 3 0 0 0 0 3 4 1 3 1 4 3 4 2 2 4 0 2 2 1 3 4 0 2 2 3 3 1 0 0 3 2 0 4 1 1 2 0 2 3 1 2 2 0 0 2 3 0 "
          ]
        }
      ],
      "source": [
        "# NEED TO SCRAMBLE THE DATA\n",
        "# https://stackoverflow.com/questions/43229034/randomly-shuffle-data-and-labels-from-different-files-in-the-same-order\n",
        "\n",
        "idx = np.random.permutation(len(x_train))\n",
        "x_train,y_train = x_train[idx], y_train[idx]\n",
        "\n",
        "for label in y_train:\n",
        "  print(label,end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMB_2Rn6orSD",
        "outputId": "4f23d8f2-79e5-4a85-ed64-a560380acfc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0 ] 2 [ 1 ] 2 [ 2 ] 2 [ 3 ] 4 [ 4 ] 2 [ 5 ] 3 [ 6 ] 3 [ 7 ] 3 [ 8 ] 4 [ 9 ] 2 [ 10 ] 3 [ 11 ] 2 [ 12 ] 0 [ 13 ] 2 [ 14 ] 2 [ 15 ] 2 [ 16 ] 3 [ 17 ] 4 [ 18 ] 2 [ 19 ] 4 [ 20 ] 3 [ 21 ] 3 [ 22 ] 3 [ 23 ] 0 [ 24 ] 4 [ 25 ] 2 [ 26 ] 2 [ 27 ] 2 [ 28 ] 2 [ 29 ] 2 [ 30 ] 2 [ 31 ] 3 [ 32 ] 2 [ 33 ] 2 [ 34 ] 2 [ 35 ] 2 [ 36 ] 2 [ 37 ] 3 [ 38 ] 2 [ 39 ] 2 [ 40 ] 3 [ 41 ] 2 [ 42 ] 2 [ 43 ] 3 [ 44 ] 2 [ 45 ] 3 [ 46 ] 2 [ 47 ] 2 [ 48 ] 4 [ 49 ] 2 [ 50 ] 3 [ 51 ] 2 [ 52 ] 1 [ 53 ] 2 [ 54 ] 4 [ 55 ] 0 [ 56 ] 2 [ 57 ] 2 [ 58 ] 0 [ 59 ] 0 [ 60 ] 3 [ 61 ] 2 [ 62 ] 4 [ 63 ] 4 [ 64 ] 2 [ 65 ] 3 [ 66 ] 2 [ 67 ] 2 [ 68 ] 3 [ 69 ] 2 [ 70 ] 2 [ 71 ] 3 [ 72 ] 2 [ 73 ] 2 [ 74 ] 4 [ 75 ] 2 [ 76 ] 3 [ 77 ] 4 [ 78 ] 0 [ 79 ] 2 [ 80 ] 3 [ 81 ] 2 [ 82 ] 3 [ 83 ] 2 [ 84 ] 4 [ 85 ] 4 [ 86 ] 3 [ 87 ] 3 [ 88 ] 4 [ 89 ] 2 [ 90 ] 2 [ 91 ] 4 [ 92 ] 2 [ 93 ] 4 [ 94 ] 2 [ 95 ] 3 [ 96 ] 2 [ 97 ] 3 [ 98 ] 3 [ 99 ] 4 [ 100 ] 2 [ 101 ] 0 [ 102 ] 2 [ 103 ] 2 [ 104 ] 3 [ 105 ] 1 [ 106 ] 3 [ 107 ] 2 [ 108 ] 0 [ 109 ] 3 [ 110 ] 4 [ 111 ] 2 [ 112 ] 3 [ 113 ] 1 [ 114 ] 0 [ 115 ] 4 [ 116 ] 3 [ 117 ] 0 [ 118 ] 3 [ 119 ] 4 [ 120 ] 1 [ 121 ] 3 [ 122 ] 2 [ 123 ] 4 [ 124 ] 3 [ 125 ] 2 [ 126 ] 3 [ 127 ] 3 [ 128 ] 2 [ 129 ] 3 [ 130 ] 3 [ 131 ] 3 [ 132 ] 4 [ 133 ] 4 [ 134 ] 2 [ 135 ] 2 [ 136 ] 3 [ 137 ] 3 [ 138 ] 2 [ 139 ] 2 [ 140 ] 3 [ 141 ] 2 [ 142 ] 2 [ 143 ] 3 [ 144 ] 0 [ 145 ] 2 [ 146 ] 3 [ 147 ] 2 [ 148 ] 2 [ 149 ] 2 [ 150 ] 3 [ 151 ] 2 [ 152 ] 2 [ 153 ] 2 [ 154 ] 3 [ 155 ] 3 [ 156 ] 2 [ 157 ] 0 [ 158 ] 4 [ 159 ] 0 [ 160 ] 2 [ 161 ] 3 [ 162 ] 3 [ 163 ] 2 [ 164 ] 3 [ 165 ] 2 [ 166 ] 2 [ 167 ] 0 [ 168 ] 4 [ 169 ] 0 [ 170 ] 2 [ 171 ] 2 [ 172 ] 2 [ 173 ] 2 [ 174 ] 3 [ 175 ] 3 [ 176 ] 0 [ 177 ] 2 [ 178 ] 3 [ 179 ] 3 [ 180 ] 2 [ 181 ] 0 [ 182 ] 2 [ 183 ] 4 [ 184 ] 3 [ 185 ] 2 [ 186 ] 2 [ 187 ] 4 [ 188 ] 3 [ 189 ] 2 [ 190 ] 2 [ 191 ] 3 [ 192 ] 0 [ 193 ] 3 [ 194 ] 3 [ 195 ] 2 [ 196 ] 3 [ 197 ] 3 [ 198 ] 2 [ 199 ] 3 [ 200 ] 4 [ 201 ] 2 [ 202 ] 2 [ 203 ] 2 [ 204 ] 2 [ 205 ] 3 [ 206 ] 2 [ 207 ] 1 [ 208 ] 1 [ 209 ] 3 [ 210 ] 2 [ 211 ] 2 [ 212 ] 4 [ 213 ] 2 [ 214 ] 3 [ 215 ] 1 [ 216 ] 3 [ 217 ] 3 [ 218 ] 2 [ 219 ] 4 [ 220 ] 2 [ 221 ] 2 [ 222 ] 3 [ 223 ] 4 [ 224 ] 2 [ 225 ] 2 [ 226 ] 4 [ 227 ] 2 [ 228 ] 3 [ 229 ] 2 [ 230 ] 3 [ 231 ] 2 [ 232 ] 3 [ 233 ] 4 [ 234 ] 2 [ 235 ] 3 [ 236 ] 3 [ 237 ] 2 [ 238 ] 2 [ 239 ] 2 [ 240 ] 2 [ 241 ] 3 [ 242 ] 2 [ 243 ] 2 [ 244 ] 2 [ 245 ] 2 [ 246 ] 1 [ 247 ] 3 [ 248 ] 3 [ 249 ] 4 [ 250 ] 4 [ 251 ] 2 [ 252 ] 2 [ 253 ] 2 [ 254 ] 3 [ 255 ] 3 [ 256 ] 0 [ 257 ] 2 [ 258 ] 3 [ 259 ] 3 [ 260 ] 4 [ 261 ] 4 [ 262 ] 3 [ 263 ] 2 [ 264 ] 3 [ 265 ] 3 [ 266 ] 3 [ 267 ] 2 [ 268 ] 0 [ 269 ] 2 [ 270 ] 2 [ 271 ] 2 [ 272 ] 2 [ 273 ] 2 [ 274 ] 3 [ 275 ] 3 [ 276 ] 3 [ 277 ] 2 [ 278 ] 2 [ 279 ] 3 [ 280 ] 4 [ 281 ] 2 [ 282 ] 2 [ 283 ] 2 [ 284 ] 4 [ 285 ] 2 [ 286 ] 2 [ 287 ] 1 [ 288 ] 2 [ 289 ] 4 [ 290 ] 4 [ 291 ] 2 [ 292 ] 4 [ 293 ] 3 [ 294 ] 2 [ 295 ] 2 [ 296 ] 2 [ 297 ] 2 [ 298 ] 2 [ 299 ] 2 [ 300 ] 3 [ 301 ] 4 [ 302 ] 2 [ 303 ] 3 [ 304 ] 3 [ 305 ] 2 [ 306 ] 4 [ 307 ] 3 [ 308 ] 4 [ 309 ] 0 [ 310 ] 2 [ 311 ] 4 [ 312 ] 2 [ 313 ] 2 [ 314 ] 2 [ 315 ] 4 [ 316 ] 3 [ 317 ] 3 [ 318 ] 2 [ 319 ] 2 [ 320 ] 3 [ 321 ] 3 [ 322 ] 4 [ 323 ] 3 [ 324 ] 0 [ 325 ] 2 [ 326 ] 4 [ 327 ] 2 [ 328 ] 4 [ 329 ] 3 [ 330 ] 4 [ 331 ] 3 [ 332 ] 1 [ 333 ] 3 [ 334 ] 3 [ 335 ] 0 [ 336 ] 2 [ 337 ] 3 [ 338 ] 2 [ 339 ] 2 [ 340 ] 2 [ 341 ] 3 [ 342 ] 3 [ 343 ] 3 [ 344 ] 4 [ 345 ] 2 [ 346 ] 4 [ 347 ] 1 [ 348 ] 3 [ 349 ] 4 [ 350 ] 2 [ 351 ] 2 [ 352 ] 0 [ 353 ] 2 [ 354 ] 4 [ 355 ] 2 [ 356 ] 2 [ 357 ] 2 [ 358 ] 2 [ 359 ] 2 [ 360 ] 2 [ 361 ] 0 [ 362 ] 3 [ 363 ] 2 [ 364 ] 1 [ 365 ] 2 [ 366 ] 2 [ 367 ] 2 [ 368 ] 2 [ 369 ] 4 [ 370 ] 0 [ 371 ] 3 [ 372 ] 2 [ 373 ] 2 [ 374 ] 3 [ 375 ] 0 [ 376 ] 3 [ 377 ] 1 [ 378 ] 2 [ 379 ] 4 [ 380 ] 2 [ 381 ] 0 [ 382 ] 4 [ 383 ] 3 [ 384 ] 2 [ 385 ] 2 [ 386 ] 2 [ 387 ] 3 [ 388 ] 3 [ 389 ] 2 [ 390 ] 4 [ 391 ] 2 [ 392 ] 4 [ 393 ] 4 [ 394 ] 2 [ 395 ] 0 [ 396 ] 2 [ 397 ] 3 [ 398 ] 2 [ 399 ] 3 [ 400 ] 3 [ 401 ] 2 [ 402 ] 4 [ 403 ] 2 [ 404 ] 4 [ 405 ] 2 [ 406 ] 2 [ 407 ] 4 [ 408 ] 3 [ 409 ] 2 [ 410 ] 2 [ 411 ] 3 [ 412 ] 3 [ 413 ] 2 [ 414 ] 2 [ 415 ] 3 [ 416 ] 2 [ 417 ] 3 [ 418 ] 3 [ 419 ] 2 [ 420 ] 2 [ 421 ] 2 [ 422 ] 3 [ 423 ] 2 [ 424 ] 4 [ 425 ] 2 [ 426 ] 2 [ 427 ] 3 [ 428 ] 4 [ 429 ] 2 [ 430 ] 2 [ 431 ] 0 [ 432 ] 4 [ 433 ] 2 [ 434 ] 3 [ 435 ] 0 [ 436 ] 2 [ 437 ] 2 [ 438 ] 2 [ 439 ] 0 [ 440 ] 3 [ 441 ] 2 [ 442 ] 3 [ 443 ] 3 [ 444 ] 2 [ 445 ] 2 [ 446 ] 2 [ 447 ] 2 [ 448 ] 3 [ 449 ] 4 [ 450 ] 3 [ 451 ] 3 [ 452 ] 2 [ 453 ] 3 [ 454 ] 0 [ 455 ] 2 [ 456 ] 3 [ 457 ] 3 [ 458 ] 2 [ 459 ] 3 [ 460 ] 2 [ 461 ] 2 [ 462 ] 3 [ 463 ] 3 [ 464 ] 0 [ 465 ] 2 [ 466 ] 2 [ 467 ] 2 [ 468 ] 2 [ 469 ] 0 [ 470 ] 2 [ 471 ] 2 [ 472 ] 0 [ 473 ] 0 [ 474 ] 3 [ 475 ] 3 [ 476 ] 3 [ 477 ] 2 [ 478 ] 2 [ 479 ] 2 [ 480 ] 2 [ 481 ] 0 [ 482 ] 2 [ 483 ] 4 [ 484 ] 4 [ 485 ] 2 [ 486 ] 3 [ 487 ] 2 [ 488 ] 4 [ 489 ] 2 [ 490 ] 2 [ 491 ] 2 [ 492 ] 2 [ 493 ] 4 [ 494 ] 2 [ 495 ] 0 [ 496 ] 3 [ 497 ] 2 [ 498 ] 2 [ 499 ] 0 [ 500 ] 2 [ 501 ] 3 [ 502 ] 3 [ 503 ] 4 [ 504 ] 3 [ 505 ] 0 [ 506 ] 2 [ 507 ] 2 [ 508 ] 4 [ 509 ] 3 [ 510 ] 2 [ 511 ] 3 [ 512 ] 3 [ 513 ] 4 [ 514 ] 3 [ 515 ] 4 [ 516 ] 3 [ 517 ] 3 [ 518 ] 1 [ 519 ] 2 [ 520 ] 2 [ 521 ] 3 [ 522 ] 4 [ 523 ] 2 [ 524 ] 2 [ 525 ] 2 [ 526 ] 2 [ 527 ] 1 [ 528 ] 4 [ 529 ] 0 [ 530 ] 0 [ 531 ] 2 [ 532 ] 1 [ 533 ] 3 [ 534 ] 4 [ 535 ] 3 [ 536 ] 4 [ 537 ] 2 [ 538 ] 2 [ 539 ] 2 [ 540 ] 2 [ 541 ] 4 [ 542 ] 2 [ 543 ] 1 [ 544 ] 2 [ 545 ] 3 [ 546 ] 3 [ 547 ] 2 [ 548 ] 0 [ 549 ] 2 [ 550 ] 2 [ 551 ] 3 [ 552 ] 4 [ 553 ] 3 [ 554 ] 2 [ 555 ] 0 [ 556 ] 1 [ 557 ] 3 [ 558 ] 2 [ 559 ] 4 [ 560 ] 3 [ 561 ] 2 [ 562 ] 2 [ 563 ] 2 [ 564 ] 2 [ 565 ] 0 [ 566 ] 2 [ 567 ] 3 [ 568 ] 0 [ 569 ] 3 [ 570 ] 2 [ 571 ] 2 [ 572 ] 2 [ 573 ] 2 [ 574 ] 3 [ 575 ] 3 [ 576 ] 3 [ 577 ] 2 [ 578 ] 4 [ 579 ] 0 [ 580 ] 2 [ 581 ] 4 [ 582 ] 4 [ 583 ] 3 [ 584 ] 3 [ 585 ] 2 [ 586 ] 2 [ 587 ] 2 [ 588 ] 2 [ 589 ] 2 [ 590 ] 3 [ 591 ] 2 [ 592 ] 3 [ 593 ] 3 [ 594 ] 2 [ 595 ] 3 [ 596 ] 2 [ 597 ] 3 [ 598 ] 2 [ 599 ] 2 [ 600 ] 1 [ 601 ] 2 [ 602 ] 2 [ 603 ] 4 [ 604 ] 2 [ 605 ] 3 [ 606 ] 2 [ 607 ] 3 [ 608 ] 2 [ 609 ] 2 [ 610 ] 0 [ 611 ] 4 [ 612 ] 0 [ 613 ] 2 [ 614 ] 3 [ 615 ] 0 [ 616 ] 2 [ 617 ] 4 [ 618 ] 2 [ 619 ] 4 [ 620 ] 2 [ 621 ] 2 [ 622 ] 3 [ 623 ] 3 [ 624 ] 4 [ 625 ] 2 [ 626 ] 2 [ 627 ] 2 [ 628 ] 2 [ 629 ] 2 [ 630 ] 4 [ 631 ] 3 [ 632 ] 0 [ 633 ] 2 [ 634 ] 2 [ 635 ] 3 [ 636 ] 4 [ 637 ] 3 [ 638 ] 3 [ 639 ] 2 [ 640 ] 4 [ 641 ] 2 [ 642 ] 0 [ 643 ] 2 [ 644 ] 4 [ 645 ] 2 [ 646 ] 3 [ 647 ] 2 [ 648 ] 2 [ 649 ] 2 [ 650 ] 2 [ 651 ] 3 [ 652 ] 0 [ 653 ] 0 [ 654 ] 3 [ 655 ] 3 [ 656 ] 2 [ 657 ] 4 [ 658 ] 2 [ 659 ] 2 [ 660 ] 3 [ 661 ] 2 [ 662 ] 2 [ 663 ] 4 [ 664 ] 2 [ 665 ] 2 [ 666 ] 2 [ 667 ] 2 [ 668 ] 2 [ 669 ] 3 [ 670 ] 2 [ 671 ] 2 [ 672 ] 2 [ 673 ] 3 [ 674 ] 2 [ 675 ] 3 [ 676 ] 2 [ 677 ] 2 [ 678 ] 1 [ 679 ] 2 [ 680 ] 2 [ 681 ] 2 [ 682 ] 4 [ 683 ] 2 [ 684 ] 3 [ 685 ] 3 [ 686 ] 4 [ 687 ] 2 [ 688 ] 3 [ 689 ] 3 [ 690 ] 2 [ 691 ] 2 [ 692 ] 2 [ 693 ] 1 [ 694 ] 3 [ 695 ] 4 [ 696 ] 1 [ 697 ] 2 [ 698 ] 2 [ 699 ] 3 [ 700 ] 2 [ 701 ] 2 [ 702 ] 2 [ 703 ] 3 [ 704 ] 0 [ 705 ] 2 [ 706 ] 3 [ 707 ] 3 [ 708 ] 4 [ 709 ] 2 [ 710 ] 3 [ 711 ] 3 [ 712 ] 4 [ 713 ] 4 [ 714 ] 3 [ 715 ] 3 [ 716 ] 1 [ 717 ] 3 [ 718 ] 2 [ 719 ] 0 [ 720 ] 2 [ 721 ] 2 [ 722 ] 2 [ 723 ] 2 [ 724 ] 3 [ 725 ] 2 [ 726 ] 4 [ 727 ] 4 [ 728 ] 3 [ 729 ] 2 [ 730 ] 3 [ 731 ] 4 [ 732 ] 2 [ 733 ] 3 [ 734 ] 4 [ 735 ] 4 [ 736 ] 4 [ 737 ] 2 [ 738 ] 3 [ 739 ] 2 [ 740 ] 1 [ 741 ] 2 [ 742 ] 2 [ 743 ] 3 [ 744 ] 0 [ 745 ] 1 [ 746 ] 3 [ 747 ] 3 [ 748 ] 3 [ 749 ] 2 [ 750 ] 4 [ 751 ] 1 [ 752 ] 2 [ 753 ] 2 [ 754 ] 1 [ 755 ] 3 [ 756 ] 4 [ 757 ] 2 [ 758 ] 0 [ 759 ] 2 [ 760 ] 3 [ 761 ] 0 [ 762 ] 2 [ 763 ] 2 [ 764 ] 4 [ 765 ] 3 [ 766 ] 2 [ 767 ] 2 [ 768 ] 0 [ 769 ] 2 [ 770 ] 2 [ 771 ] 0 [ 772 ] 2 [ 773 ] 2 [ 774 ] 3 [ 775 ] 2 [ 776 ] 4 [ 777 ] 2 [ 778 ] 3 [ 779 ] 3 [ 780 ] 3 [ 781 ] 3 [ 782 ] 2 [ 783 ] 4 [ 784 ] 2 [ 785 ] 3 [ 786 ] 0 [ 787 ] 3 [ 788 ] 2 [ 789 ] 3 [ 790 ] 2 [ 791 ] 0 [ 792 ] 3 [ 793 ] 4 [ 794 ] 2 [ 795 ] 2 [ 796 ] 4 [ 797 ] 3 [ 798 ] 2 [ 799 ] 2 [ 800 ] 2 [ 801 ] 4 [ 802 ] 2 [ 803 ] 3 [ 804 ] 2 [ 805 ] 2 [ 806 ] 3 [ 807 ] 2 [ 808 ] 3 [ 809 ] 2 [ 810 ] 2 [ 811 ] 2 [ 812 ] 3 [ 813 ] 3 [ 814 ] 4 [ 815 ] 2 [ 816 ] 3 [ 817 ] 3 [ 818 ] 2 [ 819 ] 3 [ 820 ] 1 [ 821 ] 0 [ 822 ] 2 [ 823 ] 2 [ 824 ] 4 [ 825 ] 2 [ 826 ] 2 [ 827 ] 4 [ 828 ] 2 [ 829 ] 2 [ 830 ] 2 [ 831 ] 2 [ 832 ] 3 [ 833 ] 2 [ 834 ] 0 [ 835 ] 2 [ 836 ] 1 [ 837 ] 4 [ 838 ] 2 [ 839 ] 2 [ 840 ] 2 [ 841 ] 2 [ 842 ] 2 [ 843 ] 4 [ 844 ] 2 [ 845 ] 3 [ 846 ] 2 [ 847 ] 2 [ 848 ] 2 [ 849 ] 2 [ 850 ] 2 [ 851 ] 2 [ 852 ] 4 [ 853 ] 2 [ 854 ] 3 [ 855 ] 3 [ 856 ] 3 [ 857 ] 3 [ 858 ] 2 [ 859 ] 2 [ 860 ] 2 [ 861 ] 3 [ 862 ] 2 [ 863 ] 2 [ 864 ] 0 [ 865 ] 2 [ 866 ] 3 [ 867 ] 2 [ 868 ] 3 [ 869 ] 3 [ 870 ] 1 [ 871 ] 3 [ 872 ] 2 [ 873 ] 4 [ 874 ] 2 [ 875 ] 2 [ 876 ] 3 [ 877 ] 2 [ 878 ] 2 [ 879 ] 3 [ 880 ] 2 [ 881 ] 2 [ 882 ] 2 [ 883 ] 2 [ 884 ] 3 [ 885 ] 3 [ 886 ] 2 [ 887 ] 0 [ 888 ] 3 [ 889 ] 2 [ 890 ] 4 [ 891 ] 4 [ 892 ] 2 [ 893 ] 2 [ 894 ] 2 [ 895 ] 2 [ 896 ] 3 [ 897 ] 2 [ 898 ] 3 [ 899 ] 2 [ 900 ] 2 [ 901 ] 3 [ 902 ] 2 [ 903 ] 2 [ 904 ] 2 [ 905 ] 2 [ 906 ] 2 [ 907 ] 4 [ 908 ] 2 [ 909 ] 2 [ 910 ] 3 [ 911 ] 3 [ 912 ] 4 [ 913 ] 2 [ 914 ] 2 [ 915 ] 4 [ 916 ] 0 [ 917 ] 2 [ 918 ] 2 [ 919 ] 0 [ 920 ] 3 [ 921 ] 4 [ 922 ] 3 [ 923 ] 4 [ 924 ] 3 [ 925 ] 2 [ 926 ] 2 [ 927 ] 0 [ 928 ] 4 [ 929 ] 3 [ 930 ] 2 [ 931 ] 2 [ 932 ] 2 [ 933 ] 2 [ 934 ] 3 [ 935 ] 2 [ 936 ] 3 [ 937 ] 2 [ 938 ] 0 [ 939 ] 2 [ 940 ] 2 [ 941 ] 4 [ 942 ] 2 [ 943 ] 0 [ 944 ] 2 [ 945 ] 1 [ 946 ] 2 [ 947 ] 0 [ 948 ] 2 [ 949 ] 2 [ 950 ] 2 [ 951 ] 2 [ 952 ] 3 [ 953 ] 2 [ 954 ] 4 [ 955 ] 3 [ 956 ] 4 [ 957 ] 4 [ 958 ] 4 [ 959 ] 0 [ 960 ] 3 [ 961 ] 1 [ 962 ] 2 [ 963 ] 4 [ 964 ] 2 [ 965 ] 3 [ 966 ] 3 [ 967 ] 3 [ 968 ] 2 [ 969 ] 0 [ 970 ] 2 [ 971 ] 2 [ 972 ] 3 [ 973 ] 0 [ 974 ] 0 [ 975 ] 2 [ 976 ] 2 [ 977 ] 2 [ 978 ] 3 [ 979 ] 3 [ 980 ] 2 [ 981 ] 3 [ 982 ] 2 [ 983 ] 2 [ 984 ] 2 [ 985 ] 3 [ 986 ] 3 [ 987 ] 3 [ 988 ] 2 [ 989 ] 3 [ 990 ] 2 [ 991 ] 2 [ 992 ] 4 [ 993 ] 4 [ 994 ] 3 [ 995 ] 2 [ 996 ] 4 [ 997 ] 4 [ 998 ] 2 [ 999 ] 4 [ 1000 ] 4 [ 1001 ] 2 [ 1002 ] 2 [ 1003 ] 2 [ 1004 ] 2 [ 1005 ] 2 [ 1006 ] 2 [ 1007 ] 2 [ 1008 ] 3 [ 1009 ] 3 [ 1010 ] 2 [ 1011 ] 4 [ 1012 ] 4 [ 1013 ] 2 [ 1014 ] 4 [ 1015 ] 2 [ 1016 ] 3 [ 1017 ] 2 [ 1018 ] 2 [ 1019 ] 4 [ 1020 ] 3 [ 1021 ] 0 [ 1022 ] 2 [ 1023 ] 2 [ 1024 ] 4 [ 1025 ] 2 [ 1026 ] 2 [ 1027 ] 0 [ 1028 ] 2 [ 1029 ] 3 [ 1030 ] 2 [ 1031 ] 2 [ 1032 ] 0 [ 1033 ] 2 [ 1034 ] 3 [ 1035 ] 3 [ 1036 ] 2 [ 1037 ] 4 [ 1038 ] 3 [ 1039 ] 2 [ 1040 ] 2 [ 1041 ] 3 [ 1042 ] 2 [ 1043 ] 2 [ 1044 ] 3 [ 1045 ] 4 [ 1046 ] 2 [ 1047 ] 2 [ 1048 ] 0 [ 1049 ] 2 [ 1050 ] 2 [ 1051 ] 2 [ 1052 ] 4 [ 1053 ] 0 [ 1054 ] 3 [ 1055 ] 3 [ 1056 ] 3 [ 1057 ] 0 [ 1058 ] 3 [ 1059 ] 2 [ 1060 ] 2 [ 1061 ] 2 [ 1062 ] 3 [ 1063 ] 2 [ 1064 ] 2 [ 1065 ] 2 [ 1066 ] 3 [ 1067 ] 2 [ 1068 ] 3 [ 1069 ] 3 [ 1070 ] 4 [ 1071 ] 2 [ 1072 ] 3 [ 1073 ] 3 [ 1074 ] 0 [ 1075 ] 2 [ 1076 ] 4 [ 1077 ] 3 [ 1078 ] 4 [ 1079 ] 2 [ 1080 ] 2 [ 1081 ] 0 [ 1082 ] 1 [ 1083 ] 2 [ 1084 ] 4 [ 1085 ] 2 [ 1086 ] 2 [ 1087 ] 0 [ 1088 ] 2 [ 1089 ] 2 [ 1090 ] 2 [ 1091 ] 2 [ 1092 ] 4 [ 1093 ] 3 [ 1094 ] 3 [ 1095 ] 4 [ 1096 ] 0 [ 1097 ] 3 [ 1098 ] 2 [ 1099 ] 4 [ 1100 ] 2 [ 1101 ] 2 [ 1102 ] 2 [ 1103 ] 2 [ 1104 ] 2 [ 1105 ] 3 [ 1106 ] 3 [ 1107 ] 2 [ 1108 ] 2 [ 1109 ] 4 [ 1110 ] 2 [ 1111 ] 3 [ 1112 ] 2 [ 1113 ] 2 [ 1114 ] 0 [ 1115 ] 2 [ 1116 ] 2 [ 1117 ] 2 [ 1118 ] 4 [ 1119 ] 2 [ 1120 ] 2 [ 1121 ] 2 [ 1122 ] 3 [ 1123 ] 2 [ 1124 ] 2 [ 1125 ] 2 [ 1126 ] 2 [ 1127 ] 0 [ 1128 ] 2 [ 1129 ] 3 [ 1130 ] 4 [ 1131 ] 2 [ 1132 ] 2 [ 1133 ] 3 [ 1134 ] 2 [ 1135 ] 3 [ 1136 ] 1 [ 1137 ] 2 [ 1138 ] 2 [ 1139 ] 3 [ 1140 ] 2 [ 1141 ] 2 [ 1142 ] 3 [ 1143 ] 2 [ 1144 ] 2 [ 1145 ] 2 [ 1146 ] 0 [ 1147 ] 2 [ 1148 ] 4 [ 1149 ] 3 [ 1150 ] 3 [ 1151 ] 2 [ 1152 ] 2 [ 1153 ] 0 [ 1154 ] 0 [ 1155 ] 0 [ 1156 ] 3 [ 1157 ] 0 [ 1158 ] 2 [ 1159 ] 2 [ 1160 ] 2 [ 1161 ] 0 [ 1162 ] 2 [ 1163 ] 1 [ 1164 ] 2 [ 1165 ] 3 [ 1166 ] 3 [ 1167 ] 2 [ 1168 ] 4 [ 1169 ] 0 [ 1170 ] 4 [ 1171 ] 1 [ 1172 ] 2 [ 1173 ] 3 [ 1174 ] 2 [ 1175 ] 2 [ 1176 ] 2 [ 1177 ] 4 [ 1178 ] 0 [ 1179 ] 2 [ 1180 ] 4 [ 1181 ] 0 [ 1182 ] 4 [ 1183 ] 2 [ 1184 ] 2 [ 1185 ] 2 [ 1186 ] 2 [ 1187 ] 0 [ 1188 ] 3 [ 1189 ] 2 [ 1190 ] 4 [ 1191 ] 2 [ 1192 ] 3 [ 1193 ] 0 [ 1194 ] 2 [ 1195 ] 4 [ 1196 ] 3 [ 1197 ] 3 [ 1198 ] 2 [ 1199 ] 0 [ 1200 ] 2 [ 1201 ] 2 [ 1202 ] 2 [ 1203 ] 3 [ 1204 ] 4 [ 1205 ] 3 [ 1206 ] 2 [ 1207 ] 3 [ 1208 ] 3 [ 1209 ] 2 [ 1210 ] 0 [ 1211 ] 2 [ 1212 ] 2 [ 1213 ] 2 [ 1214 ] 2 [ 1215 ] 4 [ 1216 ] 3 [ 1217 ] 2 [ 1218 ] 4 [ 1219 ] 2 [ 1220 ] 0 [ 1221 ] 2 [ 1222 ] 3 [ 1223 ] 2 [ 1224 ] 2 [ 1225 ] 2 [ 1226 ] 0 [ 1227 ] 4 [ 1228 ] 2 [ 1229 ] 3 [ 1230 ] 3 [ 1231 ] 2 [ 1232 ] 0 [ 1233 ] 3 [ 1234 ] 2 [ 1235 ] 1 [ 1236 ] 2 [ 1237 ] 2 [ 1238 ] 2 [ 1239 ] 3 [ 1240 ] 3 [ 1241 ] 2 [ 1242 ] 4 [ 1243 ] 2 [ 1244 ] 2 [ 1245 ] 2 [ 1246 ] 1 [ 1247 ] 2 [ 1248 ] 3 [ 1249 ] 0 [ 1250 ] 0 [ 1251 ] 4 [ 1252 ] 2 [ 1253 ] 1 [ 1254 ] 4 [ 1255 ] 4 [ 1256 ] 2 [ 1257 ] 2 [ 1258 ] 3 [ 1259 ] 2 [ 1260 ] 2 [ 1261 ] 3 [ 1262 ] 2 [ 1263 ] 3 [ 1264 ] 3 [ 1265 ] 0 [ 1266 ] 2 [ 1267 ] 2 [ 1268 ] 4 [ 1269 ] 2 [ 1270 ] 2 [ 1271 ] 4 [ 1272 ] 3 [ 1273 ] 2 [ 1274 ] 3 [ 1275 ] 3 [ 1276 ] 3 [ 1277 ] 2 [ 1278 ] 3 [ 1279 ] 4 [ 1280 ] 0 [ 1281 ] 2 [ 1282 ] 2 [ 1283 ] 2 [ 1284 ] 4 [ 1285 ] 3 [ 1286 ] 3 [ 1287 ] 2 [ 1288 ] 3 [ 1289 ] 2 [ 1290 ] 0 [ 1291 ] 2 [ 1292 ] 4 [ 1293 ] 3 [ 1294 ] 4 [ 1295 ] 2 [ 1296 ] 2 [ 1297 ] 2 [ 1298 ] 3 [ 1299 ] 2 [ 1300 ] 4 [ 1301 ] 3 [ 1302 ] 2 [ 1303 ] 2 [ 1304 ] 2 [ 1305 ] 0 [ 1306 ] 2 [ 1307 ] 4 [ 1308 ] 2 [ 1309 ] 4 [ 1310 ] 3 [ 1311 ] 2 [ 1312 ] 2 [ 1313 ] 3 [ 1314 ] 2 [ 1315 ] 2 [ 1316 ] 2 [ 1317 ] 4 [ 1318 ] 3 [ 1319 ] 1 [ 1320 ] 0 [ 1321 ] 2 [ 1322 ] 2 [ 1323 ] 3 [ 1324 ] 2 [ 1325 ] 2 [ 1326 ] 4 [ 1327 ] 2 [ 1328 ] 4 [ 1329 ] 3 [ 1330 ] 3 [ 1331 ] 2 [ 1332 ] 4 [ 1333 ] 3 [ 1334 ] 2 [ 1335 ] 3 [ 1336 ] 3 [ 1337 ] 2 [ 1338 ] 3 [ 1339 ] 3 [ 1340 ] 2 [ 1341 ] 2 [ 1342 ] 3 [ 1343 ] 4 [ 1344 ] 4 [ 1345 ] 3 [ 1346 ] 2 [ 1347 ] 2 [ 1348 ] 1 [ 1349 ] 4 [ 1350 ] 2 [ 1351 ] 3 [ 1352 ] 1 [ 1353 ] 2 [ 1354 ] 4 [ 1355 ] 3 [ 1356 ] 2 [ 1357 ] 0 [ 1358 ] 4 [ 1359 ] 2 [ 1360 ] 2 [ 1361 ] 0 [ 1362 ] 4 [ 1363 ] 2 [ 1364 ] 3 [ 1365 ] 3 [ 1366 ] 2 [ 1367 ] 3 [ 1368 ] 2 [ 1369 ] 3 [ 1370 ] 3 [ 1371 ] 2 [ 1372 ] 2 [ 1373 ] 2 [ 1374 ] 2 [ 1375 ] 2 [ 1376 ] 4 [ 1377 ] 2 [ 1378 ] 3 [ 1379 ] 0 [ 1380 ] 0 [ 1381 ] 2 [ 1382 ] 2 [ 1383 ] 2 [ 1384 ] 3 [ 1385 ] 2 [ 1386 ] 4 [ 1387 ] 3 [ 1388 ] 2 [ 1389 ] 4 [ 1390 ] 0 [ 1391 ] 0 [ 1392 ] 2 [ 1393 ] 2 [ 1394 ] 3 [ 1395 ] 0 [ 1396 ] 2 [ 1397 ] 2 [ 1398 ] 3 [ 1399 ] 4 [ 1400 ] 2 [ 1401 ] 2 [ 1402 ] 2 [ 1403 ] 2 [ 1404 ] 2 [ 1405 ] 3 [ 1406 ] 0 [ 1407 ] 4 [ 1408 ] 3 [ 1409 ] 0 [ 1410 ] 3 [ 1411 ] 3 [ 1412 ] 2 [ 1413 ] 4 [ 1414 ] 2 [ 1415 ] 3 [ 1416 ] 2 [ 1417 ] 0 [ 1418 ] 4 [ 1419 ] 4 [ 1420 ] 0 [ 1421 ] 2 [ 1422 ] 3 [ 1423 ] 3 [ 1424 ] 2 [ 1425 ] 2 [ 1426 ] 4 [ 1427 ] 3 [ 1428 ] 2 [ 1429 ] 2 [ 1430 ] 4 [ 1431 ] 4 [ 1432 ] 2 [ 1433 ] 3 [ 1434 ] 2 [ 1435 ] 3 [ 1436 ] 2 [ 1437 ] 2 [ 1438 ] 4 [ 1439 ] 3 [ 1440 ] 2 [ 1441 ] 2 [ 1442 ] 4 [ 1443 ] 3 [ 1444 ] 4 [ 1445 ] 2 [ 1446 ] 2 [ 1447 ] 3 [ 1448 ] 4 [ 1449 ] 1 [ 1450 ] 2 [ 1451 ] 1 [ 1452 ] 4 [ 1453 ] 3 [ 1454 ] 3 [ 1455 ] 2 [ 1456 ] 2 [ 1457 ] 4 [ 1458 ] 2 [ 1459 ] 2 [ 1460 ] 4 [ 1461 ] 2 [ 1462 ] 2 [ 1463 ] 2 [ 1464 ] 4 [ 1465 ] 2 [ 1466 ] 0 [ 1467 ] 1 [ 1468 ] 3 [ 1469 ] 2 [ 1470 ] 3 [ 1471 ] 1 [ 1472 ] 2 [ 1473 ] 2 [ 1474 ] 1 [ 1475 ] 2 [ 1476 ] 4 [ 1477 ] 2 [ 1478 ] 2 [ 1479 ] 3 [ 1480 ] 2 [ 1481 ] 2 [ 1482 ] 3 [ 1483 ] 2 [ 1484 ] 3 [ 1485 ] 3 [ 1486 ] 1 [ 1487 ] 3 [ 1488 ] 3 [ 1489 ] 2 [ 1490 ] 2 [ 1491 ] 2 [ 1492 ] 3 [ 1493 ] 2 [ 1494 ] 2 [ 1495 ] 2 [ 1496 ] 2 [ 1497 ] 2 [ 1498 ] 2 [ 1499 ] 2 [ 1500 ] 3 [ 1501 ] 2 [ 1502 ] 2 [ 1503 ] 3 [ 1504 ] 3 [ 1505 ] 2 [ 1506 ] 2 [ 1507 ] 0 [ 1508 ] 2 [ 1509 ] 0 [ 1510 ] 0 [ 1511 ] 3 [ 1512 ] 2 [ 1513 ] 0 [ 1514 ] 3 [ 1515 ] 2 [ 1516 ] 2 [ 1517 ] 2 [ 1518 ] 3 [ 1519 ] 2 [ 1520 ] 3 [ 1521 ] 2 [ 1522 ] 1 [ 1523 ] 3 [ 1524 ] 2 [ 1525 ] 2 [ 1526 ] 2 [ 1527 ] 2 [ 1528 ] 2 [ 1529 ] 2 [ 1530 ] 3 [ 1531 ] 4 [ 1532 ] 3 [ 1533 ] 2 [ 1534 ] 2 [ 1535 ] 0 [ 1536 ] 0 [ 1537 ] 2 [ 1538 ] 0 [ 1539 ] 4 [ 1540 ] 2 [ 1541 ] 2 [ 1542 ] 4 [ 1543 ] 2 [ 1544 ] 3 [ 1545 ] 2 [ 1546 ] 2 [ 1547 ] 3 [ 1548 ] 2 [ 1549 ] 2 [ 1550 ] 0 [ 1551 ] 4 [ 1552 ] 4 [ 1553 ] 3 [ 1554 ] 2 [ 1555 ] 2 [ 1556 ] 2 [ 1557 ] 0 [ 1558 ] 2 [ 1559 ] 4 [ 1560 ] 3 [ 1561 ] 0 [ 1562 ] 2 [ 1563 ] 2 [ 1564 ] 2 [ 1565 ] 3 [ 1566 ] 2 [ 1567 ] 1 [ 1568 ] 4 [ 1569 ] 2 [ 1570 ] 2 [ 1571 ] 2 [ 1572 ] 4 [ 1573 ] 2 [ 1574 ] 2 [ 1575 ] 2 [ 1576 ] 2 [ 1577 ] 1 [ 1578 ] 3 [ 1579 ] 3 [ 1580 ] 2 [ 1581 ] 4 [ 1582 ] 2 [ 1583 ] 2 [ 1584 ] 2 [ 1585 ] 2 [ 1586 ] 3 [ 1587 ] 2 [ 1588 ] 3 [ 1589 ] 0 [ 1590 ] 1 [ 1591 ] 2 [ 1592 ] 1 [ 1593 ] 2 [ 1594 ] 2 [ 1595 ] 2 [ 1596 ] 2 [ 1597 ] 3 [ 1598 ] 3 [ 1599 ] 2 [ 1600 ] 2 [ 1601 ] 4 [ 1602 ] 3 [ 1603 ] 3 [ 1604 ] 2 [ 1605 ] 1 [ 1606 ] 2 [ 1607 ] 2 [ 1608 ] 2 [ 1609 ] 2 [ 1610 ] 2 [ 1611 ] 3 [ 1612 ] 1 [ 1613 ] 2 [ 1614 ] 3 [ 1615 ] 2 [ 1616 ] 2 [ 1617 ] 3 [ 1618 ] 2 [ 1619 ] 2 [ 1620 ] 2 [ 1621 ] 2 [ 1622 ] 2 [ 1623 ] 2 [ 1624 ] 4 [ 1625 ] 4 [ 1626 ] 2 [ 1627 ] 2 [ 1628 ] 2 [ 1629 ] 2 [ 1630 ] 2 [ 1631 ] 2 [ 1632 ] 2 [ 1633 ] 4 [ 1634 ] 2 [ 1635 ] 2 [ 1636 ] 2 [ 1637 ] 2 [ 1638 ] 2 [ 1639 ] 2 [ 1640 ] 0 [ 1641 ] 3 [ 1642 ] 2 [ 1643 ] 4 [ 1644 ] 2 [ 1645 ] 2 [ 1646 ] 3 [ 1647 ] 0 [ 1648 ] 2 [ 1649 ] 2 [ 1650 ] 3 [ 1651 ] 3 [ 1652 ] 0 [ 1653 ] 4 [ 1654 ] 2 [ 1655 ] 1 [ 1656 ] 2 [ 1657 ] 2 [ 1658 ] 1 [ 1659 ] 0 [ 1660 ] 4 [ 1661 ] 2 [ 1662 ] 3 [ 1663 ] 2 [ 1664 ] 2 [ 1665 ] 4 [ 1666 ] 3 [ 1667 ] 4 [ 1668 ] 3 [ 1669 ] 3 [ 1670 ] 2 [ 1671 ] 4 [ 1672 ] 2 [ 1673 ] 0 [ 1674 ] 3 [ 1675 ] 4 [ 1676 ] 3 [ 1677 ] 2 [ 1678 ] 3 [ 1679 ] 4 [ 1680 ] 2 [ 1681 ] 1 [ 1682 ] 3 [ 1683 ] 2 [ 1684 ] 0 [ 1685 ] 4 [ 1686 ] 4 [ 1687 ] 4 [ 1688 ] 3 [ 1689 ] 4 [ 1690 ] 2 [ 1691 ] 2 [ 1692 ] 4 [ 1693 ] 2 [ 1694 ] 2 [ 1695 ] 2 [ 1696 ] 4 [ 1697 ] 0 [ 1698 ] 4 [ 1699 ] 2 [ 1700 ] 0 [ 1701 ] 3 [ 1702 ] 3 [ 1703 ] 2 [ 1704 ] 0 [ 1705 ] 3 [ 1706 ] 2 [ 1707 ] 2 [ 1708 ] 2 [ 1709 ] 2 [ 1710 ] 3 [ 1711 ] 4 [ 1712 ] 2 [ 1713 ] 2 [ 1714 ] 2 [ 1715 ] 3 [ 1716 ] 2 [ 1717 ] 3 [ 1718 ] 2 [ 1719 ] 4 [ 1720 ] 3 [ 1721 ] 3 [ 1722 ] 2 [ 1723 ] 4 [ 1724 ] 3 [ 1725 ] 2 [ 1726 ] 2 [ 1727 ] 0 [ 1728 ] 2 [ 1729 ] 3 [ 1730 ] 1 [ 1731 ] 0 [ 1732 ] 2 "
          ]
        }
      ],
      "source": [
        "# NEED TO SCRAMBLE THE DATA\n",
        "# https://stackoverflow.com/questions/43229034/randomly-shuffle-data-and-labels-from-different-files-in-the-same-order\n",
        "\n",
        "idx = np.random.permutation(len(x_val))\n",
        "x_val,y_val = x_val[idx], y_val[idx]\n",
        "\n",
        "for i in range(len(y_val)):\n",
        "  print(\"[\",i,\"]=\",y_val[i],end=\"  \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB4Bne1vWwmm"
      },
      "source": [
        "# TFLITE Interpreter\n",
        "\n",
        "https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python\n",
        "https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter#get_input_details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI5YHBJfaCKc"
      },
      "source": [
        "## lbest_doubleCNNmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V20SuHkIaFV7"
      },
      "outputs": [],
      "source": [
        "def python_predict(data_index):\n",
        "  # Load TFLite model and allocate tensors.\n",
        "  interpreter = tf.lite.Interpreter(model_path=\"best_doubleCNNmodel.tflite\")\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Get input and output tensors.\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "\n",
        "  # Test model on random input data.\n",
        "  input_shape = input_details[0]['shape']\n",
        "  input_data = np.array(x_test[data_index:data_index+1, :], dtype=np.float32)\n",
        "  print(\"TEST EEG DATA SAMPLE:\")\n",
        "  print(input_data)\n",
        "\n",
        "  interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "  interpreter.invoke()\n",
        "\n",
        "  # The function `get_tensor()` returns a copy of the tensor data.\n",
        "  # Use `tensor()` in order to get a pointer to the tensor.\n",
        "\n",
        "  output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "  print(\"LABEL:\", y_test[data_index])\n",
        "  print(\"PREDICTION:\", output_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python_predict(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9wg5IyDXgBw",
        "outputId": "8625ebb2-615a-43d5-a1f0-ab8a4f4417ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST EEG DATA SAMPLE:\n",
            "[[[[-3.7064590e-05]\n",
            "   [-4.0042542e-05]\n",
            "   [-4.1466148e-05]\n",
            "   ...\n",
            "   [-2.7382621e-06]\n",
            "   [-2.9561609e-06]\n",
            "   [-3.7115435e-06]]]]\n",
            "LABEL: 0\n",
            "PREDICTION: [[0.69921875 0.1484375  0.109375   0.03515625 0.00390625]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python_predict(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw5E7DkbYG3A",
        "outputId": "faae9445-f5af-432b-d4f7-047ab04ff647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST EEG DATA SAMPLE:\n",
            "[[[[-9.3716644e-06]\n",
            "   [-1.0735943e-05]\n",
            "   [-1.0627420e-05]\n",
            "   ...\n",
            "   [-1.0658427e-05]\n",
            "   [-8.1624166e-06]\n",
            "   [-6.9376670e-06]]]]\n",
            "LABEL: 1\n",
            "PREDICTION: [[0.0390625  0.21875    0.55859375 0.1171875  0.0625    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python_predict(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md6eYRWTYPIo",
        "outputId": "c2da3b1a-d50a-44ee-c05f-38d096769fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST EEG DATA SAMPLE:\n",
            "[[[[-8.5924776e-06]\n",
            "   [-5.7162129e-06]\n",
            "   [-3.6243839e-06]\n",
            "   ...\n",
            "   [ 6.0357975e-06]\n",
            "   [ 2.3605708e-06]\n",
            "   [-2.9706873e-06]]]]\n",
            "LABEL: 2\n",
            "PREDICTION: [[0.1328125  0.15625    0.62109375 0.08203125 0.0078125 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python_predict(24)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq7rfSCeYdoQ",
        "outputId": "03579068-2ee4-47d3-e0c6-5bf368b1a054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST EEG DATA SAMPLE:\n",
            "[[[[2.3220753e-05]\n",
            "   [2.8043580e-05]\n",
            "   [3.0716474e-05]\n",
            "   ...\n",
            "   [4.7872374e-05]\n",
            "   [4.1887419e-05]\n",
            "   [3.2953565e-05]]]]\n",
            "LABEL: 3\n",
            "PREDICTION: [[0.0390625  0.0390625  0.140625   0.77734375 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python_predict(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4wKqaE1YsUA",
        "outputId": "5f31272c-8c74-43f6-ae92-8ffa5d0ff9f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST EEG DATA SAMPLE:\n",
            "[[[[ 3.7982758e-07]\n",
            "   [ 1.1704891e-06]\n",
            "   [ 1.2014954e-06]\n",
            "   ...\n",
            "   [-4.0385748e-06]\n",
            "   [-5.1703059e-06]\n",
            "   [-6.6276034e-06]]]]\n",
            "LABEL: 4\n",
            "PREDICTION: [[0.06640625 0.2734375  0.375      0.109375   0.171875  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMr0sORFonzQ"
      },
      "source": [
        "# Calculate Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSrcjYXfoOgS"
      },
      "outputs": [],
      "source": [
        "def cal_acc(filename, x, y):\n",
        "  # Load TFLite model and allocate tensors.\n",
        "  interpreter = tf.lite.Interpreter(model_path=filename)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Get input and output tensors.\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "\n",
        "  # Test model on random input data.\n",
        "  print(\"SAMPLE CORRECT TEST PREDICTIONS: \")\n",
        "  count = 0\n",
        "  for i in range(len(x)-1):\n",
        "    input_data = np.array(x[i:i+1, :], dtype=np.float32)\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "    interpreter.invoke()\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    if np.argmax(output_data) == y[i]:\n",
        "      print(\"Prediction:\", np.argmax(output_data), \"-\", \"Label:\", y[i], end=\"  |  \")\n",
        "      count += 1\n",
        "  \n",
        "  print()\n",
        "  return count/(len(x)-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## best_doubleCNNmodel"
      ],
      "metadata": {
        "id": "k-oo4RE8_sDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TEST ACCURACY best_doubleCNNmodel: \", cal_acc(\"best_doubleCNNmodel.tflite\", x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9WnfEQKCCU",
        "outputId": "1458d995-5e58-45a9-961e-9905de64a88e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAMPLE CORRECT TEST PREDICTIONS: \n",
            "Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 1 - Label: 1  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 3 - Label: 3  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 1 - Label: 1  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 1 - Label: 1  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 1 - Label: 1  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 1 - Label: 1  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 1 - Label: 1  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 1 - Label: 1  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 1 - Label: 1  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 1 - Label: 1  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 1 - Label: 1  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 1 - Label: 1  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 1 - Label: 1  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 0 - Label: 0  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 2 - Label: 2  |  Prediction: 3 - Label: 3  |  \n",
            "TEST ACCURACY best_doubleCNNmodel:  0.5785804816223067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lfinalbest_doubleCNNmodel_3"
      ],
      "metadata": {
        "id": "6zh4C5X5xD9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test accuracy for lfinalbest_doubleCNNmodel: \", cal_acc(\"lfinalbest_doubleCNNmodel_3.tflite\", x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UevxdHn6xLt1",
        "outputId": "53e22991-a9d9-4e9b-f242-e5676bffd40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 - 4,1 - 1,2 - 2,2 - 2,0 - 0,2 - 2,1 - 1,1 - 1,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,0 - 0,2 - 2,1 - 1,1 - 1,2 - 2,2 - 2,2 - 2,0 - 0,2 - 2,3 - 3,2 - 2,2 - 2,0 - 0,1 - 1,1 - 1,2 - 2,3 - 3,4 - 4,2 - 2,1 - 1,1 - 1,0 - 0,2 - 2,3 - 3,2 - 2,0 - 0,0 - 0,1 - 1,2 - 2,3 - 3,2 - 2,2 - 2,2 - 2,2 - 2,1 - 1,0 - 0,2 - 2,4 - 4,2 - 2,2 - 2,2 - 2,2 - 2,0 - 0,1 - 1,3 - 3,2 - 2,1 - 1,2 - 2,0 - 0,2 - 2,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,2 - 2,4 - 4,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,0 - 0,2 - 2,2 - 2,4 - 4,1 - 1,2 - 2,0 - 0,2 - 2,2 - 2,4 - 4,2 - 2,2 - 2,1 - 1,0 - 0,4 - 4,2 - 2,1 - 1,2 - 2,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,1 - 1,0 - 0,2 - 2,2 - 2,2 - 2,2 - 2,1 - 1,1 - 1,2 - 2,0 - 0,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,0 - 0,4 - 4,2 - 2,2 - 2,0 - 0,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,1 - 1,3 - 3,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,1 - 1,0 - 0,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,4 - 4,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,0 - 0,0 - 0,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,0 - 0,0 - 0,2 - 2,1 - 1,2 - 2,0 - 0,4 - 4,1 - 1,1 - 1,2 - 2,0 - 0,1 - 1,2 - 2,0 - 0,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,0 - 0,2 - 2,0 - 0,0 - 0,1 - 1,2 - 2,2 - 2,3 - 3,2 - 2,2 - 2,1 - 1,0 - 0,1 - 1,1 - 1,4 - 4,2 - 2,2 - 2,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,0 - 0,4 - 4,0 - 0,2 - 2,0 - 0,1 - 1,1 - 1,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,1 - 1,0 - 0,2 - 2,2 - 2,3 - 3,2 - 2,1 - 1,1 - 1,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,0 - 0,0 - 0,2 - 2,2 - 2,2 - 2,3 - 3,0 - 0,0 - 0,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,0 - 0,2 - 2,0 - 0,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,0 - 0,0 - 0,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,0 - 0,3 - 3,2 - 2,2 - 2,2 - 2,2 - 2,1 - 1,3 - 3,1 - 1,2 - 2,3 - 3,2 - 2,1 - 1,2 - 2,2 - 2,0 - 0,2 - 2,2 - 2,1 - 1,0 - 0,1 - 1,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,1 - 1,1 - 1,1 - 1,2 - 2,1 - 1,1 - 1,0 - 0,2 - 2,1 - 1,2 - 2,2 - 2,0 - 0,2 - 2,1 - 1,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,3 - 3,2 - 2,2 - 2,2 - 2,1 - 1,0 - 0,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,0 - 0,2 - 2,2 - 2,0 - 0,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,3 - 3,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,0 - 0,2 - 2,2 - 2,0 - 0,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,1 - 1,4 - 4,2 - 2,4 - 4,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,3 - 3,2 - 2,0 - 0,2 - 2,2 - 2,0 - 0,3 - 3,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,1 - 1,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,1 - 1,3 - 3,2 - 2,1 - 1,2 - 2,3 - 3,1 - 1,2 - 2,1 - 1,1 - 1,2 - 2,2 - 2,2 - 2,0 - 0,0 - 0,3 - 3,1 - 1,1 - 1,2 - 2,1 - 1,0 - 0,3 - 3,0 - 0,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,4 - 4,0 - 0,1 - 1,0 - 0,1 - 1,2 - 2,3 - 3,0 - 0,2 - 2,2 - 2,2 - 2,3 - 3,0 - 0,1 - 1,2 - 2,2 - 2,2 - 2,4 - 4,3 - 3,0 - 0,2 - 2,2 - 2,0 - 0,0 - 0,2 - 2,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,2 - 2,4 - 4,2 - 2,2 - 2,2 - 2,2 - 2,3 - 3,2 - 2,2 - 2,2 - 2,2 - 2,4 - 4,2 - 2,0 - 0,2 - 2,2 - 2,0 - 0,1 - 1,4 - 4,4 - 4,2 - 2,1 - 1,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,3 - 3,0 - 0,0 - 0,1 - 1,0 - 0,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,4 - 4,1 - 1,2 - 2,2 - 2,0 - 0,1 - 1,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,1 - 1,1 - 1,1 - 1,1 - 1,1 - 1,2 - 2,2 - 2,1 - 1,2 - 2,0 - 0,2 - 2,0 - 0,2 - 2,1 - 1,1 - 1,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,3 - 3,1 - 1,0 - 0,2 - 2,0 - 0,3 - 3,1 - 1,0 - 0,2 - 2,2 - 2,4 - 4,2 - 2,2 - 2,1 - 1,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,3 - 3,2 - 2,0 - 0,1 - 1,4 - 4,3 - 3,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,2 - 2,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,3 - 3,1 - 1,2 - 2,3 - 3,2 - 2,2 - 2,3 - 3,2 - 2,0 - 0,3 - 3,2 - 2,2 - 2,2 - 2,2 - 2,1 - 1,2 - 2,0 - 0,2 - 2,1 - 1,1 - 1,1 - 1,3 - 3,2 - 2,2 - 2,2 - 2,3 - 3,2 - 2,1 - 1,2 - 2,2 - 2,0 - 0,0 - 0,2 - 2,2 - 2,0 - 0,3 - 3,2 - 2,2 - 2,2 - 2,0 - 0,2 - 2,1 - 1,0 - 0,2 - 2,0 - 0,2 - 2,0 - 0,1 - 1,2 - 2,2 - 2,0 - 0,2 - 2,0 - 0,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,2 - 2,\n",
            "Test accuracy for lfinalbest_doubleCNNmodel:  0.4217687074829932\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}